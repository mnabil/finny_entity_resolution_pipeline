[0m22:16:42.079190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b699710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b62a790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6bb110>]}


============================== 22:16:42.081718 | e421219e-65c1-42ac-a4d9-ffd38f71c6ff ==============================
[0m22:16:42.081718 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:16:42.082027 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'quiet': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_format': 'default', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'printer_width': '80', 'version_check': 'True', 'debug': 'False', 'write_json': 'True', 'empty': 'None', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m22:16:42.091288 [info ] [MainThread]: dbt version: 1.10.13
[0m22:16:42.091512 [info ] [MainThread]: python version: 3.11.7
[0m22:16:42.091656 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.11/bin/python3
[0m22:16:42.091781 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m22:16:42.350554 [info ] [MainThread]: Using profiles dir at /Users/mahmoud/Workspace/finny/dbt_service
[0m22:16:42.350956 [info ] [MainThread]: Using profiles.yml file at /Users/mahmoud/Workspace/finny/dbt_service/profiles.yml
[0m22:16:42.351149 [info ] [MainThread]: Using dbt_project.yml file at /Users/mahmoud/Workspace/finny/dbt_service/dbt_project.yml
[0m22:16:42.351534 [info ] [MainThread]: adapter type: postgres
[0m22:16:42.351712 [info ] [MainThread]: adapter version: 1.9.1
[0m22:16:42.405386 [info ] [MainThread]: Configuration:
[0m22:16:42.405672 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:16:42.405812 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:16:42.405944 [info ] [MainThread]: Required dependencies:
[0m22:16:42.406120 [debug] [MainThread]: Executing "git --help"
[0m22:16:42.423117 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:16:42.423641 [debug] [MainThread]: STDERR: "b''"
[0m22:16:42.423798 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:16:42.423962 [info ] [MainThread]: Connection:
[0m22:16:42.424123 [info ] [MainThread]:   host: localhost
[0m22:16:42.424233 [info ] [MainThread]:   port: 4320
[0m22:16:42.424348 [info ] [MainThread]:   user: finny_user
[0m22:16:42.424452 [info ] [MainThread]:   database: finny_db
[0m22:16:42.424556 [info ] [MainThread]:   schema: public
[0m22:16:42.424661 [info ] [MainThread]:   connect_timeout: 10
[0m22:16:42.424762 [info ] [MainThread]:   role: None
[0m22:16:42.424868 [info ] [MainThread]:   search_path: None
[0m22:16:42.424967 [info ] [MainThread]:   keepalives_idle: 0
[0m22:16:42.425068 [info ] [MainThread]:   sslmode: None
[0m22:16:42.425171 [info ] [MainThread]:   sslcert: None
[0m22:16:42.425271 [info ] [MainThread]:   sslkey: None
[0m22:16:42.425377 [info ] [MainThread]:   sslrootcert: None
[0m22:16:42.425481 [info ] [MainThread]:   application_name: dbt
[0m22:16:42.425582 [info ] [MainThread]:   retries: 1
[0m22:16:42.425861 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:16:42.479842 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m22:16:42.513197 [debug] [MainThread]: Using postgres connection "debug"
[0m22:16:42.513402 [debug] [MainThread]: On debug: select 1 as id
[0m22:16:42.513533 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:42.539635 [debug] [MainThread]: SQL status: SELECT 1 in 0.026 seconds
[0m22:16:42.540137 [debug] [MainThread]: On debug: Close
[0m22:16:42.540300 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:16:42.540459 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:16:42.567788 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.52927613, "process_in_blocks": "0", "process_kernel_time": 0.21528, "process_mem_max_rss": "125190144", "process_out_blocks": "0", "process_user_time": 0.729828}
[0m22:16:42.568139 [debug] [MainThread]: Command `dbt debug` succeeded at 22:16:42.568091 after 0.53 seconds
[0m22:16:42.568299 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:16:42.568464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6d7d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1022ac090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102376890>]}
[0m22:16:42.568655 [debug] [MainThread]: Flushing usage events
[0m22:16:42.862905 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:19:08.789921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e2b290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114eb3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114eb3f90>]}


============================== 22:19:08.792520 | df9f42de-2cfb-4e67-bf84-08634f145e6c ==============================
[0m22:19:08.792520 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:19:08.792797 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'debug': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --models staging', 'version_check': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'empty': 'False'}
[0m22:19:08.793031 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m22:19:08.793221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ed6450>]}
[0m22:19:08.912533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e44290>]}
[0m22:19:08.942960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a9b3d0>]}
[0m22:19:08.943619 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:19:08.989930 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:19:08.990337 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:19:08.990522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115508390>]}
[0m22:19:09.510675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ea8510>]}
[0m22:19:09.544611 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:19:09.545650 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:19:09.555830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116a9ead0>]}
[0m22:19:09.556067 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 449 macros
[0m22:19:09.556219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154a3150>]}
[0m22:19:09.556854 [info ] [MainThread]: 
[0m22:19:09.556997 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:19:09.557116 [info ] [MainThread]: 
[0m22:19:09.557303 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:19:09.558794 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:19:09.584283 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:19:09.584520 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:19:09.584645 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:19:09.613235 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.029 seconds
[0m22:19:09.613818 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:19:09.614392 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:19:09.616787 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:19:09.616936 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:19:09.617042 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:09.625760 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:19:09.625909 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:19:09.626043 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:19:09.630581 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:19:09.631114 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:19:09.631759 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:19:09.633553 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.633675 [debug] [MainThread]: On master: BEGIN
[0m22:19:09.633775 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:09.642418 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m22:19:09.642554 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.642728 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:19:09.647025 [debug] [MainThread]: SQL status: SELECT 0 in 0.004 seconds
[0m22:19:09.647463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e1ee50>]}
[0m22:19:09.647667 [debug] [MainThread]: On master: ROLLBACK
[0m22:19:09.648462 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.648575 [debug] [MainThread]: On master: BEGIN
[0m22:19:09.649354 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:19:09.649463 [debug] [MainThread]: On master: COMMIT
[0m22:19:09.649575 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.649681 [debug] [MainThread]: On master: COMMIT
[0m22:19:09.650065 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:19:09.650185 [debug] [MainThread]: On master: Close
[0m22:19:09.651799 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:19:09.651974 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:19:09.652194 [info ] [Thread-1 (]: 1 of 2 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:19:09.652403 [info ] [Thread-2 (]: 2 of 2 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:19:09.652599 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:19:09.652781 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:19:09.652934 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:19:09.653076 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:19:09.656288 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:19:09.657566 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:19:09.658060 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:19:09.664501 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:19:09.671746 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:19:09.673269 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:19:09.673837 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:19:09.674024 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:19:09.674163 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:19:09.674305 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:19:09.674438 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:19:09.674568 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:19:09.684739 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:19:09.684897 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:19:09.685064 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:19:09.685243 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:19:09.685403 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model extracts and flattens the JSON data from the pdl_data table



select
    id as source_id,
    data->>'pdl_id' as pdl_id,
    data->>'name' as name,
    data->>'email' as email,
    data->>'company' as company,
    case 
        when data->>'company_revenue' = 'null' then null
        else (data->>'company_revenue')::numeric
    end as company_revenue,
    data->>'title' as title,
    data->>'location' as location,
    created_at
from "finny_db"."public"."pdl_data"
  );
[0m22:19:09.685584 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model extracts and flattens the JSON data from the fxf_data table



select
    id as source_id,
    data->>'fxf_id' as fxf_id,
    data->>'name' as name,
    data->>'email' as email,
    data->>'company' as company,
    case 
        when data->>'company_revenue' = 'null' then null
        else (data->>'company_revenue')::numeric
    end as company_revenue,
    data->>'title' as title,
    data->>'location' as location,
    created_at
from "finny_db"."public"."fxf_data"
  );
[0m22:19:09.688893 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m22:19:09.689035 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m22:19:09.691501 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:19:09.692653 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:19:09.692826 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m22:19:09.692970 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m22:19:09.693897 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:19:09.699063 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:19:09.699229 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:19:09.699360 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:19:09.699498 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m22:19:09.700035 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:19:09.700176 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:19:09.700312 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:19:09.701211 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m22:19:09.704321 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public"."stg_fxf_data__dbt_backup"
[0m22:19:09.704476 [debug] [Thread-2 (]: SQL status: COMMIT in 0.004 seconds
[0m22:19:09.706276 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:19:09.707120 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public"."stg_pdl_data__dbt_backup"
[0m22:19:09.707263 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public"."stg_fxf_data__dbt_backup" cascade
[0m22:19:09.707529 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:19:09.707686 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public"."stg_pdl_data__dbt_backup" cascade
[0m22:19:09.708146 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m22:19:09.708280 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m22:19:09.709145 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:19:09.709572 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:19:09.710515 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e9cd10>]}
[0m22:19:09.710679 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df9f42de-2cfb-4e67-bf84-08634f145e6c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ee62d0>]}
[0m22:19:09.710924 [info ] [Thread-1 (]: 1 of 2 OK created sql view model public.stg_fxf_data ........................... [[32mCREATE VIEW[0m in 0.06s]
[0m22:19:09.711201 [info ] [Thread-2 (]: 2 of 2 OK created sql view model public.stg_pdl_data ........................... [[32mCREATE VIEW[0m in 0.06s]
[0m22:19:09.711430 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:19:09.711611 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:19:09.712165 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.712277 [debug] [MainThread]: On master: BEGIN
[0m22:19:09.712376 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:19:09.719922 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:19:09.720039 [debug] [MainThread]: On master: COMMIT
[0m22:19:09.720153 [debug] [MainThread]: Using postgres connection "master"
[0m22:19:09.720251 [debug] [MainThread]: On master: COMMIT
[0m22:19:09.720747 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:19:09.720851 [debug] [MainThread]: On master: Close
[0m22:19:09.720996 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:09.721106 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:19:09.721198 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:19:09.721328 [info ] [MainThread]: 
[0m22:19:09.721465 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m22:19:09.721769 [debug] [MainThread]: Command end result
[0m22:19:09.730504 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:19:09.731567 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:19:09.734146 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:19:09.734309 [info ] [MainThread]: 
[0m22:19:09.734480 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:19:09.734600 [info ] [MainThread]: 
[0m22:19:09.734752 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:19:09.735013 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:19:09.737289 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.981965, "process_in_blocks": "0", "process_kernel_time": 0.210553, "process_mem_max_rss": "143638528", "process_out_blocks": "0", "process_user_time": 1.321634}
[0m22:19:09.737633 [debug] [MainThread]: Command `dbt run` succeeded at 22:19:09.737590 after 0.98 seconds
[0m22:19:09.737840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042fc190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ed45d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104337690>]}
[0m22:19:09.738024 [debug] [MainThread]: Flushing usage events
[0m22:19:10.018593 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:20:21.406026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a64e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d338d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d33f50>]}


============================== 22:20:21.407902 | c6ed2854-722e-4c84-a9c2-397a9fd2f65d ==============================
[0m22:20:21.407902 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:20:21.408200 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt seed', 'empty': 'None', 'partial_parse': 'True', 'write_json': 'True', 'fail_fast': 'False', 'version_check': 'True', 'introspect': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error': 'None', 'use_colors': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False'}
[0m22:20:21.493200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11159a2d0>]}
[0m22:20:21.522931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a91490>]}
[0m22:20:21.523381 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:20:21.565667 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:20:21.613891 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m22:20:21.614166 [debug] [MainThread]: Partial parsing: added file: dbt_service://seeds/pdl_sample.csv
[0m22:20:21.614304 [debug] [MainThread]: Partial parsing: added file: dbt_service://seeds/fxf_sample.csv
[0m22:20:21.614450 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m22:20:21.614596 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m22:20:21.828846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112eb78d0>]}
[0m22:20:21.867235 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:20:21.868087 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:20:21.874235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f95050>]}
[0m22:20:21.874459 [info ] [MainThread]: Found 4 models, 4 data tests, 2 seeds, 2 sources, 449 macros
[0m22:20:21.874614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fd9310>]}
[0m22:20:21.875347 [info ] [MainThread]: 
[0m22:20:21.875479 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:20:21.875583 [info ] [MainThread]: 
[0m22:20:21.875775 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:20:21.877380 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:20:21.899799 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:20:21.900031 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:20:21.900145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:21.919744 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.020 seconds
[0m22:20:21.920356 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:20:21.921026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:20:21.923440 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:20:21.923581 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:20:21.923692 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:21.932554 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:20:21.932707 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:20:21.932835 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:20:21.936046 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m22:20:21.936555 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:20:21.937311 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:20:21.939379 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:21.939510 [debug] [MainThread]: On master: BEGIN
[0m22:20:21.939621 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:21.949367 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m22:20:21.949488 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:21.949640 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:20:21.953041 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:20:21.953637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112eb4410>]}
[0m22:20:21.953837 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:21.954408 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:21.954519 [debug] [MainThread]: On master: BEGIN
[0m22:20:21.955423 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:20:21.955556 [debug] [MainThread]: On master: COMMIT
[0m22:20:21.955665 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:21.955776 [debug] [MainThread]: On master: COMMIT
[0m22:20:21.956281 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:20:21.956391 [debug] [MainThread]: On master: Close
[0m22:20:21.957847 [debug] [Thread-1 (]: Began running node seed.dbt_service.fxf_sample
[0m22:20:21.958024 [debug] [Thread-2 (]: Began running node seed.dbt_service.pdl_sample
[0m22:20:21.958217 [info ] [Thread-1 (]: 1 of 2 START seed file public.fxf_sample ....................................... [RUN]
[0m22:20:21.958439 [info ] [Thread-2 (]: 2 of 2 START seed file public.pdl_sample ....................................... [RUN]
[0m22:20:21.958651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now seed.dbt_service.fxf_sample)
[0m22:20:21.958878 [debug] [Thread-2 (]: Acquiring new postgres connection 'seed.dbt_service.pdl_sample'
[0m22:20:21.959045 [debug] [Thread-1 (]: Began compiling node seed.dbt_service.fxf_sample
[0m22:20:21.959191 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.pdl_sample
[0m22:20:21.959335 [debug] [Thread-1 (]: Began executing node seed.dbt_service.fxf_sample
[0m22:20:21.959472 [debug] [Thread-2 (]: Began executing node seed.dbt_service.pdl_sample
[0m22:20:21.970424 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m22:20:21.971620 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: BEGIN
[0m22:20:21.971765 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:20:21.972328 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m22:20:21.972535 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: BEGIN
[0m22:20:21.972666 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:20:21.981637 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m22:20:21.981801 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:20:21.981965 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m22:20:21.982109 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m22:20:21.982249 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.pdl_sample"} */

    create table "finny_db"."public"."pdl_sample" ("pdl_id" text,"name" text,"email" text,"company" text,"company_revenue" integer,"title" text,"location" text)
  
[0m22:20:21.982390 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.fxf_sample"} */

    create table "finny_db"."public"."fxf_sample" ("fxf_id" text,"name" text,"email" text,"company" text,"company_revenue" integer,"title" text,"location" text)
  
[0m22:20:21.987731 [debug] [Thread-2 (]: SQL status: CREATE TABLE in 0.005 seconds
[0m22:20:21.994359 [debug] [Thread-1 (]: SQL status: CREATE TABLE in 0.012 seconds
[0m22:20:21.995838 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m22:20:21.998421 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m22:20:21.998578 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: 
          insert into "finny_db"."public"."pdl_sample" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m22:20:21.998726 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: 
          insert into "finny_db"."public"."fxf_sample" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m22:20:21.999360 [debug] [Thread-2 (]: SQL status: INSERT 0 5 in 0.000 seconds
[0m22:20:21.999513 [debug] [Thread-1 (]: SQL status: INSERT 0 5 in 0.001 seconds
[0m22:20:22.001759 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.dbt_service.pdl_sample"
[0m22:20:22.002034 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.dbt_service.fxf_sample"
[0m22:20:22.009788 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: COMMIT
[0m22:20:22.010316 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: COMMIT
[0m22:20:22.010483 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m22:20:22.010643 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m22:20:22.010793 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: COMMIT
[0m22:20:22.010933 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: COMMIT
[0m22:20:22.012384 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m22:20:22.012504 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m22:20:22.013163 [debug] [Thread-1 (]: On seed.dbt_service.fxf_sample: Close
[0m22:20:22.013437 [debug] [Thread-2 (]: On seed.dbt_service.pdl_sample: Close
[0m22:20:22.014509 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c32d90>]}
[0m22:20:22.014646 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6ed2854-722e-4c84-a9c2-397a9fd2f65d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f77cd0>]}
[0m22:20:22.014907 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file public.pdl_sample ................................... [[32mINSERT 5[0m in 0.06s]
[0m22:20:22.015138 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file public.fxf_sample ................................... [[32mINSERT 5[0m in 0.06s]
[0m22:20:22.015403 [debug] [Thread-2 (]: Finished running node seed.dbt_service.pdl_sample
[0m22:20:22.015592 [debug] [Thread-1 (]: Finished running node seed.dbt_service.fxf_sample
[0m22:20:22.016147 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:22.016265 [debug] [MainThread]: On master: BEGIN
[0m22:20:22.016379 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:22.024309 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:20:22.024463 [debug] [MainThread]: On master: COMMIT
[0m22:20:22.024585 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:22.024683 [debug] [MainThread]: On master: COMMIT
[0m22:20:22.025184 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:20:22.025317 [debug] [MainThread]: On master: Close
[0m22:20:22.025642 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:22.025819 [debug] [MainThread]: Connection 'seed.dbt_service.fxf_sample' was properly closed.
[0m22:20:22.025926 [debug] [MainThread]: Connection 'seed.dbt_service.pdl_sample' was properly closed.
[0m22:20:22.026057 [info ] [MainThread]: 
[0m22:20:22.026184 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m22:20:22.026474 [debug] [MainThread]: Command end result
[0m22:20:22.035616 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:20:22.036623 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:20:22.039383 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:20:22.039578 [info ] [MainThread]: 
[0m22:20:22.039761 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:20:22.039898 [info ] [MainThread]: 
[0m22:20:22.040039 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:20:22.041396 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 0.6684073, "process_in_blocks": "0", "process_kernel_time": 0.182154, "process_mem_max_rss": "139395072", "process_out_blocks": "0", "process_user_time": 1.084782}
[0m22:20:22.041589 [debug] [MainThread]: Command `dbt seed` succeeded at 22:20:22.041556 after 0.67 seconds
[0m22:20:22.041738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cfb490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a48190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a7f690>]}
[0m22:20:22.041909 [debug] [MainThread]: Flushing usage events
[0m22:20:22.317254 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:20:29.309195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af266d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af33650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af33d10>]}


============================== 22:20:29.311170 | 598b3610-3889-4d0e-8bf4-64379be0384f ==============================
[0m22:20:29.311170 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:20:29.311448 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'log_format': 'default', 'empty': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'fail_fast': 'False', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'version_check': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'warn_error': 'None', 'quiet': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'debug': 'False', 'no_print': 'None', 'partial_parse': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'target_path': 'None'}
[0m22:20:29.394384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aeb8450>]}
[0m22:20:29.423932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a89650>]}
[0m22:20:29.424371 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:20:29.466694 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:20:29.517980 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:20:29.518200 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:20:29.540866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c804090>]}
[0m22:20:29.578753 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:20:29.579600 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:20:29.585133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c937790>]}
[0m22:20:29.585361 [info ] [MainThread]: Found 4 models, 4 data tests, 2 seeds, 2 sources, 449 macros
[0m22:20:29.585522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf56ed0>]}
[0m22:20:29.586357 [info ] [MainThread]: 
[0m22:20:29.586518 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:20:29.586642 [info ] [MainThread]: 
[0m22:20:29.586866 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:20:29.588489 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:20:29.607797 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:20:29.607995 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:20:29.608111 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:29.628969 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.021 seconds
[0m22:20:29.629704 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:20:29.630321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:20:29.632681 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:20:29.632825 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:20:29.632930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:29.641358 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m22:20:29.641561 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:20:29.641690 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:20:29.644630 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m22:20:29.645153 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:20:29.645728 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:20:29.647903 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.648043 [debug] [MainThread]: On master: BEGIN
[0m22:20:29.648152 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:29.656377 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:20:29.656551 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.656714 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:20:29.660937 [debug] [MainThread]: SQL status: SELECT 2 in 0.004 seconds
[0m22:20:29.661558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9bd490>]}
[0m22:20:29.661762 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:29.662432 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.662561 [debug] [MainThread]: On master: BEGIN
[0m22:20:29.663469 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:20:29.663582 [debug] [MainThread]: On master: COMMIT
[0m22:20:29.663686 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.663787 [debug] [MainThread]: On master: COMMIT
[0m22:20:29.664214 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:20:29.664322 [debug] [MainThread]: On master: Close
[0m22:20:29.665312 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:20:29.665498 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:20:29.665728 [info ] [Thread-1 (]: 1 of 4 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:20:29.665952 [info ] [Thread-2 (]: 2 of 4 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:20:29.666171 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:20:29.666478 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:20:29.666633 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:20:29.666776 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:20:29.670335 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:20:29.696642 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:20:29.697225 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:20:29.697413 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:20:29.710280 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:20:29.711724 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:20:29.712089 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.712229 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:20:29.712380 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.712514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:20:29.712649 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:20:29.712860 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:20:29.723583 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m22:20:29.723757 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m22:20:29.723901 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.724030 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.724168 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model works with the seeded FXF data



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public"."fxf_sample"
  );
[0m22:20:29.724314 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model works with the seeded PDL data



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public"."pdl_sample"
  );
[0m22:20:29.726174 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m22:20:29.728541 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.728680 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m22:20:29.728813 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public"."stg_fxf_data" rename to "stg_fxf_data__dbt_backup"
[0m22:20:29.730007 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.730171 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public"."stg_pdl_data" rename to "stg_pdl_data__dbt_backup"
[0m22:20:29.730961 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.731087 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.732258 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.733273 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.733418 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m22:20:29.733555 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m22:20:29.734550 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.734683 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.740649 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:20:29.741244 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:20:29.741398 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.741553 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.741695 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:20:29.741831 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:20:29.744518 [debug] [Thread-2 (]: SQL status: COMMIT in 0.003 seconds
[0m22:20:29.746756 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public"."stg_pdl_data__dbt_backup"
[0m22:20:29.748454 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:20:29.748609 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public"."stg_pdl_data__dbt_backup" cascade
[0m22:20:29.748755 [debug] [Thread-1 (]: SQL status: COMMIT in 0.007 seconds
[0m22:20:29.750238 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public"."stg_fxf_data__dbt_backup"
[0m22:20:29.750490 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:20:29.750617 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public"."stg_fxf_data__dbt_backup" cascade
[0m22:20:29.752102 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.003 seconds
[0m22:20:29.752958 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:20:29.753521 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m22:20:29.753941 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:20:29.754242 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b623690>]}
[0m22:20:29.754624 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc56350>]}
[0m22:20:29.754508 [info ] [Thread-2 (]: 2 of 4 OK created sql view model public.stg_pdl_data ........................... [[32mCREATE VIEW[0m in 0.09s]
[0m22:20:29.755104 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:20:29.754877 [info ] [Thread-1 (]: 1 of 4 OK created sql view model public.stg_fxf_data ........................... [[32mCREATE VIEW[0m in 0.09s]
[0m22:20:29.755385 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:20:29.755699 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m22:20:29.755896 [info ] [Thread-4 (]: 3 of 4 START sql table model public.company_analysis ........................... [RUN]
[0m22:20:29.756096 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m22:20:29.756266 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m22:20:29.756447 [info ] [Thread-3 (]: 4 of 4 START sql table model public.location_analysis .......................... [RUN]
[0m22:20:29.756617 [debug] [Thread-4 (]: Began compiling node model.dbt_service.company_analysis
[0m22:20:29.756785 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_service.location_analysis'
[0m22:20:29.757973 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m22:20:29.758111 [debug] [Thread-3 (]: Began compiling node model.dbt_service.location_analysis
[0m22:20:29.759291 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m22:20:29.759581 [debug] [Thread-4 (]: Began executing node model.dbt_service.company_analysis
[0m22:20:29.759735 [debug] [Thread-3 (]: Began executing node model.dbt_service.location_analysis
[0m22:20:29.767843 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m22:20:29.769304 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m22:20:29.769909 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:20:29.770099 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:20:29.770231 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: BEGIN
[0m22:20:29.770380 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: BEGIN
[0m22:20:29.770517 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m22:20:29.770677 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:20:29.780678 [debug] [Thread-3 (]: SQL status: BEGIN in 0.010 seconds
[0m22:20:29.780849 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m22:20:29.780982 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:20:29.781132 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:20:29.781312 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public"."stg_pdl_data"
),

location_stats as (
    select
        location,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location
)

select
    location,
    contact_count,
    company_count,
    unique_titles,
    data_sources,
    round(avg_revenue::numeric, 2) as avg_revenue
from location_stats
order by contact_count desc
  );
  
[0m22:20:29.781512 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m22:20:29.787737 [debug] [Thread-4 (]: SQL status: SELECT 8 in 0.006 seconds
[0m22:20:29.787876 [debug] [Thread-3 (]: SQL status: SELECT 8 in 0.006 seconds
[0m22:20:29.790463 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:20:29.791643 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:20:29.791789 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m22:20:29.791937 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m22:20:29.792927 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.793069 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:20:29.793582 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:20:29.794098 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:20:29.794251 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:20:29.794386 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:20:29.794514 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:20:29.794648 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:20:29.795746 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m22:20:29.795884 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m22:20:29.796725 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public"."company_analysis__dbt_backup"
[0m22:20:29.797537 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public"."location_analysis__dbt_backup"
[0m22:20:29.798616 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:20:29.798887 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:20:29.799027 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public"."company_analysis__dbt_backup" cascade
[0m22:20:29.799166 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public"."location_analysis__dbt_backup" cascade
[0m22:20:29.799966 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m22:20:29.800437 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: Close
[0m22:20:29.800586 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m22:20:29.800797 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107daba10>]}
[0m22:20:29.801289 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: Close
[0m22:20:29.801546 [info ] [Thread-3 (]: 4 of 4 OK created sql table model public.location_analysis ..................... [[32mSELECT 8[0m in 0.04s]
[0m22:20:29.801800 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '598b3610-3889-4d0e-8bf4-64379be0384f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6db250>]}
[0m22:20:29.801992 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m22:20:29.802210 [info ] [Thread-4 (]: 3 of 4 OK created sql table model public.company_analysis ...................... [[32mSELECT 8[0m in 0.05s]
[0m22:20:29.802489 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m22:20:29.803014 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.803128 [debug] [MainThread]: On master: BEGIN
[0m22:20:29.803228 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:29.810734 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:20:29.811028 [debug] [MainThread]: On master: COMMIT
[0m22:20:29.811149 [debug] [MainThread]: Using postgres connection "master"
[0m22:20:29.811270 [debug] [MainThread]: On master: COMMIT
[0m22:20:29.811851 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:20:29.812047 [debug] [MainThread]: On master: Close
[0m22:20:29.812225 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:29.812355 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:20:29.812466 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:20:29.812562 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m22:20:29.812658 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m22:20:29.812839 [info ] [MainThread]: 
[0m22:20:29.813027 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m22:20:29.813449 [debug] [MainThread]: Command end result
[0m22:20:29.824028 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:20:29.824956 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:20:29.827331 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:20:29.827455 [info ] [MainThread]: 
[0m22:20:29.827600 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:20:29.827712 [info ] [MainThread]: 
[0m22:20:29.827828 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m22:20:29.829064 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.55336636, "process_in_blocks": "0", "process_kernel_time": 0.163674, "process_mem_max_rss": "134414336", "process_out_blocks": "0", "process_user_time": 0.963727}
[0m22:20:29.829257 [debug] [MainThread]: Command `dbt run` succeeded at 22:20:29.829224 after 0.55 seconds
[0m22:20:29.829397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049e0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104aaa910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a17690>]}
[0m22:20:29.829540 [debug] [MainThread]: Flushing usage events
[0m22:20:30.045514 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:22:41.982120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12772bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12779e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1277abfd0>]}


============================== 22:22:41.984626 | 4d231930-68ce-4c47-bdb0-3cca2d7265df ==============================
[0m22:22:41.984626 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:22:41.984910 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt seed --select fxf_data pdl_data', 'partial_parse': 'True', 'debug': 'False', 'target_path': 'None', 'log_cache_events': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_format': 'default', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'no_print': 'None', 'introspect': 'True', 'warn_error': 'None', 'write_json': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'printer_width': '80', 'indirect_selection': 'eager'}
[0m22:22:42.084441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1278a7590>]}
[0m22:22:42.114047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1227597d0>]}
[0m22:22:42.114879 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:22:42.162030 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:22:42.224442 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m22:22:42.224786 [debug] [MainThread]: Partial parsing: added file: dbt_service://seeds/pdl_data.csv
[0m22:22:42.224924 [debug] [MainThread]: Partial parsing: added file: dbt_service://seeds/fxf_data.csv
[0m22:22:42.225074 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m22:22:42.225218 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m22:22:42.444555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x148ad5010>]}
[0m22:22:42.482332 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:22:42.483290 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:22:42.493213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127ef5b10>]}
[0m22:22:42.493447 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:22:42.493606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122588990>]}
[0m22:22:42.494315 [info ] [MainThread]: 
[0m22:22:42.494468 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:22:42.494582 [info ] [MainThread]: 
[0m22:22:42.494780 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:22:42.496424 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:22:42.522558 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:22:42.522786 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:22:42.522903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:42.554342 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.031 seconds
[0m22:22:42.554937 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:22:42.555605 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:22:42.558018 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:22:42.558148 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:22:42.558251 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:42.568100 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m22:22:42.568238 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:22:42.568370 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:22:42.572350 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m22:22:42.572869 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:22:42.573484 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:22:42.576010 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:42.576165 [debug] [MainThread]: On master: BEGIN
[0m22:22:42.576280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:22:42.584749 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:22:42.584870 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:42.585027 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:22:42.588488 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:22:42.589060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122744090>]}
[0m22:22:42.589246 [debug] [MainThread]: On master: ROLLBACK
[0m22:22:42.589659 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:42.589771 [debug] [MainThread]: On master: BEGIN
[0m22:22:42.590469 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:22:42.590613 [debug] [MainThread]: On master: COMMIT
[0m22:22:42.590729 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:42.590837 [debug] [MainThread]: On master: COMMIT
[0m22:22:42.591213 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:22:42.591351 [debug] [MainThread]: On master: Close
[0m22:22:42.592835 [debug] [Thread-1 (]: Began running node seed.dbt_service.fxf_data
[0m22:22:42.593200 [debug] [Thread-2 (]: Began running node seed.dbt_service.pdl_data
[0m22:22:42.593047 [info ] [Thread-1 (]: 1 of 2 START seed file public.fxf_data ......................................... [RUN]
[0m22:22:42.593449 [info ] [Thread-2 (]: 2 of 2 START seed file public.pdl_data ......................................... [RUN]
[0m22:22:42.593657 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now seed.dbt_service.fxf_data)
[0m22:22:42.593854 [debug] [Thread-2 (]: Acquiring new postgres connection 'seed.dbt_service.pdl_data'
[0m22:22:42.594009 [debug] [Thread-1 (]: Began compiling node seed.dbt_service.fxf_data
[0m22:22:42.594150 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.pdl_data
[0m22:22:42.594297 [debug] [Thread-1 (]: Began executing node seed.dbt_service.fxf_data
[0m22:22:42.594438 [debug] [Thread-2 (]: Began executing node seed.dbt_service.pdl_data
[0m22:22:43.073407 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:22:43.073724 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:22:43.105239 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: BEGIN
[0m22:22:43.105586 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: BEGIN
[0m22:22:43.105848 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:22:43.106067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:22:43.118165 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m22:22:43.118397 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m22:22:43.118589 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:22:43.118737 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:22:43.118880 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.fxf_data"} */
truncate table "finny_db"."public"."fxf_data"
[0m22:22:43.119027 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.pdl_data"} */
truncate table "finny_db"."public"."pdl_data"
[0m22:22:43.125034 [debug] [Thread-2 (]: SQL status: TRUNCATE TABLE in 0.006 seconds
[0m22:22:43.131696 [debug] [Thread-1 (]: SQL status: TRUNCATE TABLE in 0.012 seconds
[0m22:22:50.933357 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:22:50.946148 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:22:50.949543 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:22:50.970553 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:22:50.992568 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "pdl_id" of relation "pdl_data" does not exist
LINE 2: ...      insert into "finny_db"."public"."pdl_data" ("pdl_id", ...
                                                             ^

[0m22:22:50.992901 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: ROLLBACK
[0m22:22:50.993458 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: Close
[0m22:22:50.999868 [debug] [Thread-2 (]: Database Error in seed pdl_data (seeds/pdl_data.csv)
  column "pdl_id" of relation "pdl_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."pdl_data" ("pdl_id", ...
                                                               ^
[0m22:22:51.001220 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14e264450>]}
[0m22:22:51.001507 [error] [Thread-2 (]: 2 of 2 ERROR loading seed file public.pdl_data ................................. [[31mERROR[0m in 8.41s]
[0m22:22:51.001761 [debug] [Thread-2 (]: Finished running node seed.dbt_service.pdl_data
[0m22:22:51.002003 [debug] [Thread-7 (]: Marking all children of 'seed.dbt_service.pdl_data' to be skipped because of status 'error'.  Reason: Database Error in seed pdl_data (seeds/pdl_data.csv)
  column "pdl_id" of relation "pdl_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."pdl_data" ("pdl_id", ...
                                                               ^.
[0m22:22:51.003667 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "fxf_id" of relation "fxf_data" does not exist
LINE 2: ...      insert into "finny_db"."public"."fxf_data" ("fxf_id", ...
                                                             ^

[0m22:22:51.003845 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: ROLLBACK
[0m22:22:51.004222 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: Close
[0m22:22:51.005655 [debug] [Thread-1 (]: Database Error in seed fxf_data (seeds/fxf_data.csv)
  column "fxf_id" of relation "fxf_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."fxf_data" ("fxf_id", ...
                                                               ^
[0m22:22:51.006396 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d231930-68ce-4c47-bdb0-3cca2d7265df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14c3305d0>]}
[0m22:22:51.006603 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file public.fxf_data ................................. [[31mERROR[0m in 8.41s]
[0m22:22:51.006819 [debug] [Thread-1 (]: Finished running node seed.dbt_service.fxf_data
[0m22:22:51.007032 [debug] [Thread-7 (]: Marking all children of 'seed.dbt_service.fxf_data' to be skipped because of status 'error'.  Reason: Database Error in seed fxf_data (seeds/fxf_data.csv)
  column "fxf_id" of relation "fxf_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."fxf_data" ("fxf_id", ...
                                                               ^.
[0m22:22:51.007594 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:51.007728 [debug] [MainThread]: On master: BEGIN
[0m22:22:51.007839 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:22:51.013969 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m22:22:51.014185 [debug] [MainThread]: On master: COMMIT
[0m22:22:51.014330 [debug] [MainThread]: Using postgres connection "master"
[0m22:22:51.014436 [debug] [MainThread]: On master: COMMIT
[0m22:22:51.014895 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:22:51.015252 [debug] [MainThread]: On master: Close
[0m22:22:51.015503 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:22:51.015658 [debug] [MainThread]: Connection 'seed.dbt_service.fxf_data' was properly closed.
[0m22:22:51.015776 [debug] [MainThread]: Connection 'seed.dbt_service.pdl_data' was properly closed.
[0m22:22:51.015956 [info ] [MainThread]: 
[0m22:22:51.016145 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 8.52 seconds (8.52s).
[0m22:22:51.016613 [debug] [MainThread]: Command end result
[0m22:22:51.026334 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:22:51.027181 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:22:51.029427 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:22:51.029556 [info ] [MainThread]: 
[0m22:22:51.029714 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m22:22:51.029834 [info ] [MainThread]: 
[0m22:22:51.029973 [error] [MainThread]: [31mFailure in seed pdl_data (seeds/pdl_data.csv)[0m
[0m22:22:51.030102 [error] [MainThread]:   Database Error in seed pdl_data (seeds/pdl_data.csv)
  column "pdl_id" of relation "pdl_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."pdl_data" ("pdl_id", ...
                                                               ^
[0m22:22:51.030211 [info ] [MainThread]: 
[0m22:22:51.030337 [error] [MainThread]: [31mFailure in seed fxf_data (seeds/fxf_data.csv)[0m
[0m22:22:51.030458 [error] [MainThread]:   Database Error in seed fxf_data (seeds/fxf_data.csv)
  column "fxf_id" of relation "fxf_data" does not exist
  LINE 2: ...      insert into "finny_db"."public"."fxf_data" ("fxf_id", ...
                                                               ^
[0m22:22:51.030561 [info ] [MainThread]: 
[0m22:22:51.030673 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m22:22:51.032898 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 9.086853, "process_in_blocks": "0", "process_kernel_time": 0.303094, "process_mem_max_rss": "318423040", "process_out_blocks": "0", "process_user_time": 9.363932}
[0m22:22:51.033250 [debug] [MainThread]: Command `dbt seed` failed at 22:22:51.033210 after 9.09 seconds
[0m22:22:51.033447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1277d4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ffc110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x148b40350>]}
[0m22:22:51.033620 [debug] [MainThread]: Flushing usage events
[0m22:22:51.388984 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:23:18.057625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1012490d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106026890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106026210>]}


============================== 22:23:18.059592 | 0f4f0bc0-d670-4208-8ecd-69d733369ff1 ==============================
[0m22:23:18.059592 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:23:18.059912 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'debug': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'no_print': 'None', 'use_colors': 'True', 'log_cache_events': 'False', 'empty': 'None', 'invocation_command': 'dbt seed --select fxf_data pdl_data', 'log_format': 'default', 'fail_fast': 'False', 'introspect': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'target_path': 'None', 'static_parser': 'True', 'quiet': 'False', 'write_json': 'True', 'printer_width': '80'}
[0m22:23:18.144503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fbb450>]}
[0m22:23:18.174036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102cd5b10>]}
[0m22:23:18.174497 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:23:18.216616 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:23:18.267869 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:23:18.268083 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:23:18.290751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ee0390>]}
[0m22:23:18.329115 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:23:18.329924 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:23:18.335516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dfc950>]}
[0m22:23:18.335757 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:23:18.335912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbae50>]}
[0m22:23:18.336588 [info ] [MainThread]: 
[0m22:23:18.336738 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:23:18.336857 [info ] [MainThread]: 
[0m22:23:18.337072 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:23:18.338683 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:23:18.357346 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:23:18.357566 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:23:18.357694 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:23:18.379235 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.021 seconds
[0m22:23:18.380030 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:23:18.380812 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:23:18.383534 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:23:18.383713 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:23:18.383829 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:23:18.392808 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:23:18.393035 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:23:18.393187 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:23:18.396590 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m22:23:18.397283 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:23:18.397756 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:23:18.400262 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:18.400437 [debug] [MainThread]: On master: BEGIN
[0m22:23:18.400568 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:23:18.408583 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:23:18.408754 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:18.408912 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:23:18.412082 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:23:18.412713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d77390>]}
[0m22:23:18.412900 [debug] [MainThread]: On master: ROLLBACK
[0m22:23:18.413414 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:18.413524 [debug] [MainThread]: On master: BEGIN
[0m22:23:18.414183 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:23:18.414320 [debug] [MainThread]: On master: COMMIT
[0m22:23:18.414434 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:18.414535 [debug] [MainThread]: On master: COMMIT
[0m22:23:18.414877 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:23:18.414999 [debug] [MainThread]: On master: Close
[0m22:23:18.416185 [debug] [Thread-1 (]: Began running node seed.dbt_service.fxf_data
[0m22:23:18.416400 [debug] [Thread-2 (]: Began running node seed.dbt_service.pdl_data
[0m22:23:18.416603 [info ] [Thread-1 (]: 1 of 2 START seed file public.fxf_data ......................................... [RUN]
[0m22:23:18.416839 [info ] [Thread-2 (]: 2 of 2 START seed file public.pdl_data ......................................... [RUN]
[0m22:23:18.417043 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now seed.dbt_service.fxf_data)
[0m22:23:18.417327 [debug] [Thread-2 (]: Acquiring new postgres connection 'seed.dbt_service.pdl_data'
[0m22:23:18.417470 [debug] [Thread-1 (]: Began compiling node seed.dbt_service.fxf_data
[0m22:23:18.417610 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.pdl_data
[0m22:23:18.417756 [debug] [Thread-1 (]: Began executing node seed.dbt_service.fxf_data
[0m22:23:18.417887 [debug] [Thread-2 (]: Began executing node seed.dbt_service.pdl_data
[0m22:23:18.901761 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:18.921917 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: BEGIN
[0m22:23:18.938148 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:18.938329 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:23:18.938517 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: BEGIN
[0m22:23:18.938769 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:23:18.950281 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m22:23:18.950447 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m22:23:18.950612 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:18.950763 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:18.950910 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.fxf_data"} */

    create table "finny_db"."public"."fxf_data" ("fxf_id" text,"name" text,"email" text,"company" text,"company_revenue" integer,"title" text,"location" text)
  
[0m22:23:18.951086 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.pdl_data"} */

    create table "finny_db"."public"."pdl_data" ("pdl_id" text,"name" text,"email" text,"company" text,"company_revenue" integer,"title" text,"location" text)
  
[0m22:23:18.957895 [debug] [Thread-2 (]: SQL status: CREATE TABLE in 0.007 seconds
[0m22:23:18.958092 [debug] [Thread-1 (]: SQL status: CREATE TABLE in 0.007 seconds
[0m22:23:26.761173 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:26.764419 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:26.764685 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:26.764930 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:26.828388 [debug] [Thread-2 (]: SQL status: INSERT 0 10000 in 0.063 seconds
[0m22:23:26.846132 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.060 seconds
[0m22:23:34.752355 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:34.771526 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:34.783790 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:34.804653 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:34.843033 [debug] [Thread-2 (]: SQL status: INSERT 0 10000 in 0.059 seconds
[0m22:23:34.863782 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.059 seconds
[0m22:23:42.696654 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:42.715766 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:42.748309 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:42.748542 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:42.781449 [debug] [Thread-2 (]: SQL status: INSERT 0 10000 in 0.059 seconds
[0m22:23:42.805826 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.057 seconds
[0m22:23:50.538647 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:50.557762 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:50.574001 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:50.594565 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:50.628776 [debug] [Thread-2 (]: SQL status: INSERT 0 10000 in 0.055 seconds
[0m22:23:50.655010 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.060 seconds
[0m22:23:58.387019 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:58.399876 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:58.449852 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:58.450171 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m22:23:58.472439 [debug] [Thread-2 (]: SQL status: INSERT 0 10000 in 0.060 seconds
[0m22:23:58.475598 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:58.475776 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m22:23:58.476471 [debug] [Thread-2 (]: SQL status: INSERT 0 5 in 0.001 seconds
[0m22:23:58.479419 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.dbt_service.pdl_data"
[0m22:23:58.488295 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: COMMIT
[0m22:23:58.488668 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m22:23:58.488829 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: COMMIT
[0m22:23:58.490473 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m22:23:58.491513 [debug] [Thread-2 (]: On seed.dbt_service.pdl_data: Close
[0m22:23:58.492493 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfc2710>]}
[0m22:23:58.492763 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file public.pdl_data ..................................... [[32mINSERT 50005[0m in 40.07s]
[0m22:23:58.493007 [debug] [Thread-2 (]: Finished running node seed.dbt_service.pdl_data
[0m22:23:58.498112 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.048 seconds
[0m22:23:58.500904 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:58.501137 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m22:23:58.501821 [debug] [Thread-1 (]: SQL status: INSERT 0 5 in 0.000 seconds
[0m22:23:58.502369 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.dbt_service.fxf_data"
[0m22:23:58.503834 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: COMMIT
[0m22:23:58.503970 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m22:23:58.504088 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: COMMIT
[0m22:23:58.505471 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m22:23:58.505923 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: Close
[0m22:23:58.506228 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f4f0bc0-d670-4208-8ecd-69d733369ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca08a50>]}
[0m22:23:58.506464 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file public.fxf_data ..................................... [[32mINSERT 50005[0m in 40.09s]
[0m22:23:58.506695 [debug] [Thread-1 (]: Finished running node seed.dbt_service.fxf_data
[0m22:23:58.507330 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:58.507486 [debug] [MainThread]: On master: BEGIN
[0m22:23:58.507610 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:23:58.513851 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m22:23:58.514117 [debug] [MainThread]: On master: COMMIT
[0m22:23:58.514229 [debug] [MainThread]: Using postgres connection "master"
[0m22:23:58.514336 [debug] [MainThread]: On master: COMMIT
[0m22:23:58.514686 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:23:58.514833 [debug] [MainThread]: On master: Close
[0m22:23:58.515062 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:23:58.515194 [debug] [MainThread]: Connection 'seed.dbt_service.fxf_data' was properly closed.
[0m22:23:58.515305 [debug] [MainThread]: Connection 'seed.dbt_service.pdl_data' was properly closed.
[0m22:23:58.515450 [info ] [MainThread]: 
[0m22:23:58.515598 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 40.18 seconds (40.18s).
[0m22:23:58.515939 [debug] [MainThread]: Command end result
[0m22:23:58.566112 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:23:58.566915 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:23:58.569590 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:23:58.569731 [info ] [MainThread]: 
[0m22:23:58.569908 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:23:58.570030 [info ] [MainThread]: 
[0m22:23:58.570164 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m22:23:58.571561 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 40.5481, "process_in_blocks": "0", "process_kernel_time": 0.568445, "process_mem_max_rss": "328663040", "process_out_blocks": "0", "process_user_time": 40.522064}
[0m22:23:58.571766 [debug] [MainThread]: Command `dbt seed` succeeded at 22:23:58.571731 after 40.55 seconds
[0m22:23:58.571923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100c84110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d4e990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100cbb710>]}
[0m22:23:58.572086 [debug] [MainThread]: Flushing usage events
[0m22:23:58.788321 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:24:05.094105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11232af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123afc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123aff90>]}


============================== 22:24:05.095988 | 195a08ab-b69c-48a3-be44-80c64f78239e ==============================
[0m22:24:05.095988 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:24:05.096300 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'log_format': 'default', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'no_print': 'None', 'write_json': 'True', 'partial_parse': 'True', 'version_check': 'True', 'introspect': 'True'}
[0m22:24:05.182917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106183e50>]}
[0m22:24:05.212459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f5e950>]}
[0m22:24:05.212953 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:24:05.255651 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:24:05.308712 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:05.308927 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:05.331906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113184450>]}
[0m22:24:05.371019 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:24:05.371849 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:24:05.377789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113170e10>]}
[0m22:24:05.378028 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:24:05.378190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129e80d0>]}
[0m22:24:05.378996 [info ] [MainThread]: 
[0m22:24:05.379149 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:24:05.379265 [info ] [MainThread]: 
[0m22:24:05.379479 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:24:05.381144 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:24:05.404986 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:24:05.405228 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:24:05.405357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:05.426499 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.021 seconds
[0m22:24:05.427167 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:24:05.427880 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:24:05.430461 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:24:05.430630 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:24:05.430746 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:05.439532 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:24:05.439694 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:24:05.439824 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:24:05.443426 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m22:24:05.444034 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:24:05.444752 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:24:05.447330 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.447499 [debug] [MainThread]: On master: BEGIN
[0m22:24:05.447614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:05.455261 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:24:05.455395 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.455568 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:24:05.459040 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:24:05.459617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129d6190>]}
[0m22:24:05.459812 [debug] [MainThread]: On master: ROLLBACK
[0m22:24:05.460397 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.460519 [debug] [MainThread]: On master: BEGIN
[0m22:24:05.461413 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:24:05.461514 [debug] [MainThread]: On master: COMMIT
[0m22:24:05.461610 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.461705 [debug] [MainThread]: On master: COMMIT
[0m22:24:05.462265 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:24:05.462367 [debug] [MainThread]: On master: Close
[0m22:24:05.463330 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:24:05.463671 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:24:05.463525 [info ] [Thread-1 (]: 1 of 4 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:24:05.463910 [info ] [Thread-2 (]: 2 of 4 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:24:05.464191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:24:05.464383 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:24:05.464532 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:24:05.464674 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:24:05.468178 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:24:05.495558 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:24:05.496016 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:24:05.496171 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:24:05.509631 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:24:05.511108 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:24:05.511514 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:24:05.511693 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:24:05.511838 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:24:05.511982 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:24:05.512131 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:24:05.512279 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:24:05.522643 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:24:05.522796 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m22:24:05.522924 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:24:05.523054 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:24:05.523193 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model works with the full PDL dataset from CSV



select
    pdl_id,
    name,
    email,
    company,
    case 
        when company_revenue = '' then null
        else company_revenue::numeric
    end as company_revenue,
    title,
    location
from "finny_db"."public"."pdl_data"
  );
[0m22:24:05.523351 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model works with the full FXF dataset from CSV



select
    fxf_id,
    name,
    email,
    company,
    case 
        when company_revenue = '' then null
        else company_revenue::numeric
    end as company_revenue,
    title,
    location
from "finny_db"."public"."fxf_data"
  );
[0m22:24:05.526999 [debug] [Thread-1 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 18:         when company_revenue = '' then null
                                        ^

[0m22:24:05.527148 [debug] [Thread-2 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 18:         when company_revenue = '' then null
                                        ^

[0m22:24:05.527294 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: ROLLBACK
[0m22:24:05.527468 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: ROLLBACK
[0m22:24:05.528117 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:24:05.528296 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:24:05.531186 [debug] [Thread-1 (]: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:05.532371 [debug] [Thread-2 (]: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:05.532617 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f8510>]}
[0m22:24:05.532778 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '195a08ab-b69c-48a3-be44-80c64f78239e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f54d10>]}
[0m22:24:05.533077 [error] [Thread-1 (]: 1 of 4 ERROR creating sql view model public.stg_fxf_data ....................... [[31mERROR[0m in 0.07s]
[0m22:24:05.533322 [error] [Thread-2 (]: 2 of 4 ERROR creating sql view model public.stg_pdl_data ....................... [[31mERROR[0m in 0.07s]
[0m22:24:05.533634 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:24:05.533864 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:24:05.534057 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_fxf_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql.
[0m22:24:05.534527 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_pdl_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql.
[0m22:24:05.534847 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m22:24:05.535019 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m22:24:05.535166 [info ] [Thread-4 (]: 3 of 4 SKIP relation public.company_analysis ................................... [[33mSKIP[0m]
[0m22:24:05.535376 [info ] [Thread-3 (]: 4 of 4 SKIP relation public.location_analysis .................................. [[33mSKIP[0m]
[0m22:24:05.535568 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m22:24:05.535711 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m22:24:05.536260 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.536371 [debug] [MainThread]: On master: BEGIN
[0m22:24:05.536470 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:05.545913 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m22:24:05.546082 [debug] [MainThread]: On master: COMMIT
[0m22:24:05.546195 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:05.546299 [debug] [MainThread]: On master: COMMIT
[0m22:24:05.546792 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:24:05.546902 [debug] [MainThread]: On master: Close
[0m22:24:05.547056 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:05.547157 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:24:05.547252 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:24:05.547396 [info ] [MainThread]: 
[0m22:24:05.547536 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m22:24:05.547870 [debug] [MainThread]: Command end result
[0m22:24:05.557750 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:24:05.558555 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:24:05.561022 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:24:05.561163 [info ] [MainThread]: 
[0m22:24:05.561325 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m22:24:05.561451 [info ] [MainThread]: 
[0m22:24:05.561605 [error] [MainThread]: [31mFailure in model stg_fxf_data (models/staging/stg_fxf_data.sql)[0m
[0m22:24:05.561753 [error] [MainThread]:   Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:05.561873 [info ] [MainThread]: 
[0m22:24:05.562008 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:05.562125 [info ] [MainThread]: 
[0m22:24:05.562259 [error] [MainThread]: [31mFailure in model stg_pdl_data (models/staging/stg_pdl_data.sql)[0m
[0m22:24:05.562402 [error] [MainThread]:   Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' then null
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:05.562680 [info ] [MainThread]: 
[0m22:24:05.562864 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:05.562979 [info ] [MainThread]: 
[0m22:24:05.563104 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=4
[0m22:24:05.564345 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.50399023, "process_in_blocks": "0", "process_kernel_time": 0.174146, "process_mem_max_rss": "135774208", "process_out_blocks": "0", "process_user_time": 0.929066}
[0m22:24:05.564540 [debug] [MainThread]: Command `dbt run` failed at 22:24:05.564509 after 0.50 seconds
[0m22:24:05.564691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123793d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11385efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123af750>]}
[0m22:24:05.564834 [debug] [MainThread]: Flushing usage events
[0m22:24:05.780881 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:24:45.610889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5b2b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c633890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c633f50>]}


============================== 22:24:45.612926 | a00291d0-cb7b-4f6c-bd6d-83ab17b068fe ==============================
[0m22:24:45.612926 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:24:45.613230 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'quiet': 'False', 'version_check': 'True', 'log_format': 'default', 'use_colors': 'True', 'log_cache_events': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'cache_selected_only': 'False', 'printer_width': '80', 'invocation_command': 'dbt run', 'target_path': 'None', 'no_print': 'None', 'debug': 'False', 'empty': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'use_experimental_parser': 'False'}
[0m22:24:45.699634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c67d550>]}
[0m22:24:45.730676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184f6010>]}
[0m22:24:45.731216 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:24:45.774364 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:24:45.828232 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:24:45.828553 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m22:24:45.828719 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m22:24:45.955811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d98b410>]}
[0m22:24:45.995920 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:24:45.996895 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:24:46.028166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c4d81d0>]}
[0m22:24:46.028467 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:24:46.028651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ccd3a10>]}
[0m22:24:46.029464 [info ] [MainThread]: 
[0m22:24:46.029629 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:24:46.029749 [info ] [MainThread]: 
[0m22:24:46.029942 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:24:46.031612 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:24:46.055451 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:24:46.055736 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:24:46.055857 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:46.075179 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.019 seconds
[0m22:24:46.075774 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:24:46.076518 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:24:46.079116 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:24:46.079274 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:24:46.079391 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.088648 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:24:46.088788 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:24:46.088942 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:24:46.091878 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m22:24:46.092396 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:24:46.093157 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:24:46.095402 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.095533 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.095646 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:46.104840 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m22:24:46.104972 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.105131 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:24:46.109320 [debug] [MainThread]: SQL status: SELECT 2 in 0.004 seconds
[0m22:24:46.109944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c4d8710>]}
[0m22:24:46.110141 [debug] [MainThread]: On master: ROLLBACK
[0m22:24:46.110793 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.110911 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.111824 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:24:46.111940 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.112048 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.112154 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.112621 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:24:46.112746 [debug] [MainThread]: On master: Close
[0m22:24:46.113953 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:24:46.114119 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:24:46.114344 [info ] [Thread-1 (]: 1 of 4 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:24:46.114552 [info ] [Thread-2 (]: 2 of 4 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:24:46.114744 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:24:46.114942 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:24:46.115091 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:24:46.115232 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:24:46.118809 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:24:46.119933 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:24:46.120298 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:24:46.120449 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:24:46.134650 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:24:46.136283 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:24:46.136665 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:24:46.136832 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:24:46.136986 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:24:46.137129 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:24:46.137268 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:24:46.137403 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:24:46.147528 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:24:46.147690 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:24:46.147830 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:24:46.147956 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:24:46.148098 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model works with the full FXF dataset from CSV



select
    fxf_id,
    name,
    email,
    company,
    case 
        when company_revenue = '' or company_revenue is null then null
        else company_revenue::bigint
    end as company_revenue,
    title,
    location
from "finny_db"."public"."fxf_data"
  );
[0m22:24:46.148247 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model works with the full PDL dataset from CSV



select
    pdl_id,
    name,
    email,
    company,
    case 
        when company_revenue = '' or company_revenue is null then null
        else company_revenue::bigint
    end as company_revenue,
    title,
    location
from "finny_db"."public"."pdl_data"
  );
[0m22:24:46.149357 [debug] [Thread-1 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 18:         when company_revenue = '' or company_revenue is null...
                                        ^

[0m22:24:46.149509 [debug] [Thread-2 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 18:         when company_revenue = '' or company_revenue is null...
                                        ^

[0m22:24:46.149668 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: ROLLBACK
[0m22:24:46.149801 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: ROLLBACK
[0m22:24:46.150396 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:24:46.150574 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:24:46.153676 [debug] [Thread-1 (]: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:46.154943 [debug] [Thread-2 (]: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:46.155192 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1afd10>]}
[0m22:24:46.155335 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a00291d0-cb7b-4f6c-bd6d-83ab17b068fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1ce290>]}
[0m22:24:46.155625 [error] [Thread-1 (]: 1 of 4 ERROR creating sql view model public.stg_fxf_data ....................... [[31mERROR[0m in 0.04s]
[0m22:24:46.155894 [error] [Thread-2 (]: 2 of 4 ERROR creating sql view model public.stg_pdl_data ....................... [[31mERROR[0m in 0.04s]
[0m22:24:46.156130 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:24:46.156334 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:24:46.156536 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_fxf_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql.
[0m22:24:46.157029 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_pdl_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql.
[0m22:24:46.157353 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m22:24:46.157514 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m22:24:46.157672 [info ] [Thread-4 (]: 3 of 4 SKIP relation public.company_analysis ................................... [[33mSKIP[0m]
[0m22:24:46.157873 [info ] [Thread-3 (]: 4 of 4 SKIP relation public.location_analysis .................................. [[33mSKIP[0m]
[0m22:24:46.158052 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m22:24:46.158230 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m22:24:46.158851 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.158970 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.159074 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:46.168195 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m22:24:46.168398 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.168536 [debug] [MainThread]: Using postgres connection "master"
[0m22:24:46.168650 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.169237 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:24:46.169367 [debug] [MainThread]: On master: Close
[0m22:24:46.169524 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:46.169625 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:24:46.169720 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:24:46.169878 [info ] [MainThread]: 
[0m22:24:46.170011 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m22:24:46.170359 [debug] [MainThread]: Command end result
[0m22:24:46.180446 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:24:46.181287 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:24:46.183992 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:24:46.184171 [info ] [MainThread]: 
[0m22:24:46.184322 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m22:24:46.184435 [info ] [MainThread]: 
[0m22:24:46.184573 [error] [MainThread]: [31mFailure in model stg_fxf_data (models/staging/stg_fxf_data.sql)[0m
[0m22:24:46.184705 [error] [MainThread]:   Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:46.184809 [info ] [MainThread]: 
[0m22:24:46.184927 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_fxf_data.sql
[0m22:24:46.185027 [info ] [MainThread]: 
[0m22:24:46.185146 [error] [MainThread]: [31mFailure in model stg_pdl_data (models/staging/stg_pdl_data.sql)[0m
[0m22:24:46.185272 [error] [MainThread]:   Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 18:         when company_revenue = '' or company_revenue is null...
                                          ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:46.185370 [info ] [MainThread]: 
[0m22:24:46.185483 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_pdl_data.sql
[0m22:24:46.185585 [info ] [MainThread]: 
[0m22:24:46.185708 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=4
[0m22:24:46.187019 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6130598, "process_in_blocks": "0", "process_kernel_time": 0.185386, "process_mem_max_rss": "141312000", "process_out_blocks": "0", "process_user_time": 1.048162}
[0m22:24:46.187238 [debug] [MainThread]: Command `dbt run` failed at 22:24:46.187204 after 0.61 seconds
[0m22:24:46.187392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10504bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c65e250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5b8910>]}
[0m22:24:46.187549 [debug] [MainThread]: Flushing usage events
[0m22:24:46.458262 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:25:13.693313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145b2f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114633990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114633f90>]}


============================== 22:25:13.695222 | 6ae7dc38-336b-47ba-915e-f15353f49066 ==============================
[0m22:25:13.695222 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:25:13.695513 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'warn_error': 'None', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'static_parser': 'True', 'fail_fast': 'False', 'version_check': 'True', 'debug': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'introspect': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'write_json': 'True'}
[0m22:25:13.780433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107588bd0>]}
[0m22:25:13.811008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f1ad0>]}
[0m22:25:13.811512 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:25:13.854620 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:25:13.907959 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:25:13.908274 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m22:25:13.908452 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m22:25:14.034993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11555b850>]}
[0m22:25:14.073741 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:25:14.074633 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:25:14.109687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f910d0>]}
[0m22:25:14.110023 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:25:14.110197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115beb110>]}
[0m22:25:14.111052 [info ] [MainThread]: 
[0m22:25:14.111212 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:25:14.111330 [info ] [MainThread]: 
[0m22:25:14.111528 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:25:14.113171 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:25:14.132063 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:25:14.132281 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:25:14.132411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:14.153663 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.021 seconds
[0m22:25:14.154428 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:25:14.155234 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:25:14.157991 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:25:14.158179 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:25:14.158308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:14.167409 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m22:25:14.167627 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:25:14.167774 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:25:14.171229 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m22:25:14.171944 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:25:14.172590 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:25:14.175223 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.175390 [debug] [MainThread]: On master: BEGIN
[0m22:25:14.175498 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:25:14.183566 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:25:14.183763 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.183928 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:25:14.187483 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:25:14.188085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116da4210>]}
[0m22:25:14.188268 [debug] [MainThread]: On master: ROLLBACK
[0m22:25:14.188798 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.188902 [debug] [MainThread]: On master: BEGIN
[0m22:25:14.189603 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:25:14.189712 [debug] [MainThread]: On master: COMMIT
[0m22:25:14.189817 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.189923 [debug] [MainThread]: On master: COMMIT
[0m22:25:14.190325 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:25:14.190465 [debug] [MainThread]: On master: Close
[0m22:25:14.191693 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:25:14.191908 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:25:14.192179 [info ] [Thread-1 (]: 1 of 4 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:25:14.192462 [info ] [Thread-2 (]: 2 of 4 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:25:14.192727 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:25:14.192999 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:25:14.193184 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:25:14.193336 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:25:14.196846 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:25:14.198137 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:25:14.198516 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:25:14.204961 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:25:14.213060 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:25:14.214709 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:25:14.215115 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:25:14.215303 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:25:14.215449 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:25:14.215597 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:25:14.215733 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:25:14.215874 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:25:14.225715 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:25:14.225898 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:25:14.226086 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:25:14.226254 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:25:14.226421 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model works with the full FXF dataset from CSV



select
    fxf_id,
    name,
    email,
    company,
    nullif(company_revenue, '')::bigint as company_revenue,
    title,
    location
from "finny_db"."public"."fxf_data"
  );
[0m22:25:14.226588 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model works with the full PDL dataset from CSV



select
    pdl_id,
    name,
    email,
    company,
    nullif(company_revenue, '')::bigint as company_revenue,
    title,
    location
from "finny_db"."public"."pdl_data"
  );
[0m22:25:14.227423 [debug] [Thread-2 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                     ^

[0m22:25:14.227591 [debug] [Thread-1 (]: Postgres adapter: Postgres error: invalid input syntax for type integer: ""
LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                     ^

[0m22:25:14.227743 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: ROLLBACK
[0m22:25:14.227899 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: ROLLBACK
[0m22:25:14.228470 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:25:14.228652 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:25:14.231759 [debug] [Thread-2 (]: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:25:14.233190 [debug] [Thread-1 (]: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:25:14.233585 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11544f990>]}
[0m22:25:14.233828 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ae7dc38-336b-47ba-915e-f15353f49066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d6d1d0>]}
[0m22:25:14.234111 [error] [Thread-2 (]: 2 of 4 ERROR creating sql view model public.stg_pdl_data ....................... [[31mERROR[0m in 0.04s]
[0m22:25:14.234350 [error] [Thread-1 (]: 1 of 4 ERROR creating sql view model public.stg_fxf_data ....................... [[31mERROR[0m in 0.04s]
[0m22:25:14.234668 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:25:14.234894 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:25:14.235117 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_pdl_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql.
[0m22:25:14.235642 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_fxf_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql.
[0m22:25:14.236004 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m22:25:14.236162 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m22:25:14.236358 [info ] [Thread-4 (]: 3 of 4 SKIP relation public.company_analysis ................................... [[33mSKIP[0m]
[0m22:25:14.236540 [info ] [Thread-3 (]: 4 of 4 SKIP relation public.location_analysis .................................. [[33mSKIP[0m]
[0m22:25:14.236735 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m22:25:14.236897 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m22:25:14.237531 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.237660 [debug] [MainThread]: On master: BEGIN
[0m22:25:14.237773 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:25:14.246764 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m22:25:14.246967 [debug] [MainThread]: On master: COMMIT
[0m22:25:14.247091 [debug] [MainThread]: Using postgres connection "master"
[0m22:25:14.247204 [debug] [MainThread]: On master: COMMIT
[0m22:25:14.247686 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:25:14.247812 [debug] [MainThread]: On master: Close
[0m22:25:14.247987 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:25:14.248093 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:25:14.248196 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:25:14.248352 [info ] [MainThread]: 
[0m22:25:14.248502 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m22:25:14.248957 [debug] [MainThread]: Command end result
[0m22:25:14.258744 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:25:14.259432 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:25:14.261986 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:25:14.262133 [info ] [MainThread]: 
[0m22:25:14.262354 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m22:25:14.262516 [info ] [MainThread]: 
[0m22:25:14.262668 [error] [MainThread]: [31mFailure in model stg_pdl_data (models/staging/stg_pdl_data.sql)[0m
[0m22:25:14.262818 [error] [MainThread]:   Database Error in model stg_pdl_data (models/staging/stg_pdl_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_pdl_data.sql
[0m22:25:14.262934 [info ] [MainThread]: 
[0m22:25:14.263064 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_pdl_data.sql
[0m22:25:14.263173 [info ] [MainThread]: 
[0m22:25:14.263300 [error] [MainThread]: [31mFailure in model stg_fxf_data (models/staging/stg_fxf_data.sql)[0m
[0m22:25:14.263437 [error] [MainThread]:   Database Error in model stg_fxf_data (models/staging/stg_fxf_data.sql)
  invalid input syntax for type integer: ""
  LINE 17:     nullif(company_revenue, '')::bigint as company_revenue,
                                       ^
  compiled code at target/run/dbt_service/models/staging/stg_fxf_data.sql
[0m22:25:14.263541 [info ] [MainThread]: 
[0m22:25:14.263653 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_fxf_data.sql
[0m22:25:14.263751 [info ] [MainThread]: 
[0m22:25:14.263862 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=2 NO-OP=0 TOTAL=4
[0m22:25:14.265155 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6061103, "process_in_blocks": "0", "process_kernel_time": 0.164904, "process_mem_max_rss": "140951552", "process_out_blocks": "0", "process_user_time": 1.042978}
[0m22:25:14.265368 [debug] [MainThread]: Command `dbt run` failed at 22:25:14.265335 after 0.61 seconds
[0m22:25:14.265520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11439f6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eabfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145a77d0>]}
[0m22:25:14.265669 [debug] [MainThread]: Flushing usage events
[0m22:25:14.477732 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:26:15.745549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49f650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4b7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4b7f90>]}


============================== 22:26:15.747662 | ab0e3fa8-1a5d-4d52-a79d-e221069b856b ==============================
[0m22:26:15.747662 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:26:15.747974 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'target_path': 'None', 'printer_width': '80', 'use_colors': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'debug': 'False', 'partial_parse': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'warn_error': 'None', 'version_check': 'True', 'indirect_selection': 'eager', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m22:26:15.833279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4fd850>]}
[0m22:26:15.863680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0f8990>]}
[0m22:26:15.864150 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:26:15.907323 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:26:15.960158 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:26:15.960471 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m22:26:15.960636 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m22:26:16.092111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f788090>]}
[0m22:26:16.130917 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:26:16.131852 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:26:16.162902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0e4550>]}
[0m22:26:16.163184 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:26:16.163364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec4cc90>]}
[0m22:26:16.164161 [info ] [MainThread]: 
[0m22:26:16.164322 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:26:16.164440 [info ] [MainThread]: 
[0m22:26:16.164639 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:26:16.166313 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:26:16.185112 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:26:16.185350 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:26:16.185474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:16.205420 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.020 seconds
[0m22:26:16.206147 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:26:16.206962 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m22:26:16.209522 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:26:16.209679 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:26:16.209798 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:16.217820 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m22:26:16.217979 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:26:16.218106 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:26:16.221598 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m22:26:16.222147 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:26:16.222768 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:26:16.225149 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.225273 [debug] [MainThread]: On master: BEGIN
[0m22:26:16.225374 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:16.232817 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:26:16.232988 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.233155 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:26:16.236374 [debug] [MainThread]: SQL status: SELECT 2 in 0.003 seconds
[0m22:26:16.237025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fca55d0>]}
[0m22:26:16.237223 [debug] [MainThread]: On master: ROLLBACK
[0m22:26:16.237703 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.237812 [debug] [MainThread]: On master: BEGIN
[0m22:26:16.239209 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:26:16.239319 [debug] [MainThread]: On master: COMMIT
[0m22:26:16.239425 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.239528 [debug] [MainThread]: On master: COMMIT
[0m22:26:16.240064 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:26:16.240185 [debug] [MainThread]: On master: Close
[0m22:26:16.241324 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:26:16.241497 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:26:16.241710 [info ] [Thread-1 (]: 1 of 4 START sql view model public.stg_fxf_data ................................ [RUN]
[0m22:26:16.241970 [info ] [Thread-2 (]: 2 of 4 START sql view model public.stg_pdl_data ................................ [RUN]
[0m22:26:16.242166 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m22:26:16.242363 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m22:26:16.242523 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:26:16.242672 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:26:16.246157 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:26:16.247230 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:26:16.247628 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:26:16.247903 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:26:16.262430 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:26:16.263968 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:26:16.264389 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.264548 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:26:16.264721 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.264870 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:26:16.265012 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:26:16.265225 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m22:26:16.275104 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:26:16.275255 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:26:16.275389 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.275530 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.275707 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model works with the full FXF dataset from CSV



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public"."fxf_data"
  );
[0m22:26:16.275893 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model works with the full PDL dataset from CSV



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public"."pdl_data"
  );
[0m22:26:16.277877 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m22:26:16.278020 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m22:26:16.280606 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.281838 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.281988 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public"."stg_pdl_data" rename to "stg_pdl_data__dbt_backup"
[0m22:26:16.282133 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public"."stg_fxf_data" rename to "stg_fxf_data__dbt_backup"
[0m22:26:16.283118 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:26:16.283254 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:26:16.284412 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.285452 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.285595 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m22:26:16.285732 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m22:26:16.286762 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:26:16.286895 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:26:16.292910 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:26:16.293486 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:26:16.293637 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.293772 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.293950 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:26:16.294090 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:26:16.295783 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m22:26:16.298739 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public"."stg_pdl_data__dbt_backup"
[0m22:26:16.300532 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:26:16.300682 [debug] [Thread-1 (]: SQL status: COMMIT in 0.006 seconds
[0m22:26:16.300820 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public"."stg_pdl_data__dbt_backup" cascade
[0m22:26:16.301671 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public"."stg_fxf_data__dbt_backup"
[0m22:26:16.301964 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:26:16.302094 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public"."stg_fxf_data__dbt_backup" cascade
[0m22:26:16.303786 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.002 seconds
[0m22:26:16.304658 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:26:16.304809 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m22:26:16.305526 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:26:16.306087 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd4a410>]}
[0m22:26:16.306363 [info ] [Thread-2 (]: 2 of 4 OK created sql view model public.stg_pdl_data ........................... [[32mCREATE VIEW[0m in 0.06s]
[0m22:26:16.306531 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd48c10>]}
[0m22:26:16.306733 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:26:16.306957 [info ] [Thread-1 (]: 1 of 4 OK created sql view model public.stg_fxf_data ........................... [[32mCREATE VIEW[0m in 0.06s]
[0m22:26:16.307268 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:26:16.307600 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m22:26:16.307781 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m22:26:16.308022 [info ] [Thread-4 (]: 3 of 4 START sql table model public.company_analysis ........................... [RUN]
[0m22:26:16.308214 [info ] [Thread-3 (]: 4 of 4 START sql table model public.location_analysis .......................... [RUN]
[0m22:26:16.308419 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m22:26:16.308581 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_service.location_analysis'
[0m22:26:16.308955 [debug] [Thread-4 (]: Began compiling node model.dbt_service.company_analysis
[0m22:26:16.309125 [debug] [Thread-3 (]: Began compiling node model.dbt_service.location_analysis
[0m22:26:16.310746 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m22:26:16.311996 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m22:26:16.312445 [debug] [Thread-4 (]: Began executing node model.dbt_service.company_analysis
[0m22:26:16.312610 [debug] [Thread-3 (]: Began executing node model.dbt_service.location_analysis
[0m22:26:16.321408 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m22:26:16.322816 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m22:26:16.323345 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.323588 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.323730 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: BEGIN
[0m22:26:16.323873 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: BEGIN
[0m22:26:16.324004 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m22:26:16.324129 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m22:26:16.333694 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m22:26:16.333876 [debug] [Thread-3 (]: SQL status: BEGIN in 0.010 seconds
[0m22:26:16.334035 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.334182 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.334362 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m22:26:16.334557 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public"."stg_pdl_data"
),

location_stats as (
    select
        location,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location
)

select
    location,
    contact_count,
    company_count,
    unique_titles,
    data_sources,
    round(avg_revenue::numeric, 2) as avg_revenue
from location_stats
order by contact_count desc
  );
  
[0m22:26:16.410459 [debug] [Thread-4 (]: SQL status: SELECT 91 in 0.076 seconds
[0m22:26:16.413926 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.414097 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public"."company_analysis" rename to "company_analysis__dbt_backup"
[0m22:26:16.414818 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:26:16.416115 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.416272 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m22:26:16.416711 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:26:16.417379 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:26:16.417542 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.417696 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:26:16.418689 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m22:26:16.419663 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public"."company_analysis__dbt_backup"
[0m22:26:16.420927 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:26:16.421078 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public"."company_analysis__dbt_backup" cascade
[0m22:26:16.422077 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m22:26:16.422615 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: Close
[0m22:26:16.422887 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebc20d0>]}
[0m22:26:16.423157 [info ] [Thread-4 (]: 3 of 4 OK created sql table model public.company_analysis ...................... [[32mSELECT 91[0m in 0.11s]
[0m22:26:16.423391 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m22:26:16.459813 [debug] [Thread-3 (]: SQL status: SELECT 76 in 0.125 seconds
[0m22:26:16.461437 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.461669 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public"."location_analysis" rename to "location_analysis__dbt_backup"
[0m22:26:16.462286 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:26:16.464638 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.464863 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m22:26:16.465402 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:26:16.465923 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:26:16.466087 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.466264 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:26:16.466939 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m22:26:16.467905 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public"."location_analysis__dbt_backup"
[0m22:26:16.468228 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:26:16.468365 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public"."location_analysis__dbt_backup" cascade
[0m22:26:16.469333 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m22:26:16.469763 [debug] [Thread-3 (]: On model.dbt_service.location_analysis: Close
[0m22:26:16.470047 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab0e3fa8-1a5d-4d52-a79d-e221069b856b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fdc4090>]}
[0m22:26:16.470520 [info ] [Thread-3 (]: 4 of 4 OK created sql table model public.location_analysis ..................... [[32mSELECT 76[0m in 0.16s]
[0m22:26:16.470851 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m22:26:16.471645 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.471890 [debug] [MainThread]: On master: BEGIN
[0m22:26:16.472048 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:16.478707 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:26:16.478911 [debug] [MainThread]: On master: COMMIT
[0m22:26:16.479025 [debug] [MainThread]: Using postgres connection "master"
[0m22:26:16.479147 [debug] [MainThread]: On master: COMMIT
[0m22:26:16.479485 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:26:16.479676 [debug] [MainThread]: On master: Close
[0m22:26:16.479880 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:16.479996 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:26:16.480103 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:26:16.480213 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m22:26:16.480313 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m22:26:16.480493 [info ] [MainThread]: 
[0m22:26:16.480658 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m22:26:16.481091 [debug] [MainThread]: Command end result
[0m22:26:16.493137 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:26:16.494087 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:26:16.497110 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:26:16.497262 [info ] [MainThread]: 
[0m22:26:16.497445 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:26:16.497588 [info ] [MainThread]: 
[0m22:26:16.497738 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m22:26:16.499277 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7895412, "process_in_blocks": "0", "process_kernel_time": 0.168089, "process_mem_max_rss": "142426112", "process_out_blocks": "0", "process_user_time": 1.09998}
[0m22:26:16.499525 [debug] [MainThread]: Command `dbt run` succeeded at 22:26:16.499486 after 0.79 seconds
[0m22:26:16.499705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e9ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc17c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ee7510>]}
[0m22:26:16.499882 [debug] [MainThread]: Flushing usage events
[0m22:26:16.825079 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:28:31.848045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ab7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ab7e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b3fe50>]}


============================== 22:28:31.850513 | e83cad78-dde7-49f5-aa60-bd13718d74af ==============================
[0m22:28:31.850513 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:28:31.850811 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'empty': 'None', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run-operation query --args {"sql": "SELECT * FROM company_analysis ORDER BY total_employees DESC LIMIT 10"}', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'write_json': 'True', 'target_path': 'None', 'warn_error': 'None', 'use_colors': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'no_print': 'None', 'log_format': 'default', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False', 'printer_width': '80', 'introspect': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m22:28:31.951326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e83cad78-dde7-49f5-aa60-bd13718d74af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b5fcd0>]}
[0m22:28:31.981669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e83cad78-dde7-49f5-aa60-bd13718d74af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039f4c50>]}
[0m22:28:31.982487 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:28:32.028467 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:28:32.088515 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:32.088737 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:32.111503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e83cad78-dde7-49f5-aa60-bd13718d74af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d96810>]}
[0m22:28:32.149580 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:28:32.150723 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:28:32.159892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e83cad78-dde7-49f5-aa60-bd13718d74af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10402ab10>]}
[0m22:28:32.160113 [info ] [MainThread]: Found 4 models, 4 data tests, 4 seeds, 2 sources, 449 macros
[0m22:28:32.160268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e83cad78-dde7-49f5-aa60-bd13718d74af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b21fd0>]}
[0m22:28:32.160529 [debug] [MainThread]: Acquiring new postgres connection 'macro_query'
[0m22:28:32.160677 [debug] [MainThread]: Using postgres connection "macro_query"
[0m22:28:32.160790 [debug] [MainThread]: On macro_query: BEGIN
[0m22:28:32.160904 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:28:32.191141 [debug] [MainThread]: SQL status: BEGIN in 0.030 seconds
[0m22:28:32.191363 [debug] [MainThread]: On macro_query: COMMIT
[0m22:28:32.191482 [debug] [MainThread]: Using postgres connection "macro_query"
[0m22:28:32.191586 [debug] [MainThread]: On macro_query: COMMIT
[0m22:28:32.192269 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m22:28:32.192464 [debug] [MainThread]: On macro_query: Close
[0m22:28:32.192629 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "query" in any package
[0m22:28:32.193197 [debug] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1294, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "query" in any package

[0m22:28:32.193381 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt could not find a macro with the name 'query' in any package. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m22:28:32.195144 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 0.38264957, "process_in_blocks": "0", "process_kernel_time": 0.17702, "process_mem_max_rss": "130957312", "process_out_blocks": "0", "process_user_time": 0.811747}
[0m22:28:32.195364 [debug] [MainThread]: Command `dbt run-operation` failed at 22:28:32.195329 after 0.38 seconds
[0m22:28:32.195487 [debug] [MainThread]: Connection 'macro_query' was properly closed.
[0m22:28:32.195629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dec350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b3f590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072bba10>]}
[0m22:28:32.195772 [debug] [MainThread]: Flushing usage events
[0m22:28:32.536125 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:09.768738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8b3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a213550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a926210>]}


============================== 22:49:09.771257 | 8656dd12-02ec-493f-8271-99654d2aac93 ==============================
[0m22:49:09.771257 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:49:09.771589 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'empty': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_format': 'default', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'target_path': 'None', 'version_check': 'True', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'debug': 'False', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m22:49:09.900870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094871d0>]}
[0m22:49:09.931668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff14d0>]}
[0m22:49:09.932394 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:49:09.978501 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:49:10.024064 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m22:49:10.024350 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:49:10.024530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b932b90>]}
[0m22:49:10.618780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be20f50>]}
[0m22:49:10.684283 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:49:10.685318 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:49:10.696834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4d7d10>]}
[0m22:49:10.697073 [info ] [MainThread]: Found 6 models, 4 seeds, 18 data tests, 2 sources, 449 macros
[0m22:49:10.697228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c483090>]}
[0m22:49:10.698072 [info ] [MainThread]: 
[0m22:49:10.698220 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:49:10.698341 [info ] [MainThread]: 
[0m22:49:10.698539 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:49:10.700180 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:49:10.700391 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:49:10.700594 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:49:10.738848 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:49:10.739042 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:49:10.739183 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:49:10.739377 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:49:10.739517 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:49:10.739650 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:49:10.739791 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:10.739917 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:10.740035 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:10.780300 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.040 seconds
[0m22:49:10.780488 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.041 seconds
[0m22:49:10.780641 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.041 seconds
[0m22:49:10.781232 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:49:10.781676 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:49:10.782065 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:49:10.782556 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now create_finny_db_public_raw)
[0m22:49:10.782698 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now create_finny_db_public_staging)
[0m22:49:10.782871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now create_finny_db_public_marts)
[0m22:49:10.783052 [debug] [ThreadPool]: Creating schema "database: "finny_db"
schema: "public_raw"
"
[0m22:49:10.783222 [debug] [ThreadPool]: Creating schema "database: "finny_db"
schema: "public_staging"
"
[0m22:49:10.783400 [debug] [ThreadPool]: Creating schema "database: "finny_db"
schema: "public_marts"
"
[0m22:49:10.785647 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw"
[0m22:49:10.786369 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging"
[0m22:49:10.787144 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_marts"
[0m22:49:10.787265 [debug] [ThreadPool]: On create_finny_db_public_raw: BEGIN
[0m22:49:10.787398 [debug] [ThreadPool]: On create_finny_db_public_staging: BEGIN
[0m22:49:10.787518 [debug] [ThreadPool]: On create_finny_db_public_marts: BEGIN
[0m22:49:10.787628 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.787735 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.787841 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.799615 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m22:49:10.799758 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging"
[0m22:49:10.799873 [debug] [ThreadPool]: On create_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "create_finny_db_public_staging"} */
create schema if not exists "public_staging"
[0m22:49:10.800140 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m22:49:10.800288 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m22:49:10.800420 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_marts"
[0m22:49:10.800566 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw"
[0m22:49:10.800707 [debug] [ThreadPool]: On create_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "create_finny_db_public_marts"} */
create schema if not exists "public_marts"
[0m22:49:10.800837 [debug] [ThreadPool]: On create_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "create_finny_db_public_raw"} */
create schema if not exists "public_raw"
[0m22:49:10.802316 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.002 seconds
[0m22:49:10.802479 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.001 seconds
[0m22:49:10.802689 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.002 seconds
[0m22:49:10.803059 [debug] [ThreadPool]: On create_finny_db_public_staging: COMMIT
[0m22:49:10.803381 [debug] [ThreadPool]: On create_finny_db_public_raw: COMMIT
[0m22:49:10.803676 [debug] [ThreadPool]: On create_finny_db_public_marts: COMMIT
[0m22:49:10.803811 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging"
[0m22:49:10.803932 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw"
[0m22:49:10.804055 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_marts"
[0m22:49:10.804191 [debug] [ThreadPool]: On create_finny_db_public_staging: COMMIT
[0m22:49:10.804313 [debug] [ThreadPool]: On create_finny_db_public_raw: COMMIT
[0m22:49:10.804423 [debug] [ThreadPool]: On create_finny_db_public_marts: COMMIT
[0m22:49:10.806390 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m22:49:10.806537 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m22:49:10.806704 [debug] [ThreadPool]: On create_finny_db_public_staging: Close
[0m22:49:10.806986 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m22:49:10.807157 [debug] [ThreadPool]: On create_finny_db_public_marts: Close
[0m22:49:10.807353 [debug] [ThreadPool]: On create_finny_db_public_raw: Close
[0m22:49:10.808415 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_finny_db_public_raw, now list_finny_db_public)
[0m22:49:10.808660 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_finny_db_public_staging, now list_finny_db_public_raw)
[0m22:49:10.808874 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_finny_db_public_marts, now list_finny_db_public_staging)
[0m22:49:10.811436 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:49:10.811653 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m22:49:10.812569 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:49:10.813824 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:49:10.813955 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:49:10.814731 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:49:10.814877 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m22:49:10.814995 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m22:49:10.815117 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.815239 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m22:49:10.815349 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.815461 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:10.815676 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:10.825931 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m22:49:10.826136 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m22:49:10.826253 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m22:49:10.826400 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m22:49:10.826544 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:49:10.826665 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:49:10.826781 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:49:10.826897 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:49:10.827042 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m22:49:10.827189 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:49:10.827338 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m22:49:10.827486 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m22:49:10.830568 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.003 seconds
[0m22:49:10.830725 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m22:49:10.830848 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.003 seconds
[0m22:49:10.831015 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.003 seconds
[0m22:49:10.831483 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m22:49:10.831954 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:49:10.832399 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m22:49:10.832784 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m22:49:10.833164 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m22:49:10.833283 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m22:49:10.833403 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:49:10.833527 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m22:49:10.836750 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:10.836920 [debug] [MainThread]: On master: BEGIN
[0m22:49:10.837032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:49:10.844272 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:49:10.844451 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:10.844602 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:49:10.848724 [debug] [MainThread]: SQL status: SELECT 2 in 0.004 seconds
[0m22:49:10.849429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca2b3d0>]}
[0m22:49:10.849632 [debug] [MainThread]: On master: ROLLBACK
[0m22:49:10.849965 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:10.850087 [debug] [MainThread]: On master: BEGIN
[0m22:49:10.850563 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m22:49:10.850702 [debug] [MainThread]: On master: COMMIT
[0m22:49:10.850817 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:10.850920 [debug] [MainThread]: On master: COMMIT
[0m22:49:10.851205 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:49:10.851326 [debug] [MainThread]: On master: Close
[0m22:49:10.852817 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m22:49:10.853000 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m22:49:10.853213 [info ] [Thread-1 (]: 1 of 6 START sql table model public_raw.raw_fxf_data ........................... [RUN]
[0m22:49:10.853442 [info ] [Thread-2 (]: 2 of 6 START sql table model public_raw.raw_pdl_data ........................... [RUN]
[0m22:49:10.853656 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_fxf_data)
[0m22:49:10.853815 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_pdl_data)
[0m22:49:10.853996 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m22:49:10.854139 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m22:49:10.857414 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m22:49:10.858525 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m22:49:10.858942 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m22:49:10.859100 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m22:49:10.873971 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m22:49:10.875383 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m22:49:10.875850 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m22:49:10.876010 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m22:49:10.876143 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m22:49:10.876286 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m22:49:10.876411 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:10.876539 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:10.885380 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:10.885527 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:10.885670 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m22:49:10.885802 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m22:49:10.885941 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m22:49:10.886088 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m22:49:10.939014 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.053 seconds
[0m22:49:10.943959 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m22:49:10.944169 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.058 seconds
[0m22:49:10.944367 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m22:49:10.946676 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m22:49:10.946894 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m22:49:10.947305 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:49:10.947487 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:49:10.953987 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m22:49:10.954612 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m22:49:10.954780 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m22:49:10.954930 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m22:49:10.955074 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m22:49:10.955233 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m22:49:10.961372 [debug] [Thread-1 (]: SQL status: COMMIT in 0.006 seconds
[0m22:49:10.961572 [debug] [Thread-2 (]: SQL status: COMMIT in 0.006 seconds
[0m22:49:10.964639 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m22:49:10.965712 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m22:49:10.967834 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m22:49:10.968166 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m22:49:10.968344 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m22:49:10.968511 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m22:49:10.969051 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m22:49:10.969224 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m22:49:10.970332 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m22:49:10.970854 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m22:49:10.972054 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9920d0>]}
[0m22:49:10.972223 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca7b350>]}
[0m22:49:10.972544 [info ] [Thread-1 (]: 1 of 6 OK created sql table model public_raw.raw_fxf_data ...................... [[32mSELECT 50005[0m in 0.12s]
[0m22:49:10.972823 [info ] [Thread-2 (]: 2 of 6 OK created sql table model public_raw.raw_pdl_data ...................... [[32mSELECT 50005[0m in 0.12s]
[0m22:49:10.973095 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m22:49:10.973306 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m22:49:10.973616 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_fxf_data
[0m22:49:10.973809 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_pdl_data
[0m22:49:10.973999 [info ] [Thread-4 (]: 3 of 6 START sql view model public_staging.stg_fxf_data ........................ [RUN]
[0m22:49:10.974250 [info ] [Thread-1 (]: 4 of 6 START sql view model public_staging.stg_pdl_data ........................ [RUN]
[0m22:49:10.974478 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_fxf_data)
[0m22:49:10.974650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_fxf_data, now model.dbt_service.stg_pdl_data)
[0m22:49:10.974818 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m22:49:10.974971 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m22:49:10.976346 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m22:49:10.977530 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m22:49:10.977921 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_pdl_data
[0m22:49:10.978083 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_fxf_data
[0m22:49:10.985631 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m22:49:10.987177 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m22:49:10.987617 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:49:10.987773 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:49:10.987941 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m22:49:10.988101 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m22:49:10.988247 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:49:10.988387 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:10.998201 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m22:49:10.998383 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:49:10.998552 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:49:10.998701 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:49:10.998863 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
  );
[0m22:49:10.999046 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
  );
[0m22:49:11.000414 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m22:49:11.000587 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m22:49:11.001964 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:49:11.003196 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:49:11.003382 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m22:49:11.003542 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m22:49:11.004091 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:49:11.004245 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:49:11.004768 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:49:11.005247 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:49:11.005391 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:49:11.005537 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:49:11.005679 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m22:49:11.005814 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m22:49:11.006343 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m22:49:11.008112 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m22:49:11.008269 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m22:49:11.009564 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m22:49:11.010473 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m22:49:11.010628 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m22:49:11.010945 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m22:49:11.011121 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m22:49:11.011377 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m22:49:11.011515 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m22:49:11.012014 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: Close
[0m22:49:11.012451 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: Close
[0m22:49:11.012702 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb8e450>]}
[0m22:49:11.012950 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbaea50>]}
[0m22:49:11.013173 [info ] [Thread-1 (]: 4 of 6 OK created sql view model public_staging.stg_pdl_data ................... [[32mCREATE VIEW[0m in 0.04s]
[0m22:49:11.013447 [info ] [Thread-4 (]: 3 of 6 OK created sql view model public_staging.stg_fxf_data ................... [[32mCREATE VIEW[0m in 0.04s]
[0m22:49:11.013687 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_pdl_data
[0m22:49:11.013908 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_fxf_data
[0m22:49:11.014251 [debug] [Thread-3 (]: Began running node model.dbt_service.company_analysis
[0m22:49:11.014430 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m22:49:11.014620 [info ] [Thread-3 (]: 5 of 6 START sql table model public_marts.company_analysis ..................... [RUN]
[0m22:49:11.014847 [info ] [Thread-2 (]: 6 of 6 START sql table model public_marts.location_analysis .................... [RUN]
[0m22:49:11.015040 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.company_analysis)
[0m22:49:11.015204 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_pdl_data, now model.dbt_service.location_analysis)
[0m22:49:11.015352 [debug] [Thread-3 (]: Began compiling node model.dbt_service.company_analysis
[0m22:49:11.015493 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m22:49:11.016916 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m22:49:11.018280 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m22:49:11.018663 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m22:49:11.018812 [debug] [Thread-3 (]: Began executing node model.dbt_service.company_analysis
[0m22:49:11.020124 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m22:49:11.021446 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m22:49:11.021956 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:49:11.022105 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:49:11.022252 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: BEGIN
[0m22:49:11.022388 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m22:49:11.022511 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:49:11.022651 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:11.029253 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m22:49:11.029485 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m22:49:11.029701 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:49:11.029879 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:49:11.030077 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location
)

select
    location,
    contact_count,
    company_count,
    unique_titles,
    data_sources,
    round(avg_revenue::numeric, 2) as avg_revenue
from location_stats
order by contact_count desc
  );
  
[0m22:49:11.030310 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_marts"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m22:49:11.095179 [debug] [Thread-3 (]: SQL status: SELECT 91 in 0.065 seconds
[0m22:49:11.096702 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:49:11.096860 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_marts"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m22:49:11.097422 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:49:11.097951 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:49:11.098107 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:49:11.098249 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: COMMIT
[0m22:49:11.099313 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m22:49:11.100218 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."company_analysis__dbt_backup"
[0m22:49:11.100493 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m22:49:11.100631 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_marts"."company_analysis__dbt_backup" cascade
[0m22:49:11.101010 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.000 seconds
[0m22:49:11.101487 [debug] [Thread-3 (]: On model.dbt_service.company_analysis: Close
[0m22:49:11.101732 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca19c90>]}
[0m22:49:11.101997 [info ] [Thread-3 (]: 5 of 6 OK created sql table model public_marts.company_analysis ................ [[32mSELECT 91[0m in 0.09s]
[0m22:49:11.102228 [debug] [Thread-3 (]: Finished running node model.dbt_service.company_analysis
[0m22:49:11.128778 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.098 seconds
[0m22:49:11.130596 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:49:11.130821 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m22:49:11.131413 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m22:49:11.131886 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:49:11.132034 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:49:11.132211 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m22:49:11.132957 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m22:49:11.134006 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m22:49:11.134427 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m22:49:11.134625 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m22:49:11.135044 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.000 seconds
[0m22:49:11.135528 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m22:49:11.135792 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8656dd12-02ec-493f-8271-99654d2aac93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbadf50>]}
[0m22:49:11.136050 [info ] [Thread-2 (]: 6 of 6 OK created sql table model public_marts.location_analysis ............... [[32mSELECT 76[0m in 0.12s]
[0m22:49:11.136273 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m22:49:11.136918 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:11.137048 [debug] [MainThread]: On master: BEGIN
[0m22:49:11.137153 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:49:11.143394 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m22:49:11.143572 [debug] [MainThread]: On master: COMMIT
[0m22:49:11.143725 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:11.143842 [debug] [MainThread]: On master: COMMIT
[0m22:49:11.144177 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:49:11.144304 [debug] [MainThread]: On master: Close
[0m22:49:11.144467 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:49:11.144583 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m22:49:11.144685 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m22:49:11.144781 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m22:49:11.144876 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m22:49:11.145034 [info ] [MainThread]: 
[0m22:49:11.145195 [info ] [MainThread]: Finished running 4 table models, 2 view models in 0 hours 0 minutes and 0.45 seconds (0.45s).
[0m22:49:11.145685 [debug] [MainThread]: Command end result
[0m22:49:11.161613 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:49:11.162750 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:49:11.166563 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:49:11.166822 [info ] [MainThread]: 
[0m22:49:11.167016 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:49:11.167164 [info ] [MainThread]: 
[0m22:49:11.167320 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m22:49:11.169424 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.440126, "process_in_blocks": "0", "process_kernel_time": 0.238985, "process_mem_max_rss": "146391040", "process_out_blocks": "0", "process_user_time": 1.61772}
[0m22:49:11.169699 [debug] [MainThread]: Command `dbt run` succeeded at 22:49:11.169661 after 1.44 seconds
[0m22:49:11.169899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a927950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d3ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d83590>]}
[0m22:49:11.170081 [debug] [MainThread]: Flushing usage events
[0m22:49:11.549126 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:32.444080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b2bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba7ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba7f90>]}


============================== 22:49:32.446645 | ebcc66a9-76cb-43de-8563-f4d700965c6d ==============================
[0m22:49:32.446645 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:49:32.446936 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'target_path': 'None', 'log_format': 'default', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False', 'use_colors': 'True', 'quiet': 'False', 'version_check': 'True', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'no_print': 'None', 'empty': 'None', 'printer_width': '80', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'static_parser': 'True'}
[0m22:49:32.543877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba4150>]}
[0m22:49:32.575268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102dfd810>]}
[0m22:49:32.575966 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:49:32.622412 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:49:32.685823 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:49:32.686035 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:49:32.709681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a92e10>]}
[0m22:49:32.749312 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:49:32.750642 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:49:32.766421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d18410>]}
[0m22:49:32.766696 [info ] [MainThread]: Found 6 models, 4 seeds, 18 data tests, 2 sources, 449 macros
[0m22:49:32.766863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10725f110>]}
[0m22:49:32.768008 [info ] [MainThread]: 
[0m22:49:32.768170 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:49:32.768288 [info ] [MainThread]: 
[0m22:49:32.768520 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:49:32.770419 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m22:49:32.770681 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m22:49:32.770911 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m22:49:32.771122 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m22:49:32.824829 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:49:32.825040 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:49:32.825202 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:49:32.825348 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:49:32.825503 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m22:49:32.825651 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:49:32.825789 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m22:49:32.825923 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m22:49:32.826055 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:32.826172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:32.826290 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:32.826408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:32.866317 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m22:49:32.866507 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m22:49:32.866636 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m22:49:32.866745 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m22:49:32.866912 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:49:32.867046 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:49:32.867209 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:49:32.867319 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:49:32.867456 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:49:32.867597 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m22:49:32.867739 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m22:49:32.867873 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m22:49:32.871626 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:49:32.871779 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:49:32.871915 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m22:49:32.872421 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m22:49:32.872887 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m22:49:32.873032 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m22:49:32.873539 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:49:32.873981 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m22:49:32.874117 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m22:49:32.874250 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m22:49:32.874540 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:49:32.875340 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m22:49:32.877486 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:32.877616 [debug] [MainThread]: On master: BEGIN
[0m22:49:32.877732 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:49:32.885027 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:49:32.885195 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:32.885368 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:49:32.888837 [debug] [MainThread]: SQL status: SELECT 4 in 0.003 seconds
[0m22:49:32.889583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebcc66a9-76cb-43de-8563-f4d700965c6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7fa90>]}
[0m22:49:32.889777 [debug] [MainThread]: On master: ROLLBACK
[0m22:49:32.890157 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:32.890258 [debug] [MainThread]: On master: BEGIN
[0m22:49:32.890888 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:49:32.890989 [debug] [MainThread]: On master: COMMIT
[0m22:49:32.891085 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:32.891178 [debug] [MainThread]: On master: COMMIT
[0m22:49:32.891589 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:49:32.891690 [debug] [MainThread]: On master: Close
[0m22:49:32.893138 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:49:32.893322 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:49:32.893620 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:49:32.893479 [info ] [Thread-1 (]: 1 of 18 START test not_null_company_analysis_company ........................... [RUN]
[0m22:49:32.893823 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:49:32.893995 [info ] [Thread-2 (]: 2 of 18 START test not_null_location_analysis_location ......................... [RUN]
[0m22:49:32.894179 [info ] [Thread-3 (]: 3 of 18 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m22:49:32.894473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m22:49:32.894634 [info ] [Thread-4 (]: 4 of 18 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m22:49:32.894810 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m22:49:32.894956 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m22:49:32.895085 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:49:32.895223 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m22:49:32.895355 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:49:32.895478 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:49:32.901721 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:49:32.901916 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:49:32.903418 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:49:32.905131 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:49:32.906594 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:49:32.906983 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:49:32.907147 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:49:32.907322 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:49:32.913746 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:49:32.914174 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:49:32.915181 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:49:32.916775 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:49:32.917658 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:49:32.918194 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:49:32.918381 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m22:49:32.918559 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:49:32.918711 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:49:32.918837 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:32.918981 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m22:49:32.919117 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:49:32.919245 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m22:49:32.919456 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:32.919596 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m22:49:32.919733 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:49:32.919928 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:49:32.932309 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m22:49:32.932459 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m22:49:32.932598 [debug] [Thread-2 (]: SQL status: BEGIN in 0.014 seconds
[0m22:49:32.932738 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:49:32.932868 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m22:49:32.932998 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:49:32.933125 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:49:32.933277 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_marts"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.933423 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:49:32.933565 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.933712 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.933881 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.935427 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m22:49:32.935589 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m22:49:32.937226 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m22:49:32.937709 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m22:49:32.938289 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m22:49:32.938421 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m22:49:32.938639 [info ] [Thread-1 (]: 1 of 18 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.04s]
[0m22:49:32.938897 [info ] [Thread-2 (]: 2 of 18 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.04s]
[0m22:49:32.939106 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:49:32.939282 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:49:32.939430 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:49:32.939604 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:49:32.939759 [info ] [Thread-1 (]: 5 of 18 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m22:49:32.939921 [info ] [Thread-2 (]: 6 of 18 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m22:49:32.940091 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m22:49:32.940232 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m22:49:32.940360 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:49:32.940483 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:49:32.941978 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:49:32.942115 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.008 seconds
[0m22:49:32.942234 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.008 seconds
[0m22:49:32.943727 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:49:32.944452 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m22:49:32.945241 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m22:49:32.945608 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:49:32.946681 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:49:32.946835 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m22:49:32.946987 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m22:49:32.947120 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:49:32.947404 [info ] [Thread-3 (]: 3 of 18 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m22:49:32.947661 [info ] [Thread-4 (]: 4 of 18 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m22:49:32.948639 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:49:32.948801 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:49:32.949004 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:49:32.949191 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:49:32.949369 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m22:49:32.949538 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_email.526f6609b7
[0m22:49:32.949744 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:49:32.949902 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:49:32.950066 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:32.950222 [info ] [Thread-3 (]: 7 of 18 START test not_null_stg_pdl_data_email ................................. [RUN]
[0m22:49:32.950411 [info ] [Thread-4 (]: 8 of 18 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m22:49:32.950569 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m22:49:32.950814 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_email.526f6609b7)
[0m22:49:32.950984 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m22:49:32.951138 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:32.951286 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_email.526f6609b7
[0m22:49:32.951425 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:49:32.953335 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_email.526f6609b7"
[0m22:49:32.955543 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:49:32.956030 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:49:32.956175 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_email.526f6609b7
[0m22:49:32.957170 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:49:32.958084 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_email.526f6609b7"
[0m22:49:32.958419 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:49:32.958572 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_email.526f6609b7"
[0m22:49:32.958722 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m22:49:32.958935 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_email.526f6609b7: BEGIN
[0m22:49:32.959193 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:49:32.959377 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:49:32.960832 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m22:49:32.960988 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m22:49:32.961122 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:49:32.961271 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:49:32.961429 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.961584 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.966219 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.004 seconds
[0m22:49:32.966393 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.005 seconds
[0m22:49:32.967159 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m22:49:32.967662 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m22:49:32.968122 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m22:49:32.968269 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m22:49:32.968431 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:32.968582 [debug] [Thread-4 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:32.968865 [info ] [Thread-2 (]: 6 of 18 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.03s]
[0m22:49:32.969092 [info ] [Thread-1 (]: 5 of 18 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.03s]
[0m22:49:32.969254 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_email.526f6609b7"
[0m22:49:32.969405 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:49:32.969624 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:49:32.969827 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:49:32.969991 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_email.526f6609b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_email.526f6609b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_pdl_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.970146 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.970320 [debug] [Thread-2 (]: Began running node test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d
[0m22:49:32.970537 [debug] [Thread-1 (]: Began running node test.dbt_service.source_not_null_public_pdl_data_id.ed56406345
[0m22:49:32.970788 [info ] [Thread-2 (]: 9 of 18 START test source_not_null_public_fxf_data_id .......................... [RUN]
[0m22:49:32.970959 [info ] [Thread-1 (]: 10 of 18 START test source_not_null_public_pdl_data_id ......................... [RUN]
[0m22:49:32.971162 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d)
[0m22:49:32.971330 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.source_not_null_public_pdl_data_id.ed56406345)
[0m22:49:32.971481 [debug] [Thread-2 (]: Began compiling node test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d
[0m22:49:32.971638 [debug] [Thread-1 (]: Began compiling node test.dbt_service.source_not_null_public_pdl_data_id.ed56406345
[0m22:49:32.973604 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d"
[0m22:49:32.973766 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.003 seconds
[0m22:49:32.973926 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.003 seconds
[0m22:49:32.976594 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.source_not_null_public_pdl_data_id.ed56406345"
[0m22:49:32.977220 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_email.526f6609b7: ROLLBACK
[0m22:49:32.977723 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m22:49:32.978016 [debug] [Thread-2 (]: Began executing node test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d
[0m22:49:32.979263 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d"
[0m22:49:32.979439 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_email.526f6609b7: Close
[0m22:49:32.979596 [debug] [Thread-1 (]: Began executing node test.dbt_service.source_not_null_public_pdl_data_id.ed56406345
[0m22:49:32.979724 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m22:49:32.981042 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.source_not_null_public_pdl_data_id.ed56406345"
[0m22:49:32.980015 [error] [Thread-3 (]: 7 of 18 FAIL 2553 not_null_stg_pdl_data_email .................................. [[31mFAIL 2553[0m in 0.03s]
[0m22:49:32.981490 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d"
[0m22:49:32.981799 [info ] [Thread-4 (]: 8 of 18 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.03s]
[0m22:49:32.982072 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_email.526f6609b7
[0m22:49:32.982231 [debug] [Thread-2 (]: On test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d: BEGIN
[0m22:49:32.982451 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:49:32.982626 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.source_not_null_public_pdl_data_id.ed56406345"
[0m22:49:32.982805 [debug] [Thread-3 (]: Began running node test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd
[0m22:49:32.982993 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:32.983158 [debug] [Thread-4 (]: Began running node test.dbt_service.source_unique_public_pdl_data_id.d276d66e43
[0m22:49:32.983324 [debug] [Thread-1 (]: On test.dbt_service.source_not_null_public_pdl_data_id.ed56406345: BEGIN
[0m22:49:32.983477 [info ] [Thread-3 (]: 11 of 18 START test source_unique_public_fxf_data_id ........................... [RUN]
[0m22:49:32.983715 [info ] [Thread-4 (]: 12 of 18 START test source_unique_public_pdl_data_id ........................... [RUN]
[0m22:49:32.983865 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:32.984030 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_email.526f6609b7, now test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd)
[0m22:49:32.984217 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.source_unique_public_pdl_data_id.d276d66e43)
[0m22:49:32.984446 [debug] [Thread-3 (]: Began compiling node test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd
[0m22:49:32.984590 [debug] [Thread-4 (]: Began compiling node test.dbt_service.source_unique_public_pdl_data_id.d276d66e43
[0m22:49:32.987197 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd"
[0m22:49:32.988570 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.source_unique_public_pdl_data_id.d276d66e43"
[0m22:49:32.989016 [debug] [Thread-4 (]: Began executing node test.dbt_service.source_unique_public_pdl_data_id.d276d66e43
[0m22:49:32.989176 [debug] [Thread-3 (]: Began executing node test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd
[0m22:49:32.990395 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.source_unique_public_pdl_data_id.d276d66e43"
[0m22:49:32.990559 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m22:49:32.990717 [debug] [Thread-2 (]: SQL status: BEGIN in 0.008 seconds
[0m22:49:32.991709 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd"
[0m22:49:32.991913 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.source_not_null_public_pdl_data_id.ed56406345"
[0m22:49:32.992094 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d"
[0m22:49:32.992290 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.source_unique_public_pdl_data_id.d276d66e43"
[0m22:49:32.992455 [debug] [Thread-1 (]: On test.dbt_service.source_not_null_public_pdl_data_id.ed56406345: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.source_not_null_public_pdl_data_id.ed56406345"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "finny_db"."public"."pdl_data"
where id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.992624 [debug] [Thread-2 (]: On test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "finny_db"."public"."fxf_data"
where id is null



  
  
      
    ) dbt_internal_test
[0m22:49:32.992773 [debug] [Thread-4 (]: On test.dbt_service.source_unique_public_pdl_data_id.d276d66e43: BEGIN
[0m22:49:32.992942 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd"
[0m22:49:32.993138 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:49:32.993282 [debug] [Thread-3 (]: On test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd: BEGIN
[0m22:49:32.993494 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:49:32.994064 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 16: select id
                ^

[0m22:49:32.994210 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 16: select id
                ^

[0m22:49:32.994375 [debug] [Thread-2 (]: On test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d: ROLLBACK
[0m22:49:32.994619 [debug] [Thread-1 (]: On test.dbt_service.source_not_null_public_pdl_data_id.ed56406345: ROLLBACK
[0m22:49:32.995351 [debug] [Thread-2 (]: On test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d: Close
[0m22:49:32.995599 [debug] [Thread-1 (]: On test.dbt_service.source_not_null_public_pdl_data_id.ed56406345: Close
[0m22:49:33.001867 [debug] [Thread-1 (]: Database Error in test source_not_null_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_pdl_data_id.sql
[0m22:49:33.002489 [debug] [Thread-2 (]: Database Error in test source_not_null_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_fxf_data_id.sql
[0m22:49:33.002740 [error] [Thread-1 (]: 10 of 18 ERROR source_not_null_public_pdl_data_id .............................. [[31mERROR[0m in 0.03s]
[0m22:49:33.002962 [error] [Thread-2 (]: 9 of 18 ERROR source_not_null_public_fxf_data_id ............................... [[31mERROR[0m in 0.03s]
[0m22:49:33.003217 [debug] [Thread-1 (]: Finished running node test.dbt_service.source_not_null_public_pdl_data_id.ed56406345
[0m22:49:33.003454 [debug] [Thread-2 (]: Finished running node test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d
[0m22:49:33.003659 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:49:33.003854 [debug] [Thread-7 (]: Marking all children of 'test.dbt_service.source_not_null_public_pdl_data_id.ed56406345' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_pdl_data_id.sql.
[0m22:49:33.004033 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:49:33.004204 [info ] [Thread-1 (]: 13 of 18 START test unique_company_analysis_company ............................ [RUN]
[0m22:49:33.004727 [debug] [Thread-7 (]: Marking all children of 'test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_fxf_data_id.sql.
[0m22:49:33.004907 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m22:49:33.005053 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m22:49:33.005208 [info ] [Thread-2 (]: 14 of 18 START test unique_location_analysis_location .......................... [RUN]
[0m22:49:33.005413 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.source_not_null_public_pdl_data_id.ed56406345, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m22:49:33.005630 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.source_unique_public_pdl_data_id.d276d66e43"
[0m22:49:33.005786 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd"
[0m22:49:33.005945 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.source_not_null_public_fxf_data_id.8a047cdf7d, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m22:49:33.006103 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:49:33.006274 [debug] [Thread-4 (]: On test.dbt_service.source_unique_public_pdl_data_id.d276d66e43: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.source_unique_public_pdl_data_id.d276d66e43"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "finny_db"."public"."pdl_data"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.006440 [debug] [Thread-3 (]: On test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "finny_db"."public"."fxf_data"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.006602 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:49:33.008353 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:49:33.009987 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:49:33.010158 [debug] [Thread-4 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 15:     id as unique_field,
             ^

[0m22:49:33.010315 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 15:     id as unique_field,
             ^

[0m22:49:33.010557 [debug] [Thread-4 (]: On test.dbt_service.source_unique_public_pdl_data_id.d276d66e43: ROLLBACK
[0m22:49:33.010745 [debug] [Thread-3 (]: On test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd: ROLLBACK
[0m22:49:33.010918 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:49:33.011054 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:49:33.012092 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:49:33.012281 [debug] [Thread-4 (]: On test.dbt_service.source_unique_public_pdl_data_id.d276d66e43: Close
[0m22:49:33.012450 [debug] [Thread-3 (]: On test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd: Close
[0m22:49:33.013446 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:49:33.014861 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:49:33.015090 [debug] [Thread-1 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m22:49:33.015373 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:49:33.016698 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:33.017333 [debug] [Thread-4 (]: Database Error in test source_unique_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_pdl_data_id.sql
[0m22:49:33.017839 [debug] [Thread-3 (]: Database Error in test source_unique_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_fxf_data_id.sql
[0m22:49:33.017991 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m22:49:33.018245 [error] [Thread-4 (]: 12 of 18 ERROR source_unique_public_pdl_data_id ................................ [[31mERROR[0m in 0.03s]
[0m22:49:33.018609 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:33.018456 [error] [Thread-3 (]: 11 of 18 ERROR source_unique_public_fxf_data_id ................................ [[31mERROR[0m in 0.03s]
[0m22:49:33.018905 [debug] [Thread-4 (]: Finished running node test.dbt_service.source_unique_public_pdl_data_id.d276d66e43
[0m22:49:33.019205 [debug] [Thread-3 (]: Finished running node test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd
[0m22:49:33.019412 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:49:33.019604 [debug] [Thread-7 (]: Marking all children of 'test.dbt_service.source_unique_public_pdl_data_id.d276d66e43' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_pdl_data_id.sql.
[0m22:49:33.019784 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:49:33.019944 [info ] [Thread-4 (]: 15 of 18 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m22:49:33.020177 [debug] [Thread-7 (]: Marking all children of 'test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_fxf_data_id.sql.
[0m22:49:33.020342 [info ] [Thread-3 (]: 16 of 18 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m22:49:33.020557 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.source_unique_public_pdl_data_id.d276d66e43, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m22:49:33.020783 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.source_unique_public_fxf_data_id.6c1a1dc1fd, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m22:49:33.021001 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:49:33.021204 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:49:33.023046 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:49:33.024648 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:49:33.025011 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:49:33.026022 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:49:33.026217 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:49:33.027190 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:49:33.027373 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m22:49:33.027518 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:33.027741 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:49:33.027905 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:49:33.028060 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:49:33.028187 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:49:33.028325 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m22:49:33.028476 [debug] [Thread-1 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.028632 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m22:49:33.028828 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.029009 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:49:33.029226 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:49:33.030661 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m22:49:33.030813 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.001 seconds
[0m22:49:33.031455 [debug] [Thread-1 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m22:49:33.031930 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m22:49:33.032513 [debug] [Thread-1 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m22:49:33.032690 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m22:49:33.032945 [info ] [Thread-1 (]: 13 of 18 PASS unique_company_analysis_company .................................. [[32mPASS[0m in 0.03s]
[0m22:49:33.033285 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:49:33.033728 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:49:33.033555 [info ] [Thread-2 (]: 14 of 18 PASS unique_location_analysis_location ................................ [[32mPASS[0m in 0.03s]
[0m22:49:33.033957 [info ] [Thread-1 (]: 17 of 18 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m22:49:33.034180 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:49:33.034349 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m22:49:33.034498 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:49:33.034653 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:49:33.034797 [info ] [Thread-2 (]: 18 of 18 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m22:49:33.036569 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:49:33.036807 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m22:49:33.037001 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:49:33.038540 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:49:33.038704 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m22:49:33.038879 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m22:49:33.039040 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:49:33.039287 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:49:33.039472 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:49:33.040528 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:49:33.040668 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:49:33.040852 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.041040 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.042087 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:49:33.042353 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:49:33.042502 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m22:49:33.042633 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:33.042845 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:49:33.042977 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m22:49:33.043097 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:49:33.050152 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m22:49:33.050336 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m22:49:33.050501 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:49:33.050653 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:49:33.050814 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.050979 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:49:33.055000 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.013 seconds
[0m22:49:33.055672 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m22:49:33.056082 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m22:49:33.056362 [info ] [Thread-4 (]: 15 of 18 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m22:49:33.056615 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:49:33.056767 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.015 seconds
[0m22:49:33.057367 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m22:49:33.057964 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m22:49:33.058205 [info ] [Thread-3 (]: 16 of 18 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.04s]
[0m22:49:33.058429 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:49:33.065682 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.015 seconds
[0m22:49:33.066281 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m22:49:33.066436 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.015 seconds
[0m22:49:33.066988 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m22:49:33.067132 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m22:49:33.067412 [info ] [Thread-1 (]: 17 of 18 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m22:49:33.067625 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m22:49:33.067851 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:49:33.068127 [info ] [Thread-2 (]: 18 of 18 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.03s]
[0m22:49:33.068433 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:49:33.069087 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:33.069318 [debug] [MainThread]: On master: BEGIN
[0m22:49:33.069440 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:49:33.075434 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m22:49:33.075652 [debug] [MainThread]: On master: COMMIT
[0m22:49:33.075789 [debug] [MainThread]: Using postgres connection "master"
[0m22:49:33.075905 [debug] [MainThread]: On master: COMMIT
[0m22:49:33.076271 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:49:33.076432 [debug] [MainThread]: On master: Close
[0m22:49:33.076618 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:49:33.076740 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m22:49:33.076855 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m22:49:33.076946 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m22:49:33.077035 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m22:49:33.077175 [info ] [MainThread]: 
[0m22:49:33.077317 [info ] [MainThread]: Finished running 18 data tests in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m22:49:33.078335 [debug] [MainThread]: Command end result
[0m22:49:33.090781 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:49:33.091770 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:49:33.094637 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:49:33.094776 [info ] [MainThread]: 
[0m22:49:33.094942 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 0 warnings:[0m
[0m22:49:33.095060 [info ] [MainThread]: 
[0m22:49:33.095221 [error] [MainThread]: [31mFailure in test not_null_stg_pdl_data_email (models/staging/schema.yml)[0m
[0m22:49:33.095361 [error] [MainThread]:   Got 2553 results, configured to fail if != 0
[0m22:49:33.095470 [info ] [MainThread]: 
[0m22:49:33.095601 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/schema.yml/not_null_stg_pdl_data_email.sql
[0m22:49:33.095772 [info ] [MainThread]: 
[0m22:49:33.095991 [error] [MainThread]: [31mFailure in test source_not_null_public_pdl_data_id (models/staging/_sources.yml)[0m
[0m22:49:33.096147 [error] [MainThread]:   Database Error in test source_not_null_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_pdl_data_id.sql
[0m22:49:33.096276 [info ] [MainThread]: 
[0m22:49:33.096416 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/_sources.yml/source_not_null_public_pdl_data_id.sql
[0m22:49:33.096529 [info ] [MainThread]: 
[0m22:49:33.096663 [error] [MainThread]: [31mFailure in test source_not_null_public_fxf_data_id (models/staging/_sources.yml)[0m
[0m22:49:33.096804 [error] [MainThread]:   Database Error in test source_not_null_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_not_null_public_fxf_data_id.sql
[0m22:49:33.096912 [info ] [MainThread]: 
[0m22:49:33.097037 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/_sources.yml/source_not_null_public_fxf_data_id.sql
[0m22:49:33.097143 [info ] [MainThread]: 
[0m22:49:33.097284 [error] [MainThread]: [31mFailure in test source_unique_public_pdl_data_id (models/staging/_sources.yml)[0m
[0m22:49:33.097423 [error] [MainThread]:   Database Error in test source_unique_public_pdl_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_pdl_data_id.sql
[0m22:49:33.097530 [info ] [MainThread]: 
[0m22:49:33.097654 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/_sources.yml/source_unique_public_pdl_data_id.sql
[0m22:49:33.097762 [info ] [MainThread]: 
[0m22:49:33.097890 [error] [MainThread]: [31mFailure in test source_unique_public_fxf_data_id (models/staging/_sources.yml)[0m
[0m22:49:33.098022 [error] [MainThread]:   Database Error in test source_unique_public_fxf_data_id (models/staging/_sources.yml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/dbt_service/models/staging/_sources.yml/source_unique_public_fxf_data_id.sql
[0m22:49:33.098130 [info ] [MainThread]: 
[0m22:49:33.098252 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/_sources.yml/source_unique_public_fxf_data_id.sql
[0m22:49:33.098368 [info ] [MainThread]: 
[0m22:49:33.098493 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=5 SKIP=0 NO-OP=0 TOTAL=18
[0m22:49:33.100406 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.6920914, "process_in_blocks": "0", "process_kernel_time": 0.22619, "process_mem_max_rss": "140689408", "process_out_blocks": "0", "process_user_time": 1.113665}
[0m22:49:33.100635 [debug] [MainThread]: Command `dbt test` failed at 22:49:33.100599 after 0.69 seconds
[0m22:49:33.100796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106913410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d04190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d99150>]}
[0m22:49:33.100962 [debug] [MainThread]: Flushing usage events
[0m22:49:33.328938 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:50:11.818872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11282ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128a7710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128a7e10>]}


============================== 22:50:11.821471 | 48485e09-87e5-45e4-804d-3f40f728bc1b ==============================
[0m22:50:11.821471 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:50:11.821761 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test', 'empty': 'None', 'warn_error': 'None', 'use_experimental_parser': 'False', 'quiet': 'False', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'no_print': 'None', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'indirect_selection': 'eager', 'version_check': 'True', 'log_cache_events': 'False', 'log_format': 'default', 'use_colors': 'True', 'target_path': 'None', 'debug': 'False'}
[0m22:50:11.920157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129ae410>]}
[0m22:50:11.952742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c75110>]}
[0m22:50:11.953622 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:50:12.000647 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:50:12.063332 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:50:12.063698 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/_sources.yml
[0m22:50:12.063919 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/schema.yml
[0m22:50:12.326375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112dcf190>]}
[0m22:50:12.364480 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:50:12.365628 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:50:12.381254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ba8a10>]}
[0m22:50:12.381536 [info ] [MainThread]: Found 6 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m22:50:12.381703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d09010>]}
[0m22:50:12.382735 [info ] [MainThread]: 
[0m22:50:12.382885 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:50:12.382996 [info ] [MainThread]: 
[0m22:50:12.383195 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:50:12.384961 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m22:50:12.385202 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m22:50:12.389521 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m22:50:12.389785 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m22:50:12.412968 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:50:12.413149 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:50:12.413317 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:50:12.413454 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:50:12.413625 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m22:50:12.413788 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m22:50:12.413914 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:50:12.414041 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m22:50:12.414192 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:12.414316 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:12.414445 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:12.414572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:12.450972 [debug] [ThreadPool]: SQL status: BEGIN in 0.037 seconds
[0m22:50:12.451139 [debug] [ThreadPool]: SQL status: BEGIN in 0.037 seconds
[0m22:50:12.451295 [debug] [ThreadPool]: SQL status: BEGIN in 0.037 seconds
[0m22:50:12.451445 [debug] [ThreadPool]: SQL status: BEGIN in 0.037 seconds
[0m22:50:12.451594 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:50:12.451731 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:50:12.451864 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:50:12.451995 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:50:12.452135 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m22:50:12.452289 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m22:50:12.452446 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m22:50:12.452599 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:50:12.456627 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m22:50:12.456767 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:50:12.456876 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:50:12.456990 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:50:12.457556 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:50:12.457974 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m22:50:12.458364 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m22:50:12.458722 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m22:50:12.459558 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:50:12.459674 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m22:50:12.459795 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m22:50:12.459884 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m22:50:12.462871 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.462995 [debug] [MainThread]: On master: BEGIN
[0m22:50:12.463094 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:50:12.471589 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:50:12.471699 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.471854 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:50:12.476117 [debug] [MainThread]: SQL status: SELECT 4 in 0.004 seconds
[0m22:50:12.476896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48485e09-87e5-45e4-804d-3f40f728bc1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a7d350>]}
[0m22:50:12.477103 [debug] [MainThread]: On master: ROLLBACK
[0m22:50:12.477717 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.477829 [debug] [MainThread]: On master: BEGIN
[0m22:50:12.478746 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m22:50:12.478862 [debug] [MainThread]: On master: COMMIT
[0m22:50:12.478972 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.479071 [debug] [MainThread]: On master: COMMIT
[0m22:50:12.479683 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m22:50:12.479795 [debug] [MainThread]: On master: Close
[0m22:50:12.481159 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:50:12.481335 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:50:12.481612 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:50:12.481775 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:50:12.481486 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m22:50:12.481944 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m22:50:12.482138 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m22:50:12.482321 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m22:50:12.482511 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m22:50:12.482667 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m22:50:12.482913 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m22:50:12.483087 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m22:50:12.483260 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:50:12.483422 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:50:12.483572 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:50:12.483717 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:50:12.490699 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:50:12.492366 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:50:12.494355 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:50:12.496138 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:50:12.496629 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:50:12.496902 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:50:12.503199 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:50:12.504272 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:50:12.504412 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:50:12.505527 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:50:12.506458 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:50:12.507423 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:50:12.507718 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:50:12.507968 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m22:50:12.508128 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:50:12.508275 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:50:12.508432 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:50:12.508586 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:50:12.508733 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m22:50:12.508877 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m22:50:12.509091 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m22:50:12.509230 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:50:12.509365 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:50:12.509500 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:50:12.520427 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m22:50:12.520599 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m22:50:12.520752 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m22:50:12.520903 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m22:50:12.521072 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m22:50:12.521230 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m22:50:12.521387 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m22:50:12.521541 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m22:50:12.521710 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.521870 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.522029 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_marts"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.522199 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.523807 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.001 seconds
[0m22:50:12.523933 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m22:50:12.525652 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m22:50:12.526180 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m22:50:12.527359 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m22:50:12.527503 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m22:50:12.527740 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m22:50:12.528005 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m22:50:12.528221 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m22:50:12.528401 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m22:50:12.528550 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:50:12.528712 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.006 seconds
[0m22:50:12.528871 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:50:12.529055 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.007 seconds
[0m22:50:12.529185 [info ] [Thread-2 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m22:50:12.529658 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m22:50:12.529803 [info ] [Thread-1 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m22:50:12.530264 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m22:50:12.530428 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m22:50:12.530602 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m22:50:12.530760 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:50:12.530887 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:50:12.531020 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m22:50:12.531149 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m22:50:12.532944 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:50:12.534459 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:50:12.534763 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m22:50:12.535009 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m22:50:12.535276 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m22:50:12.535484 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m22:50:12.535685 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:50:12.535845 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:50:12.535980 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:50:12.536116 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:50:12.536277 [info ] [Thread-3 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m22:50:12.538149 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:50:12.539070 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:50:12.539223 [info ] [Thread-4 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m22:50:12.539406 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m22:50:12.539654 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m22:50:12.539824 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:50:12.540000 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:50:12.540155 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:50:12.540311 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:50:12.542180 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:50:12.544848 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:50:12.545070 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m22:50:12.545235 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m22:50:12.545620 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:50:12.545786 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:50:12.545997 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:50:12.546164 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:50:12.547412 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:50:12.548465 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:50:12.549015 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:50:12.549245 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:50:12.549416 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m22:50:12.549568 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m22:50:12.549707 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:50:12.549849 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:50:12.554327 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m22:50:12.554512 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m22:50:12.554659 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.557669 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.003 seconds
[0m22:50:12.558359 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m22:50:12.558704 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m22:50:12.558970 [info ] [Thread-2 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.03s]
[0m22:50:12.559234 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m22:50:12.559424 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:50:12.559867 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m22:50:12.559716 [info ] [Thread-2 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m22:50:12.560081 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m22:50:12.560290 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m22:50:12.560478 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.560636 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m22:50:12.560774 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m22:50:12.560939 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:50:12.561134 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m22:50:12.561290 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m22:50:12.563158 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:50:12.563367 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.563537 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m22:50:12.563829 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m22:50:12.564457 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m22:50:12.564628 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.001 seconds
[0m22:50:12.565181 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m22:50:12.565351 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:50:12.565488 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m22:50:12.566663 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:50:12.566845 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m22:50:12.567003 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.003 seconds
[0m22:50:12.567351 [info ] [Thread-1 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.04s]
[0m22:50:12.567562 [info ] [Thread-4 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.03s]
[0m22:50:12.568299 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m22:50:12.568487 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:50:12.568708 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m22:50:12.568931 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m22:50:12.569108 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m22:50:12.569298 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:50:12.569458 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m22:50:12.569643 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:50:12.569803 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:50:12.569963 [info ] [Thread-1 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m22:50:12.570173 [info ] [Thread-4 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m22:50:12.570407 [info ] [Thread-3 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.03s]
[0m22:50:12.570740 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m22:50:12.570921 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m22:50:12.571150 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m22:50:12.571296 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:50:12.571443 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:50:12.571609 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:50:12.573314 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:50:12.575372 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:50:12.575612 [info ] [Thread-3 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m22:50:12.575914 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m22:50:12.576114 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:50:12.576273 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m22:50:12.576440 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:50:12.576609 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:50:12.578365 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:50:12.578557 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m22:50:12.580654 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:50:12.581642 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:50:12.581873 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.582183 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:50:12.583236 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:50:12.583413 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:50:12.583567 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.001 seconds
[0m22:50:12.583720 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:50:12.583904 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m22:50:12.584564 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m22:50:12.584728 [debug] [Thread-1 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m22:50:12.584875 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:50:12.585009 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m22:50:12.585161 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:50:12.585299 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m22:50:12.585499 [debug] [Thread-2 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m22:50:12.585691 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:50:12.585923 [info ] [Thread-2 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.03s]
[0m22:50:12.586234 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m22:50:12.586407 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:50:12.586628 [info ] [Thread-2 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m22:50:12.586852 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m22:50:12.587031 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:50:12.588758 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:50:12.589059 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:50:12.589916 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:50:12.590168 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:50:12.590313 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m22:50:12.590441 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:50:12.594859 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m22:50:12.595189 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m22:50:12.595363 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m22:50:12.595548 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m22:50:12.595707 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m22:50:12.595852 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m22:50:12.596011 [debug] [Thread-1 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.596178 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.596344 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.601195 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m22:50:12.602237 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m22:50:12.602464 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m22:50:12.615828 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.019 seconds
[0m22:50:12.616592 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m22:50:12.617326 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.021 seconds
[0m22:50:12.617667 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m22:50:12.618213 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m22:50:12.618497 [info ] [Thread-3 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m22:50:12.618899 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m22:50:12.619087 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m22:50:12.619754 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.023 seconds
[0m22:50:12.619555 [info ] [Thread-4 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.05s]
[0m22:50:12.620571 [debug] [Thread-1 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m22:50:12.620830 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m22:50:12.621372 [debug] [Thread-1 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m22:50:12.621659 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.019 seconds
[0m22:50:12.621904 [info ] [Thread-1 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.05s]
[0m22:50:12.622420 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m22:50:12.622634 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m22:50:12.623152 [debug] [Thread-2 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m22:50:12.623595 [info ] [Thread-2 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.04s]
[0m22:50:12.623973 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m22:50:12.624821 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.624998 [debug] [MainThread]: On master: BEGIN
[0m22:50:12.625115 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:50:12.631598 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m22:50:12.631877 [debug] [MainThread]: On master: COMMIT
[0m22:50:12.632023 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:12.632133 [debug] [MainThread]: On master: COMMIT
[0m22:50:12.632590 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:50:12.632885 [debug] [MainThread]: On master: Close
[0m22:50:12.633117 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:50:12.633251 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m22:50:12.633370 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m22:50:12.633469 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m22:50:12.633577 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m22:50:12.633736 [info ] [MainThread]: 
[0m22:50:12.633893 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:50:12.634714 [debug] [MainThread]: Command end result
[0m22:50:12.646074 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:50:12.646888 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:50:12.649661 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:50:12.649791 [info ] [MainThread]: 
[0m22:50:12.649938 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:50:12.650058 [info ] [MainThread]: 
[0m22:50:12.650182 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m22:50:12.652280 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.867959, "process_in_blocks": "0", "process_kernel_time": 0.233979, "process_mem_max_rss": "149061632", "process_out_blocks": "0", "process_user_time": 1.262372}
[0m22:50:12.652664 [debug] [MainThread]: Command `dbt test` succeeded at 22:50:12.652616 after 0.87 seconds
[0m22:50:12.652859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128d1f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11289aa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c86910>]}
[0m22:50:12.653054 [debug] [MainThread]: Flushing usage events
[0m22:50:12.869327 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:50:55.833230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f77510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ffbad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ffbf90>]}


============================== 22:50:55.835838 | b5c297eb-36d1-4911-9d9c-d15afd775139 ==============================
[0m22:50:55.835838 [info ] [MainThread]: Running with dbt=1.10.13
[0m22:50:55.836148 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'fail_fast': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'write_json': 'True', 'warn_error': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'empty': 'False', 'invocation_command': 'dbt run --models data_overview', 'cache_selected_only': 'False', 'printer_width': '80', 'introspect': 'True', 'log_cache_events': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'no_print': 'None', 'target_path': 'None', 'static_parser': 'True'}
[0m22:50:55.836391 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m22:50:55.836581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe94d0>]}
[0m22:50:55.964183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe9890>]}
[0m22:50:55.994308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104620a50>]}
[0m22:50:55.994962 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m22:50:56.041350 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m22:50:56.110444 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m22:50:56.110790 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/marts/data_overview.sql
[0m22:50:56.238132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c06c50>]}
[0m22:50:56.302253 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:50:56.303519 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:50:56.315137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b5950>]}
[0m22:50:56.315404 [info ] [MainThread]: Found 7 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m22:50:56.315573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108106c10>]}
[0m22:50:56.316340 [info ] [MainThread]: 
[0m22:50:56.316543 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:50:56.316673 [info ] [MainThread]: 
[0m22:50:56.316882 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m22:50:56.317291 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m22:50:56.343440 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m22:50:56.343660 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m22:50:56.343820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:56.373012 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.029 seconds
[0m22:50:56.373593 [debug] [ThreadPool]: On list_finny_db: Close
[0m22:50:56.375464 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m22:50:56.375701 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m22:50:56.377954 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:50:56.378177 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m22:50:56.378431 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m22:50:56.379309 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:50:56.379468 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m22:50:56.380444 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:50:56.381158 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:50:56.381295 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m22:50:56.381434 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:50:56.381581 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m22:50:56.381707 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m22:50:56.381808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:56.381987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:56.382106 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:56.396830 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m22:50:56.397002 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m22:50:56.397117 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m22:50:56.397219 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m22:50:56.397314 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m22:50:56.397427 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m22:50:56.397540 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m22:50:56.397664 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m22:50:56.397783 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m22:50:56.397905 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m22:50:56.398046 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m22:50:56.398219 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m22:50:56.401308 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m22:50:56.401824 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m22:50:56.401961 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m22:50:56.402075 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:50:56.402174 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m22:50:56.402674 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m22:50:56.403089 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m22:50:56.403553 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m22:50:56.403659 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m22:50:56.404714 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m22:50:56.404834 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m22:50:56.404953 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m22:50:56.407710 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.407844 [debug] [MainThread]: On master: BEGIN
[0m22:50:56.407945 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:50:56.414773 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m22:50:56.414926 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.415097 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m22:50:56.416854 [debug] [MainThread]: SQL status: SELECT 4 in 0.002 seconds
[0m22:50:56.417595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d22350>]}
[0m22:50:56.417813 [debug] [MainThread]: On master: ROLLBACK
[0m22:50:56.418166 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.418278 [debug] [MainThread]: On master: BEGIN
[0m22:50:56.418890 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m22:50:56.419123 [debug] [MainThread]: On master: COMMIT
[0m22:50:56.419257 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.419375 [debug] [MainThread]: On master: COMMIT
[0m22:50:56.419820 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:50:56.420045 [debug] [MainThread]: On master: Close
[0m22:50:56.422382 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m22:50:56.422662 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.data_overview ......................... [RUN]
[0m22:50:56.422900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.data_overview)
[0m22:50:56.423055 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m22:50:56.426600 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m22:50:56.426938 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m22:50:56.440909 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m22:50:56.441255 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m22:50:56.441414 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m22:50:56.441538 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:50:56.450699 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m22:50:56.450977 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m22:50:56.451170 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_marts"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m22:50:56.454090 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m22:50:56.456868 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m22:50:56.457020 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m22:50:56.457861 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m22:50:56.463518 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m22:50:56.463698 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m22:50:56.463831 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m22:50:56.465406 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m22:50:56.467654 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m22:50:56.469439 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m22:50:56.469598 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m22:50:56.470380 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m22:50:56.471294 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m22:50:56.472215 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5c297eb-36d1-4911-9d9c-d15afd775139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107692290>]}
[0m22:50:56.472477 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.05s]
[0m22:50:56.472717 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m22:50:56.473281 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.473426 [debug] [MainThread]: On master: BEGIN
[0m22:50:56.473547 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:50:56.481071 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m22:50:56.481218 [debug] [MainThread]: On master: COMMIT
[0m22:50:56.481331 [debug] [MainThread]: Using postgres connection "master"
[0m22:50:56.481433 [debug] [MainThread]: On master: COMMIT
[0m22:50:56.481869 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m22:50:56.481976 [debug] [MainThread]: On master: Close
[0m22:50:56.482117 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:50:56.482216 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m22:50:56.482312 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m22:50:56.482400 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m22:50:56.482494 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m22:50:56.482634 [info ] [MainThread]: 
[0m22:50:56.482767 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m22:50:56.483014 [debug] [MainThread]: Command end result
[0m22:50:56.495181 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m22:50:56.496132 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m22:50:56.498717 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m22:50:56.498858 [info ] [MainThread]: 
[0m22:50:56.499028 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:50:56.499157 [info ] [MainThread]: 
[0m22:50:56.499293 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:50:56.499535 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:50:56.501608 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7057509, "process_in_blocks": "0", "process_kernel_time": 0.214722, "process_mem_max_rss": "140509184", "process_out_blocks": "0", "process_user_time": 1.054624}
[0m22:50:56.501824 [debug] [MainThread]: Command `dbt run` succeeded at 22:50:56.501791 after 0.71 seconds
[0m22:50:56.501973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe94d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10262c1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026faa10>]}
[0m22:50:56.502120 [debug] [MainThread]: Flushing usage events
[0m22:50:56.778212 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:18:56.174907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075b2ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107633610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107625fd0>]}


============================== 23:18:56.177541 | 65562228-7cbc-4fc6-80b9-4bbe8a4f4107 ==============================
[0m23:18:56.177541 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:18:56.177853 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'write_json': 'True', 'introspect': 'True', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'fail_fast': 'False', 'empty': 'False', 'printer_width': '80', 'version_check': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'target_path': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'no_print': 'None'}
[0m23:18:56.304780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d48290>]}
[0m23:18:56.334367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103af9450>]}
[0m23:18:56.335028 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:18:56.381677 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:18:56.429417 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m23:18:56.429709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db8450>]}
[0m23:18:57.040486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118fb4850>]}
[0m23:18:57.109679 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:18:57.111013 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:18:57.123908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199a3490>]}
[0m23:18:57.124160 [info ] [MainThread]: Found 9 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:18:57.124316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e45990>]}
[0m23:18:57.125094 [info ] [MainThread]: 
[0m23:18:57.125254 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:18:57.125366 [info ] [MainThread]: 
[0m23:18:57.125578 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:18:57.127200 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:18:57.127427 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:18:57.155967 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:18:57.156181 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:18:57.156362 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:18:57.156511 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:18:57.156634 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:57.156756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:57.192191 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.035 seconds
[0m23:18:57.192393 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.036 seconds
[0m23:18:57.192984 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:18:57.193458 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:18:57.193884 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now create_finny_db_public_staging_analysis)
[0m23:18:57.194040 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now create_finny_db_public_raw_analysis)
[0m23:18:57.194243 [debug] [ThreadPool]: Creating schema "database: "finny_db"
schema: "public_staging_analysis"
"
[0m23:18:57.194407 [debug] [ThreadPool]: Creating schema "database: "finny_db"
schema: "public_raw_analysis"
"
[0m23:18:57.196597 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging_analysis"
[0m23:18:57.197403 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw_analysis"
[0m23:18:57.197542 [debug] [ThreadPool]: On create_finny_db_public_staging_analysis: BEGIN
[0m23:18:57.197662 [debug] [ThreadPool]: On create_finny_db_public_raw_analysis: BEGIN
[0m23:18:57.197767 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.197870 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.208629 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:18:57.208758 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging_analysis"
[0m23:18:57.208871 [debug] [ThreadPool]: On create_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "create_finny_db_public_staging_analysis"} */
create schema if not exists "public_staging_analysis"
[0m23:18:57.208997 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:18:57.209152 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw_analysis"
[0m23:18:57.209288 [debug] [ThreadPool]: On create_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "create_finny_db_public_raw_analysis"} */
create schema if not exists "public_raw_analysis"
[0m23:18:57.209765 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.001 seconds
[0m23:18:57.210141 [debug] [ThreadPool]: On create_finny_db_public_staging_analysis: COMMIT
[0m23:18:57.210275 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_staging_analysis"
[0m23:18:57.210392 [debug] [ThreadPool]: On create_finny_db_public_staging_analysis: COMMIT
[0m23:18:57.210515 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.001 seconds
[0m23:18:57.210865 [debug] [ThreadPool]: On create_finny_db_public_raw_analysis: COMMIT
[0m23:18:57.210985 [debug] [ThreadPool]: Using postgres connection "create_finny_db_public_raw_analysis"
[0m23:18:57.211105 [debug] [ThreadPool]: On create_finny_db_public_raw_analysis: COMMIT
[0m23:18:57.212273 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m23:18:57.212408 [debug] [ThreadPool]: On create_finny_db_public_staging_analysis: Close
[0m23:18:57.212690 [debug] [ThreadPool]: SQL status: COMMIT in 0.001 seconds
[0m23:18:57.212808 [debug] [ThreadPool]: On create_finny_db_public_raw_analysis: Close
[0m23:18:57.213548 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_finny_db_public_raw_analysis, now list_finny_db_public_marts_company)
[0m23:18:57.215937 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts_company"
[0m23:18:57.216112 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_finny_db_public_staging_analysis, now list_finny_db_public_raw_analysis)
[0m23:18:57.216221 [debug] [ThreadPool]: On list_finny_db_public_marts_company: BEGIN
[0m23:18:57.216455 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:18:57.216671 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:18:57.217394 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:18:57.217566 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.218291 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:18:57.219454 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:18:57.219601 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:18:57.219808 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:18:57.219949 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:18:57.220073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.220186 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:57.220320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:57.231294 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:18:57.231552 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:18:57.231767 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:18:57.231893 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:18:57.232055 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:18:57.232173 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts_company"
[0m23:18:57.232302 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:18:57.232407 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:18:57.232539 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:18:57.232785 [debug] [ThreadPool]: On list_finny_db_public_marts_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts_company"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts_company'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts_company'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts_company'
  
[0m23:18:57.232998 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:18:57.233150 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:18:57.234883 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.002 seconds
[0m23:18:57.235065 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.002 seconds
[0m23:18:57.235176 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:18:57.235317 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:18:57.235821 [debug] [ThreadPool]: On list_finny_db_public_marts_company: ROLLBACK
[0m23:18:57.236205 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:18:57.236615 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:18:57.236998 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:18:57.237410 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:18:57.237561 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:18:57.237686 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:18:57.237818 [debug] [ThreadPool]: On list_finny_db_public_marts_company: Close
[0m23:18:57.238028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public)
[0m23:18:57.238446 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_marts_location)
[0m23:18:57.238677 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging_analysis)
[0m23:18:57.239889 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:18:57.240064 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts_company, now list_finny_db_public_marts)
[0m23:18:57.240943 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts_location"
[0m23:18:57.241674 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:18:57.241800 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:18:57.242537 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:18:57.242667 [debug] [ThreadPool]: On list_finny_db_public_marts_location: BEGIN
[0m23:18:57.242780 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:18:57.242892 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.243003 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:18:57.243107 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.243210 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.243407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:18:57.252573 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:18:57.252746 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:18:57.252866 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m23:18:57.252961 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:18:57.253107 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:18:57.253223 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts_location"
[0m23:18:57.253335 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:18:57.253439 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:18:57.253563 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:18:57.253739 [debug] [ThreadPool]: On list_finny_db_public_marts_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts_location"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts_location'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts_location'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts_location'
  
[0m23:18:57.253915 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:18:57.254075 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:18:57.255864 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.002 seconds
[0m23:18:57.256067 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.002 seconds
[0m23:18:57.256217 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:18:57.256335 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.002 seconds
[0m23:18:57.256930 [debug] [ThreadPool]: On list_finny_db_public_marts_location: ROLLBACK
[0m23:18:57.257396 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:18:57.257780 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:18:57.258129 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:18:57.258559 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:18:57.258695 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:18:57.258830 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:18:57.258975 [debug] [ThreadPool]: On list_finny_db_public_marts_location: Close
[0m23:18:57.262383 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.262579 [debug] [MainThread]: On master: BEGIN
[0m23:18:57.262690 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:18:57.268590 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:18:57.268742 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.268888 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:18:57.271469 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:18:57.272591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e340d0>]}
[0m23:18:57.272805 [debug] [MainThread]: On master: ROLLBACK
[0m23:18:57.273132 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.273241 [debug] [MainThread]: On master: BEGIN
[0m23:18:57.273761 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:18:57.273957 [debug] [MainThread]: On master: COMMIT
[0m23:18:57.274085 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.274219 [debug] [MainThread]: On master: COMMIT
[0m23:18:57.274677 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:18:57.274894 [debug] [MainThread]: On master: Close
[0m23:18:57.277243 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:18:57.277469 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:18:57.277686 [info ] [Thread-1 (]: 1 of 2 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:18:57.277926 [info ] [Thread-2 (]: 2 of 2 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:18:57.278125 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_data_profiling)
[0m23:18:57.278271 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.staging_data_profiling)
[0m23:18:57.278416 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:18:57.278554 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:18:57.282045 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:18:57.283546 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:18:57.284120 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:18:57.284284 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:18:57.300512 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:18:57.302055 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:18:57.302622 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:18:57.302802 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:18:57.302970 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:18:57.303122 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:18:57.303261 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:18:57.303399 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:18:57.313494 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m23:18:57.313654 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m23:18:57.313804 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:18:57.313934 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:18:57.314119 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:18:57.314457 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:18:57.422455 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.108 seconds
[0m23:18:57.426715 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:18:57.426908 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:18:57.427532 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:18:57.434273 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:18:57.434504 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:18:57.434698 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:18:57.435897 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:18:57.438316 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:18:57.440128 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:18:57.440281 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:18:57.440771 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:18:57.441780 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:18:57.442862 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d6b110>]}
[0m23:18:57.443158 [info ] [Thread-1 (]: 1 of 2 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.16s]
[0m23:18:57.443412 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:18:57.503151 [debug] [Thread-2 (]: SQL status: SELECT 2 in 0.188 seconds
[0m23:18:57.505293 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:18:57.505571 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:18:57.506396 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:18:57.506912 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:18:57.507093 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:18:57.507314 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:18:57.508391 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:18:57.509521 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:18:57.509988 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:18:57.510230 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:18:57.510928 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:18:57.511663 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:18:57.512016 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65562228-7cbc-4fc6-80b9-4bbe8a4f4107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c2f410>]}
[0m23:18:57.512267 [info ] [Thread-2 (]: 2 of 2 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.23s]
[0m23:18:57.512495 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:18:57.513076 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.513200 [debug] [MainThread]: On master: BEGIN
[0m23:18:57.513327 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:18:57.520120 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:18:57.520292 [debug] [MainThread]: On master: COMMIT
[0m23:18:57.520410 [debug] [MainThread]: Using postgres connection "master"
[0m23:18:57.520523 [debug] [MainThread]: On master: COMMIT
[0m23:18:57.520863 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:18:57.520990 [debug] [MainThread]: On master: Close
[0m23:18:57.521156 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:18:57.521276 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m23:18:57.521384 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:18:57.521484 [debug] [MainThread]: Connection 'list_finny_db_public_marts_location' was properly closed.
[0m23:18:57.521580 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m23:18:57.521731 [info ] [MainThread]: 
[0m23:18:57.521871 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m23:18:57.522186 [debug] [MainThread]: Command end result
[0m23:18:57.538649 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:18:57.539803 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:18:57.542825 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:18:57.542998 [info ] [MainThread]: 
[0m23:18:57.543199 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:18:57.543350 [info ] [MainThread]: 
[0m23:18:57.543509 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:18:57.545945 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.4101155, "process_in_blocks": "0", "process_kernel_time": 0.233286, "process_mem_max_rss": "145244160", "process_out_blocks": "0", "process_user_time": 1.605908}
[0m23:18:57.546342 [debug] [MainThread]: Command `dbt run` succeeded at 23:18:57.546283 after 1.41 seconds
[0m23:18:57.546559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10765fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010d28d0>]}
[0m23:18:57.546778 [debug] [MainThread]: Flushing usage events
[0m23:18:57.837561 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:26:23.479681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d2ae90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9e210>]}


============================== 23:26:23.482886 | 2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1 ==============================
[0m23:26:23.482886 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:26:23.483198 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'fail_fast': 'False', 'introspect': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'printer_width': '80', 'log_cache_events': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'invocation_command': 'dbt run --select company_analysis location_analysis', 'target_path': 'None', 'quiet': 'False', 'log_format': 'default', 'no_print': 'None', 'empty': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False'}
[0m23:26:23.620089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c0cc10>]}
[0m23:26:23.651189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10381ce90>]}
[0m23:26:23.651924 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:26:23.699407 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:26:23.765865 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m23:26:23.766266 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/company_analysis.sql
[0m23:26:23.766464 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/marts/company_analysis.sql
[0m23:26:23.766642 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/location_analysis.sql
[0m23:26:23.999547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131c60d0>]}
[0m23:26:24.038795 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:26:24.039896 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:26:24.052287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136f15d0>]}
[0m23:26:24.052577 [info ] [MainThread]: Found 9 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:26:24.052735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f95bd0>]}
[0m23:26:24.053491 [info ] [MainThread]: 
[0m23:26:24.053640 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:26:24.053756 [info ] [MainThread]: 
[0m23:26:24.053955 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:26:24.055650 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:26:24.056046 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:26:24.084212 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:26:24.084458 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:26:24.084673 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:26:24.084830 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:26:24.084972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:24.085094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:24.120282 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m23:26:24.120467 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m23:26:24.121046 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:26:24.121452 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:26:24.122368 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:26:24.122580 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m23:26:24.122808 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:26:24.125410 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:26:24.125625 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:26:24.126722 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:26:24.127558 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:26:24.127689 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:26:24.128420 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:26:24.128541 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:26:24.128645 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:26:24.128772 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:26:24.128882 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:26:24.129013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:26:24.129129 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:24.129332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:24.143716 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:26:24.143911 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:26:24.144043 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:26:24.144158 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:26:24.144353 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:26:24.144480 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:26:24.144614 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:26:24.144730 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:26:24.144859 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:26:24.145044 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:26:24.145282 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:26:24.145467 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:26:24.150194 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m23:26:24.150344 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.005 seconds
[0m23:26:24.150867 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:26:24.150988 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:26:24.151108 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:26:24.151484 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:26:24.152071 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:26:24.152219 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:26:24.152622 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:26:24.152921 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_marts)
[0m23:26:24.153491 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:26:24.153596 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:26:24.153697 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:26:24.154524 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:26:24.154754 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_staging_analysis)
[0m23:26:24.155333 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:26:24.156068 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:26:24.156182 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:26:24.156294 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:26:24.156467 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:26:24.165985 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:26:24.166208 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:26:24.166393 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:26:24.166569 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:26:24.166720 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:26:24.166887 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:26:24.169288 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.002 seconds
[0m23:26:24.169438 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:26:24.169936 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:26:24.170346 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:26:24.170711 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:26:24.170840 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:26:24.173495 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.173652 [debug] [MainThread]: On master: BEGIN
[0m23:26:24.173773 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:26:24.179456 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:26:24.179637 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.179806 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:26:24.182297 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:26:24.183647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136cef90>]}
[0m23:26:24.183881 [debug] [MainThread]: On master: ROLLBACK
[0m23:26:24.184240 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.184375 [debug] [MainThread]: On master: BEGIN
[0m23:26:24.184904 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:26:24.185075 [debug] [MainThread]: On master: COMMIT
[0m23:26:24.185201 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.185314 [debug] [MainThread]: On master: COMMIT
[0m23:26:24.185622 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:26:24.185759 [debug] [MainThread]: On master: Close
[0m23:26:24.187852 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:26:24.188035 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m23:26:24.188252 [info ] [Thread-1 (]: 1 of 2 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:26:24.188503 [info ] [Thread-2 (]: 2 of 2 START sql table model public_marts.location_analysis .................... [RUN]
[0m23:26:24.188719 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.company_analysis)
[0m23:26:24.188873 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.location_analysis)
[0m23:26:24.189024 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:26:24.189169 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m23:26:24.192561 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:26:24.195082 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m23:26:24.195496 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:26:24.208345 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m23:26:24.211579 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:26:24.213344 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m23:26:24.213747 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.213919 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:26:24.214061 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m23:26:24.214196 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:26:24.214349 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:26:24.214517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:26:24.224837 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m23:26:24.224994 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m23:26:24.225146 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:26:24.225285 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.225455 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:26:24.225656 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location
)

select
    location,
    contact_count,
    company_count,
    unique_titles,
    data_sources,
    round(avg_revenue::numeric, 2) as avg_revenue
from location_stats
order by contact_count desc
  );
  
[0m23:26:24.296035 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.070 seconds
[0m23:26:24.300684 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:26:24.300875 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:26:24.301569 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:26:24.307441 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:26:24.307648 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:26:24.307798 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:26:24.308904 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:26:24.311484 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:26:24.313392 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:26:24.313550 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:26:24.314029 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:26:24.314947 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:26:24.315922 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113acffd0>]}
[0m23:26:24.316260 [info ] [Thread-1 (]: 1 of 2 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.13s]
[0m23:26:24.316536 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:26:24.347736 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.122 seconds
[0m23:26:24.349479 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.349674 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m23:26:24.350505 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:26:24.351955 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.352144 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m23:26:24.352635 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:26:24.354787 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m23:26:24.355034 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.355220 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m23:26:24.356111 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:26:24.357535 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m23:26:24.357952 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m23:26:24.358149 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m23:26:24.359754 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:26:24.360499 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m23:26:24.360819 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2e4c6f17-3a23-479d-af8d-92ee1d3bd9d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bd1f10>]}
[0m23:26:24.361108 [info ] [Thread-2 (]: 2 of 2 OK created sql table model public_marts.location_analysis ............... [[32mSELECT 76[0m in 0.17s]
[0m23:26:24.361339 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m23:26:24.362049 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.362201 [debug] [MainThread]: On master: BEGIN
[0m23:26:24.362327 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:26:24.368208 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:26:24.368480 [debug] [MainThread]: On master: COMMIT
[0m23:26:24.368622 [debug] [MainThread]: Using postgres connection "master"
[0m23:26:24.368741 [debug] [MainThread]: On master: COMMIT
[0m23:26:24.369045 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:26:24.369167 [debug] [MainThread]: On master: Close
[0m23:26:24.369348 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:26:24.369461 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m23:26:24.369568 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m23:26:24.369667 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m23:26:24.369763 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m23:26:24.369920 [info ] [MainThread]: 
[0m23:26:24.370054 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m23:26:24.370364 [debug] [MainThread]: Command end result
[0m23:26:24.384242 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:26:24.385091 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:26:24.387658 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:26:24.387807 [info ] [MainThread]: 
[0m23:26:24.387979 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:26:24.388112 [info ] [MainThread]: 
[0m23:26:24.388249 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m23:26:24.390658 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.95234793, "process_in_blocks": "0", "process_kernel_time": 0.243236, "process_mem_max_rss": "144097280", "process_out_blocks": "0", "process_user_time": 1.230753}
[0m23:26:24.391122 [debug] [MainThread]: Command `dbt run` succeeded at 23:26:24.391071 after 0.95 seconds
[0m23:26:24.391319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f2c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9d2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f676d0>]}
[0m23:26:24.391514 [debug] [MainThread]: Flushing usage events
[0m23:26:24.700287 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:37:01.620744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f07e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f7a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f83dd0>]}


============================== 23:37:01.623318 | ff42e979-5230-4662-9287-fb1355b0e499 ==============================
[0m23:37:01.623318 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:37:01.623613 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'introspect': 'True', 'target_path': 'None', 'no_print': 'None', 'static_parser': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'quiet': 'False', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'debug': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'partial_parse': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m23:37:01.750888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f4d950>]}
[0m23:37:01.780645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1025898d0>]}
[0m23:37:01.781334 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:37:01.827702 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:37:01.893627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m23:37:01.893911 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/raw_analysis/raw_country_profiling.sql
[0m23:37:01.894041 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/raw_analysis/raw_company_profiling.sql
[0m23:37:01.894158 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/staging_country_profiling.sql
[0m23:37:01.894286 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/staging_company_profiling.sql
[0m23:37:02.023939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ccab50>]}
[0m23:37:02.088401 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:37:02.089450 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:37:02.101962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c606d0>]}
[0m23:37:02.102232 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:37:02.102394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e8c910>]}
[0m23:37:02.103373 [info ] [MainThread]: 
[0m23:37:02.103519 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:37:02.103632 [info ] [MainThread]: 
[0m23:37:02.103822 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:37:02.105521 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:02.105760 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:02.109883 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:02.134012 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:02.134190 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:02.134334 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:02.134510 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:02.134652 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:02.134791 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:02.134919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:02.135036 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:02.135154 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:02.177026 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.042 seconds
[0m23:37:02.177218 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.042 seconds
[0m23:37:02.177367 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.042 seconds
[0m23:37:02.177965 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:02.178396 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:02.178793 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:02.179829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m23:37:02.182243 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:37:02.182446 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:37:02.182567 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:37:02.182755 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m23:37:02.182965 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:37:02.183890 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:37:02.184046 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:02.184774 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:37:02.185458 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:37:02.185609 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:37:02.185809 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:37:02.185938 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:37:02.186065 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:02.186183 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:02.186330 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:02.198754 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:37:02.198928 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:37:02.199064 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:37:02.199221 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:37:02.199332 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:37:02.199430 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:37:02.199559 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:37:02.199686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:37:02.199800 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:37:02.199930 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:37:02.200076 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:37:02.200214 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:37:02.202476 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m23:37:02.203108 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:37:02.203244 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.003 seconds
[0m23:37:02.203394 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:37:02.203502 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:37:02.203996 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:37:02.204126 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:37:02.204484 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:37:02.204882 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:37:02.205170 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_marts)
[0m23:37:02.205750 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:37:02.205861 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:37:02.206940 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:37:02.207133 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:37:02.207436 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging_analysis)
[0m23:37:02.207723 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:37:02.209130 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:37:02.209307 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:02.209445 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:37:02.209672 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:02.218021 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m23:37:02.218168 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:37:02.218317 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:37:02.218428 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:37:02.218571 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:37:02.218720 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:37:02.220976 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:37:02.221118 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:37:02.221713 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:37:02.222217 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:37:02.222766 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:37:02.222944 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:37:02.225833 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.225992 [debug] [MainThread]: On master: BEGIN
[0m23:37:02.226111 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:37:02.232824 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:37:02.232987 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.233152 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:37:02.235557 [debug] [MainThread]: SQL status: SELECT 4 in 0.002 seconds
[0m23:37:02.236330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10654f390>]}
[0m23:37:02.236542 [debug] [MainThread]: On master: ROLLBACK
[0m23:37:02.236909 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.237039 [debug] [MainThread]: On master: BEGIN
[0m23:37:02.237593 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:37:02.237794 [debug] [MainThread]: On master: COMMIT
[0m23:37:02.237927 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.238060 [debug] [MainThread]: On master: COMMIT
[0m23:37:02.238642 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:37:02.238825 [debug] [MainThread]: On master: Close
[0m23:37:02.241003 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:37:02.241198 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:37:02.241352 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_country_profiling
[0m23:37:02.241707 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:37:02.241559 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:37:02.241981 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:37:02.242178 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_country_profiling ......... [RUN]
[0m23:37:02.242379 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:37:02.242588 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.company_analysis)
[0m23:37:02.242757 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_company_profiling)
[0m23:37:02.242898 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_country_profiling)
[0m23:37:02.243038 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_data_profiling)
[0m23:37:02.243181 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:37:02.243319 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:37:02.243468 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_country_profiling
[0m23:37:02.243595 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:37:02.247104 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:37:02.248576 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:37:02.249949 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_country_profiling"
[0m23:37:02.251186 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:37:02.251826 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:37:02.251989 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:37:02.252155 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_country_profiling
[0m23:37:02.263240 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:37:02.268599 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:37:02.270189 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:37:02.271542 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_country_profiling"
[0m23:37:02.272924 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:37:02.273390 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.273597 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:37:02.273790 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:02.273927 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:02.274069 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:37:02.274236 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.274379 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: BEGIN
[0m23:37:02.274513 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:37:02.274743 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:37:02.274891 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:37:02.275048 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:37:02.275180 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:37:02.287140 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m23:37:02.287316 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m23:37:02.287444 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m23:37:02.287566 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:37:02.287701 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:02.287836 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:02.287959 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.288082 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.288266 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:37:02.288521 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Country profiling analysis for raw data
-- This model analyzes geographic distribution patterns in the raw data layer



with fxf_country_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        -- Extract country from location (assuming format like "City, State" or "City, Country")
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by country
),

pdl_country_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        -- Extract country from location
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by country
),

combined_country_summary as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        country,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_country_profile
        union all
        select * from pdl_country_profile
    ) combined
    group by country
)

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from combined_country_summary
order by contact_count desc
  );
  
[0m23:37:02.288744 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:37:02.288959 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:37:02.367366 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.078 seconds
[0m23:37:02.371984 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.372199 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:37:02.372876 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.374224 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.374389 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:37:02.374834 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.381475 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:37:02.381688 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.381833 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:37:02.383090 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.385794 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:37:02.387774 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:02.387944 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:37:02.389217 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:02.390402 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:37:02.391438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc8650>]}
[0m23:37:02.391816 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.15s]
[0m23:37:02.392065 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:37:02.392238 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:37:02.392527 [info ] [Thread-1 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:37:02.392738 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_company_profiling)
[0m23:37:02.392885 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:37:02.395215 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:37:02.395811 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:37:02.397371 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:37:02.397828 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:02.397975 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:37:02.398109 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:37:02.403058 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.114 seconds
[0m23:37:02.404628 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:02.404801 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m23:37:02.404972 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.116 seconds
[0m23:37:02.405168 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:37:02.405347 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:02.406773 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.407131 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as total_employees,
        sum(unique_employees) as total_unique_employees,
        sum(unique_titles) as total_unique_titles,
        sum(office_locations) as total_office_locations,
        sum(employees_with_revenue_info) as total_with_revenue_info,
        sum(employees_missing_revenue) as total_missing_revenue,
        avg(avg_revenue) as overall_avg_revenue,
        min(min_revenue) as overall_min_revenue,
        max(max_revenue) as overall_max_revenue,
        avg(email_completeness_pct) as avg_email_completeness,
        avg(email_validity_pct) as avg_email_validity,
        sum(email_domain_count) as total_email_domains
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:37:02.407445 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.407619 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:37:02.408264 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:37:02.408476 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:02.408643 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:37:02.408817 [debug] [Thread-1 (]: Postgres adapter: Postgres error: each UNION query must have the same number of columns
LINE 102: select * from company_quality_comparison
                 ^

[0m23:37:02.408979 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:37:02.409153 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: ROLLBACK
[0m23:37:02.410473 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.410618 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m23:37:02.410831 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:37:02.411050 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:37:02.412087 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:37:02.413045 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:02.413198 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:37:02.413374 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:37:02.414023 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:37:02.414238 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.414374 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:37:02.414543 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:37:02.415103 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:37:02.415274 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.415609 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106691010>]}
[0m23:37:02.416725 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:37:02.417030 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.17s]
[0m23:37:02.417383 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:02.417632 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:37:02.417810 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:37:02.418022 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_country_profiling
[0m23:37:02.418329 [info ] [Thread-2 (]: 6 of 8 START sql table model public_staging_analysis.staging_country_profiling . [RUN]
[0m23:37:02.418566 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_country_profiling)
[0m23:37:02.418734 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_country_profiling
[0m23:37:02.420392 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_country_profiling"
[0m23:37:02.420603 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:37:02.421188 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:37:02.421505 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066361d0>]}
[0m23:37:02.421660 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_country_profiling
[0m23:37:02.423223 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_country_profiling"
[0m23:37:02.423458 [info ] [Thread-4 (]: 4 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.18s]
[0m23:37:02.423821 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:37:02.424018 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:37:02.424228 [info ] [Thread-4 (]: 7 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:37:02.424443 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_data_profiling)
[0m23:37:02.424608 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:02.424780 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:37:02.424937 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: BEGIN
[0m23:37:02.426479 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:37:02.426650 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:37:02.427182 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:37:02.429676 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:37:02.430230 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.431056 [debug] [Thread-1 (]: Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  each UNION query must have the same number of columns
  LINE 102: select * from company_quality_comparison
                   ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m23:37:02.431226 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:37:02.431495 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106654850>]}
[0m23:37:02.431648 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:37:02.431948 [error] [Thread-1 (]: 5 of 8 ERROR creating sql table model public_staging_analysis.staging_company_profiling  [[31mERROR[0m in 0.04s]
[0m23:37:02.432249 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:37:02.432434 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m23:37:02.432628 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.staging_company_profiling' to be skipped because of status 'error'.  Reason: Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  each UNION query must have the same number of columns
  LINE 102: select * from company_quality_comparison
                   ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql.
[0m23:37:02.432854 [info ] [Thread-1 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:37:02.433054 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m23:37:02.433566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_company_profiling, now model.dbt_service.data_overview)
[0m23:37:02.433730 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:02.433899 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m23:37:02.434169 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Country profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_country_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location (assuming format like "City, State" or "City, Country")
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by country
),

pdl_country_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by country
),

country_data_quality as (
    select
        'quality_summary' as source_table,
        'staging' as data_layer,
        country,
        sum(contact_count) as total_contacts,
        sum(unique_contacts) as total_unique_contacts,
        sum(company_count) as total_companies,
        sum(title_count) as total_titles,
        sum(contacts_with_revenue) as total_with_revenue,
        avg(avg_revenue) as overall_avg_revenue,
        min(min_revenue) as overall_min_revenue,
        max(max_revenue) as overall_max_revenue,
        avg(email_completeness_pct) as avg_email_completeness,
        avg(revenue_completeness_pct) as avg_revenue_completeness,
        sum(valid_email_count) as total_valid_emails,
        avg(email_validity_pct) as avg_email_validity
    from (
        select * from fxf_country_profile
        union all
        select * from pdl_country_profile
    ) combined
    group by country
)

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from country_data_quality
order by contact_count desc
  );
  
[0m23:37:02.436119 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:37:02.436927 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m23:37:02.443687 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:37:02.444562 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:37:02.444742 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.445098 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:37:02.445477 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:02.445656 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m23:37:02.445822 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:37:02.451363 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:37:02.451572 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:02.451759 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:37:02.453180 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:37:02.454569 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:02.454736 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:37:02.455218 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.455946 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m23:37:02.456097 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:02.456228 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m23:37:02.456984 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.457955 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:37:02.459212 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:02.459401 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:37:02.459890 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:37:02.460473 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m23:37:02.460745 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106707d50>]}
[0m23:37:02.461027 [info ] [Thread-1 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:37:02.461259 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m23:37:02.490878 [debug] [Thread-3 (]: SQL status: SELECT 64 in 0.202 seconds
[0m23:37:02.492489 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:02.492656 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_tmp" rename to "raw_country_profiling"
[0m23:37:02.493159 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.493686 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: COMMIT
[0m23:37:02.493831 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:02.493961 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: COMMIT
[0m23:37:02.494646 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.495679 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_backup"
[0m23:37:02.495988 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:02.496142 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_backup" cascade
[0m23:37:02.496532 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:37:02.497034 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: Close
[0m23:37:02.497282 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f4210>]}
[0m23:37:02.497552 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_country_profiling .... [[32mSELECT 64[0m in 0.25s]
[0m23:37:02.497784 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_country_profiling
[0m23:37:02.612581 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.167 seconds
[0m23:37:02.614208 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.614401 [debug] [Thread-2 (]: SQL status: SELECT 64 in 0.178 seconds
[0m23:37:02.614576 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:37:02.616204 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:02.616501 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp" rename to "staging_country_profiling"
[0m23:37:02.616837 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.617021 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.618635 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.619433 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m23:37:02.619673 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:37:02.619892 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:02.620142 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m23:37:02.620403 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:02.620973 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:37:02.621134 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.621267 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.621405 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:37:02.624160 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup"
[0m23:37:02.624633 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:02.624799 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup" cascade
[0m23:37:02.624965 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:02.625906 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:37:02.626294 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:02.626509 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:37:02.626702 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:37:02.627366 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: Close
[0m23:37:02.627718 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107043090>]}
[0m23:37:02.627994 [info ] [Thread-2 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_country_profiling  [[32mSELECT 64[0m in 0.21s]
[0m23:37:02.628218 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_country_profiling
[0m23:37:02.628718 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:02.629175 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:37:02.629399 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff42e979-5230-4662-9287-fb1355b0e499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070407d0>]}
[0m23:37:02.629647 [info ] [Thread-4 (]: 7 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.20s]
[0m23:37:02.629857 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:37:02.630428 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.630554 [debug] [MainThread]: On master: BEGIN
[0m23:37:02.630655 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:37:02.636522 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:37:02.636684 [debug] [MainThread]: On master: COMMIT
[0m23:37:02.636802 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:02.636903 [debug] [MainThread]: On master: COMMIT
[0m23:37:02.637173 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:37:02.637288 [debug] [MainThread]: On master: Close
[0m23:37:02.637454 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:37:02.637562 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:37:02.637664 [debug] [MainThread]: Connection 'model.dbt_service.staging_country_profiling' was properly closed.
[0m23:37:02.637770 [debug] [MainThread]: Connection 'model.dbt_service.raw_country_profiling' was properly closed.
[0m23:37:02.637865 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:37:02.638025 [info ] [MainThread]: 
[0m23:37:02.638167 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.53 seconds (0.53s).
[0m23:37:02.638785 [debug] [MainThread]: Command end result
[0m23:37:02.654751 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:37:02.655884 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:37:02.658993 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:37:02.659143 [info ] [MainThread]: 
[0m23:37:02.659322 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:37:02.659452 [info ] [MainThread]: 
[0m23:37:02.659622 [error] [MainThread]: [31mFailure in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)[0m
[0m23:37:02.659795 [error] [MainThread]:   Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  each UNION query must have the same number of columns
  LINE 102: select * from company_quality_comparison
                   ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m23:37:02.659930 [info ] [MainThread]: 
[0m23:37:02.660089 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m23:37:02.660222 [info ] [MainThread]: 
[0m23:37:02.660364 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=8
[0m23:37:02.662761 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0799965, "process_in_blocks": "0", "process_kernel_time": 0.24353, "process_mem_max_rss": "141688832", "process_out_blocks": "0", "process_user_time": 1.225218}
[0m23:37:02.663061 [debug] [MainThread]: Command `dbt run` failed at 23:37:02.663016 after 1.08 seconds
[0m23:37:02.663259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1004f4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fb3cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106712290>]}
[0m23:37:02.663445 [debug] [MainThread]: Flushing usage events
[0m23:37:03.021325 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:37:35.945779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196b1710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119726210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119733fd0>]}


============================== 23:37:35.948628 | 134080f5-2620-4960-ad49-096729d46d23 ==============================
[0m23:37:35.948628 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:37:35.948912 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'static_parser': 'True', 'fail_fast': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_colors': 'True', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+'}
[0m23:37:36.047171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d10dd0>]}
[0m23:37:36.080717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064f9c50>]}
[0m23:37:36.081735 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:37:36.129973 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:37:36.196003 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:37:36.196392 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_company_profiling.sql
[0m23:37:36.329938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9aa110>]}
[0m23:37:36.396155 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:37:36.397237 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:37:36.408805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a91e210>]}
[0m23:37:36.409079 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:37:36.409242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aaadb10>]}
[0m23:37:36.410306 [info ] [MainThread]: 
[0m23:37:36.410477 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:37:36.410592 [info ] [MainThread]: 
[0m23:37:36.410789 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:37:36.412526 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:36.412738 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:36.412955 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:37:36.440205 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:36.440388 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:36.440536 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:37:36.440727 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:36.440870 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:36.441005 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:37:36.441145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:36.441269 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:36.441381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:36.478080 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:37:36.478280 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:37:36.478424 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:37:36.479085 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:36.479659 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:36.480072 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:37:36.481118 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:37:36.481298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m23:37:36.483658 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:37:36.483823 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m23:37:36.484078 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:37:36.485101 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:37:36.485270 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:37:36.486068 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:37:36.486806 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:37:36.486935 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:37:36.487047 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:36.487159 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:37:36.487268 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:37:36.487375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:36.487579 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:36.487713 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:37:36.500736 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:37:36.500906 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:37:36.501047 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:37:36.501157 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:37:36.501284 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:37:36.501396 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:37:36.501502 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:37:36.501632 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:37:36.501784 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:37:36.501924 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:37:36.502044 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:37:36.502243 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:37:36.507723 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.006 seconds
[0m23:37:36.507865 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.006 seconds
[0m23:37:36.507974 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.006 seconds
[0m23:37:36.508076 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.006 seconds
[0m23:37:36.508603 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:37:36.508991 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:37:36.509380 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:37:36.509815 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:37:36.510562 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:37:36.510681 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:37:36.510805 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:37:36.511028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m23:37:36.511129 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:37:36.511439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_staging_analysis)
[0m23:37:36.512686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:37:36.513627 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:37:36.514215 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:37:36.514335 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:37:36.514440 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:36.514540 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:37:36.521125 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:37:36.521298 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:37:36.521459 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:37:36.521591 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:37:36.521718 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:37:36.521865 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:37:36.523616 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:37:36.523751 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:37:36.524233 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:37:36.524595 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:37:36.525011 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:37:36.525145 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:37:36.527967 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.528106 [debug] [MainThread]: On master: BEGIN
[0m23:37:36.528211 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:37:36.534678 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:37:36.534823 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.534980 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:37:36.538787 [debug] [MainThread]: SQL status: SELECT 10 in 0.004 seconds
[0m23:37:36.539797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1197036d0>]}
[0m23:37:36.539989 [debug] [MainThread]: On master: ROLLBACK
[0m23:37:36.540362 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.540479 [debug] [MainThread]: On master: BEGIN
[0m23:37:36.541038 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:37:36.541308 [debug] [MainThread]: On master: COMMIT
[0m23:37:36.541435 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.541545 [debug] [MainThread]: On master: COMMIT
[0m23:37:36.541944 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:37:36.542155 [debug] [MainThread]: On master: Close
[0m23:37:36.544453 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:37:36.544652 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:37:36.545017 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_country_profiling
[0m23:37:36.544866 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:37:36.545233 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:37:36.545398 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:37:36.545593 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_country_profiling ......... [RUN]
[0m23:37:36.545777 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.company_analysis)
[0m23:37:36.545951 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:37:36.546122 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_company_profiling)
[0m23:37:36.546260 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_country_profiling)
[0m23:37:36.546393 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:37:36.546520 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.raw_data_profiling)
[0m23:37:36.546648 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:37:36.546769 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_country_profiling
[0m23:37:36.550364 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:37:36.550608 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:37:36.552049 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:37:36.553333 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_country_profiling"
[0m23:37:36.554745 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:37:36.555126 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:37:36.555330 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:37:36.572364 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:37:36.572677 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_country_profiling
[0m23:37:36.574164 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:37:36.574394 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:37:36.575754 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_country_profiling"
[0m23:37:36.576046 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.577525 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:37:36.577760 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:37:36.577958 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.578183 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:37:36.578338 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.578504 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:37:36.578752 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.578887 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: BEGIN
[0m23:37:36.579029 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:37:36.579183 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:37:36.579324 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:37:36.579535 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:37:36.592296 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m23:37:36.592532 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m23:37:36.592675 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m23:37:36.592817 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.592987 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.593123 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.593327 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:37:36.593532 [debug] [Thread-3 (]: SQL status: BEGIN in 0.014 seconds
[0m23:37:36.593727 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:37:36.593969 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:37:36.594171 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.594429 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Country profiling analysis for raw data
-- This model analyzes geographic distribution patterns in the raw data layer



with fxf_country_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        -- Extract country from location (assuming format like "City, State" or "City, Country")
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by country
),

pdl_country_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        -- Extract country from location
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by country
),

combined_country_summary as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        country,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_country_profile
        union all
        select * from pdl_country_profile
    ) combined
    group by country
)

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from combined_country_summary
order by contact_count desc
  );
  
[0m23:37:36.664748 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.070 seconds
[0m23:37:36.669281 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.669498 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:37:36.670134 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.671571 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.671766 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:37:36.672282 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.678942 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:37:36.679147 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.679312 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:37:36.681228 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m23:37:36.683990 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:37:36.686064 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:37:36.686231 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:37:36.687663 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:36.688762 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:37:36.689686 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af93610>]}
[0m23:37:36.689989 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.14s]
[0m23:37:36.690230 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:37:36.690398 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:37:36.690675 [info ] [Thread-1 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:37:36.690945 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_company_profiling)
[0m23:37:36.691091 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:37:36.693353 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:37:36.693775 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:37:36.695396 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:37:36.695817 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:36.695960 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:37:36.696098 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:37:36.698792 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.104 seconds
[0m23:37:36.700283 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.700448 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:37:36.700873 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.107 seconds
[0m23:37:36.701030 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.702486 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.702645 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m23:37:36.703856 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.704062 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:37:36.704297 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:36.704704 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:37:36.705041 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:37:36.705332 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.706674 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.706823 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:37:36.706986 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:37:36.707628 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:37:36.707830 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.707972 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:37:36.708158 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.708902 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:37:36.709087 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:36.709285 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.710348 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:37:36.710502 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:37:36.710809 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:37:36.710995 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:37:36.711320 [debug] [Thread-4 (]: SQL status: COMMIT in 0.000 seconds
[0m23:37:36.712287 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:37:36.712437 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:36.712705 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:37:36.713188 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:37:36.713346 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:37:36.713614 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af9c2d0>]}
[0m23:37:36.713889 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.17s]
[0m23:37:36.714123 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:37:36.714308 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_country_profiling
[0m23:37:36.714522 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:36.714719 [info ] [Thread-2 (]: 6 of 8 START sql table model public_staging_analysis.staging_country_profiling . [RUN]
[0m23:37:36.715318 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:37:36.715497 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_country_profiling)
[0m23:37:36.715774 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_country_profiling
[0m23:37:36.715916 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dc4310>]}
[0m23:37:36.717597 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_country_profiling"
[0m23:37:36.717896 [info ] [Thread-4 (]: 4 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.17s]
[0m23:37:36.718156 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:37:36.718324 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:37:36.718537 [info ] [Thread-4 (]: 7 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:37:36.718720 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_data_profiling)
[0m23:37:36.718878 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_country_profiling
[0m23:37:36.719012 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:37:36.720623 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_country_profiling"
[0m23:37:36.722357 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:37:36.722824 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:37:36.723018 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.725507 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:37:36.725745 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: BEGIN
[0m23:37:36.725977 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:37:36.726354 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.726499 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:37:36.726626 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:37:36.732290 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m23:37:36.732485 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:37:36.732692 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.732851 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.733097 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Country profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_country_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location (assuming format like "City, State" or "City, Country")
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by country
),

pdl_country_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by country
),

country_data_quality as (
    select
        'quality_summary' as source_table,
        'staging' as data_layer,
        country,
        sum(contact_count) as total_contacts,
        sum(unique_contacts) as total_unique_contacts,
        sum(company_count) as total_companies,
        sum(title_count) as total_titles,
        sum(contacts_with_revenue) as total_with_revenue,
        avg(avg_revenue) as overall_avg_revenue,
        min(min_revenue) as overall_min_revenue,
        max(max_revenue) as overall_max_revenue,
        avg(email_completeness_pct) as avg_email_completeness,
        avg(revenue_completeness_pct) as avg_revenue_completeness,
        sum(valid_email_count) as total_valid_emails,
        avg(email_validity_pct) as avg_email_validity
    from (
        select * from fxf_country_profile
        union all
        select * from pdl_country_profile
    ) combined
    group by country
)

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from country_data_quality
order by contact_count desc
  );
  
[0m23:37:36.733467 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:37:36.784757 [debug] [Thread-3 (]: SQL status: SELECT 64 in 0.190 seconds
[0m23:37:36.786290 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.786454 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_country_profiling" rename to "raw_country_profiling__dbt_backup"
[0m23:37:36.787053 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.788274 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.788430 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_tmp" rename to "raw_country_profiling"
[0m23:37:36.788833 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.789410 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: COMMIT
[0m23:37:36.789566 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.789695 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: COMMIT
[0m23:37:36.790464 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:36.791543 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_backup"
[0m23:37:36.791846 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_country_profiling"
[0m23:37:36.791995 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_country_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_country_profiling__dbt_backup" cascade
[0m23:37:36.793022 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:36.793558 [debug] [Thread-3 (]: On model.dbt_service.raw_country_profiling: Close
[0m23:37:36.793813 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b028c10>]}
[0m23:37:36.794075 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_country_profiling .... [[32mSELECT 64[0m in 0.25s]
[0m23:37:36.794300 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_country_profiling
[0m23:37:36.794508 [debug] [Thread-3 (]: Began running node model.dbt_service.data_overview
[0m23:37:36.794769 [info ] [Thread-3 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:37:36.794971 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_country_profiling, now model.dbt_service.data_overview)
[0m23:37:36.795116 [debug] [Thread-3 (]: Began compiling node model.dbt_service.data_overview
[0m23:37:36.796901 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:37:36.797220 [debug] [Thread-3 (]: Began executing node model.dbt_service.data_overview
[0m23:37:36.804818 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:37:36.805448 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:36.805678 [debug] [Thread-3 (]: On model.dbt_service.data_overview: BEGIN
[0m23:37:36.805831 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:37:36.811970 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:37:36.812253 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:36.812444 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:37:36.813796 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:37:36.815189 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:36.815346 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:37:36.815826 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.816396 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:37:36.816543 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:36.816657 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:37:36.817485 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:36.818430 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:37:36.820042 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:37:36.820330 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:37:36.820835 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:37:36.821334 [debug] [Thread-3 (]: On model.dbt_service.data_overview: Close
[0m23:37:36.821606 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b028c10>]}
[0m23:37:36.821913 [info ] [Thread-3 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:37:36.822174 [debug] [Thread-3 (]: Finished running node model.dbt_service.data_overview
[0m23:37:36.900721 [debug] [Thread-1 (]: SQL status: SELECT 271 in 0.195 seconds
[0m23:37:36.902480 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:36.902715 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:37:36.903356 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.903979 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:37:36.904178 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:36.904363 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:37:36.905856 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:36.906835 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:37:36.907139 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:37:36.907292 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:37:36.907742 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:37:36.908325 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:37:36.908605 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a764390>]}
[0m23:37:36.908865 [info ] [Thread-1 (]: 5 of 8 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.22s]
[0m23:37:36.909090 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:37:36.922803 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.189 seconds
[0m23:37:36.925342 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.925515 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:37:36.926313 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:37:36.928121 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.928298 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:37:36.928767 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.929529 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:37:36.929695 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.929833 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:37:36.930532 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:37:36.931678 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:37:36.931839 [debug] [Thread-2 (]: SQL status: SELECT 64 in 0.198 seconds
[0m23:37:36.932162 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:37:36.933826 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.934017 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:37:36.934216 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_country_profiling" rename to "staging_country_profiling__dbt_backup"
[0m23:37:36.934841 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.936152 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.936362 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:37:36.936545 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp" rename to "staging_country_profiling"
[0m23:37:36.937270 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:37:36.937664 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aed73d0>]}
[0m23:37:36.937841 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:37:36.938742 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m23:37:36.938123 [info ] [Thread-4 (]: 7 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.22s]
[0m23:37:36.938938 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.939212 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:37:36.939349 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m23:37:36.939970 [debug] [Thread-2 (]: SQL status: COMMIT in 0.000 seconds
[0m23:37:36.941098 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup"
[0m23:37:36.941527 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m23:37:36.941709 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup" cascade
[0m23:37:36.943034 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:37:36.943582 [debug] [Thread-2 (]: On model.dbt_service.staging_country_profiling: Close
[0m23:37:36.943904 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '134080f5-2620-4960-ad49-096729d46d23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b005d10>]}
[0m23:37:36.944220 [info ] [Thread-2 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_country_profiling  [[32mSELECT 64[0m in 0.23s]
[0m23:37:36.944448 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_country_profiling
[0m23:37:36.945213 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.945463 [debug] [MainThread]: On master: BEGIN
[0m23:37:36.945594 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:37:36.951500 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:37:36.951703 [debug] [MainThread]: On master: COMMIT
[0m23:37:36.951831 [debug] [MainThread]: Using postgres connection "master"
[0m23:37:36.951941 [debug] [MainThread]: On master: COMMIT
[0m23:37:36.952325 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:37:36.952451 [debug] [MainThread]: On master: Close
[0m23:37:36.952645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:37:36.952754 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m23:37:36.952857 [debug] [MainThread]: Connection 'model.dbt_service.staging_country_profiling' was properly closed.
[0m23:37:36.952955 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:37:36.953051 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:37:36.953224 [info ] [MainThread]: 
[0m23:37:36.953374 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.54 seconds (0.54s).
[0m23:37:36.953948 [debug] [MainThread]: Command end result
[0m23:37:36.967399 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:37:36.968291 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:37:36.970783 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:37:36.970919 [info ] [MainThread]: 
[0m23:37:36.971076 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:37:36.971208 [info ] [MainThread]: 
[0m23:37:36.971338 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m23:37:36.973687 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0668328, "process_in_blocks": "0", "process_kernel_time": 0.247136, "process_mem_max_rss": "141803520", "process_out_blocks": "0", "process_user_time": 1.262169}
[0m23:37:36.974081 [debug] [MainThread]: Command `dbt run` succeeded at 23:37:36.974041 after 1.07 seconds
[0m23:37:36.974262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196b2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10430c350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043478d0>]}
[0m23:37:36.974431 [debug] [MainThread]: Flushing usage events
[0m23:37:37.261342 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:38:49.817833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108acd710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c722210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c72fc90>]}


============================== 23:38:49.820560 | 65b7b998-7a8c-4021-b346-f5a8d7976b92 ==============================
[0m23:38:49.820560 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:38:49.820888 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'static_parser': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'write_json': 'True', 'no_print': 'None', 'partial_parse': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'debug': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'empty': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'log_cache_events': 'False'}
[0m23:38:49.920554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b94f890>]}
[0m23:38:49.950891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108af8d10>]}
[0m23:38:49.951734 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:38:49.999174 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:38:50.063620 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:38:50.063839 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:38:50.086921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cba4810>]}
[0m23:38:50.126420 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:38:50.127548 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:38:50.142439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9a8ed0>]}
[0m23:38:50.142703 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:38:50.142866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce21bd0>]}
[0m23:38:50.144008 [info ] [MainThread]: 
[0m23:38:50.144161 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:38:50.144280 [info ] [MainThread]: 
[0m23:38:50.144506 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:38:50.146479 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:38:50.146691 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:38:50.146966 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:38:50.151861 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:38:50.207584 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:38:50.207813 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:38:50.207984 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:38:50.208142 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:38:50.208314 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:38:50.208455 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:38:50.208580 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:38:50.208703 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:38:50.208826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:50.208942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:50.209063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:50.209174 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:50.248672 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m23:38:50.248877 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m23:38:50.249020 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m23:38:50.249134 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m23:38:50.249278 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:38:50.249401 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:38:50.249524 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:38:50.249633 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:38:50.249778 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:38:50.249936 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:38:50.250084 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:38:50.250225 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:38:50.253705 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m23:38:50.253843 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:38:50.253979 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:38:50.254640 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:38:50.255078 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:38:50.255241 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:38:50.255712 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:38:50.256159 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:38:50.256286 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:38:50.256395 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:38:50.256690 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_marts)
[0m23:38:50.256876 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public)
[0m23:38:50.256990 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:38:50.257943 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:38:50.258404 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:38:50.259124 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:38:50.259330 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:38:50.259651 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:38:50.259930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:38:50.260047 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:38:50.268325 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:38:50.268482 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:38:50.268586 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:38:50.268711 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:38:50.268838 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:38:50.268972 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:38:50.273813 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m23:38:50.273960 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.005 seconds
[0m23:38:50.274527 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:38:50.274980 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:38:50.275565 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:38:50.275686 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:38:50.278626 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.278750 [debug] [MainThread]: On master: BEGIN
[0m23:38:50.278849 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:38:50.284585 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:38:50.284754 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.284913 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:38:50.287507 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:38:50.288579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65b7b998-7a8c-4021-b346-f5a8d7976b92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b95dd50>]}
[0m23:38:50.288819 [debug] [MainThread]: On master: ROLLBACK
[0m23:38:50.289226 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.289345 [debug] [MainThread]: On master: BEGIN
[0m23:38:50.289965 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m23:38:50.290092 [debug] [MainThread]: On master: COMMIT
[0m23:38:50.290203 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.290317 [debug] [MainThread]: On master: COMMIT
[0m23:38:50.290648 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:38:50.290783 [debug] [MainThread]: On master: Close
[0m23:38:50.292289 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:38:50.292453 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:38:50.292719 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:38:50.292869 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:38:50.293016 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m23:38:50.293194 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m23:38:50.293354 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m23:38:50.293529 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m23:38:50.293715 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m23:38:50.293884 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m23:38:50.294052 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m23:38:50.294232 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m23:38:50.294388 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:38:50.294537 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:38:50.294681 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:38:50.294826 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:38:50.301667 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:38:50.303342 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:38:50.304883 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:38:50.306726 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:38:50.307605 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:38:50.307866 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:38:50.308034 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:38:50.308177 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:38:50.316580 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:38:50.317828 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:38:50.318789 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:38:50.319743 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:38:50.320630 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:38:50.320812 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:38:50.321013 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:38:50.321137 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m23:38:50.321309 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m23:38:50.321452 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:38:50.321581 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m23:38:50.321705 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:38:50.321828 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:38:50.321951 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m23:38:50.322068 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:38:50.322361 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:38:50.339171 [debug] [Thread-3 (]: SQL status: BEGIN in 0.017 seconds
[0m23:38:50.339408 [debug] [Thread-2 (]: SQL status: BEGIN in 0.017 seconds
[0m23:38:50.339594 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:38:50.339725 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:38:50.339862 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.340027 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.340335 [debug] [Thread-4 (]: SQL status: BEGIN in 0.018 seconds
[0m23:38:50.340485 [debug] [Thread-1 (]: SQL status: BEGIN in 0.019 seconds
[0m23:38:50.340616 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:38:50.340756 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:38:50.340897 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.341053 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.342379 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:38:50.342528 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:38:50.344179 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m23:38:50.344688 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m23:38:50.344915 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.005 seconds
[0m23:38:50.345161 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.004 seconds
[0m23:38:50.346043 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m23:38:50.346225 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m23:38:50.346357 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m23:38:50.346862 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m23:38:50.347220 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m23:38:50.347663 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m23:38:50.347932 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:38:50.347470 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m23:38:50.348171 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m23:38:50.348372 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:38:50.348586 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:38:50.348877 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:38:50.349131 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:38:50.349299 [info ] [Thread-1 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m23:38:50.349514 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:38:50.349700 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:38:50.349925 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:38:50.350088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m23:38:50.350245 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:38:50.350405 [info ] [Thread-2 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m23:38:50.350580 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:38:50.350741 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:38:50.350938 [info ] [Thread-3 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m23:38:50.351149 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m23:38:50.351308 [info ] [Thread-4 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m23:38:50.353559 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:38:50.353810 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m23:38:50.353982 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:38:50.354136 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m23:38:50.354364 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:38:50.355997 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:38:50.356187 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:38:50.356342 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:38:50.357856 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:38:50.360401 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:38:50.361575 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:38:50.361876 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:38:50.362305 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:38:50.363484 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:38:50.363656 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:38:50.364619 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:38:50.364854 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:38:50.366992 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:38:50.367246 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m23:38:50.367411 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:38:50.367610 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:38:50.367749 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:38:50.367906 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m23:38:50.368131 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:38:50.368275 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m23:38:50.368417 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:38:50.368554 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m23:38:50.368690 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:38:50.368888 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:38:50.383518 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m23:38:50.383774 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:38:50.384214 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.384408 [debug] [Thread-2 (]: SQL status: BEGIN in 0.016 seconds
[0m23:38:50.384544 [debug] [Thread-4 (]: SQL status: BEGIN in 0.016 seconds
[0m23:38:50.384669 [debug] [Thread-3 (]: SQL status: BEGIN in 0.016 seconds
[0m23:38:50.387640 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:38:50.387796 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:38:50.388749 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:38:50.389001 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.392846 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.393036 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:38:50.393455 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.006 seconds
[0m23:38:50.394229 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m23:38:50.394404 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:38:50.395028 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m23:38:50.395223 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m23:38:50.395588 [info ] [Thread-1 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.05s]
[0m23:38:50.395785 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m23:38:50.396016 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:38:50.396154 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:38:50.396272 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:38:50.396591 [info ] [Thread-4 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.04s]
[0m23:38:50.396877 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:38:50.397530 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m23:38:50.398015 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m23:38:50.398231 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:38:50.398378 [info ] [Thread-1 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m23:38:50.398608 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:38:50.398858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m23:38:50.399015 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m23:38:50.399166 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m23:38:50.399302 [info ] [Thread-4 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m23:38:50.399469 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:38:50.399728 [info ] [Thread-2 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:38:50.400078 [info ] [Thread-3 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:38:50.400298 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m23:38:50.402193 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:38:50.402446 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:38:50.402632 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:38:50.402764 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:38:50.402978 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:38:50.403182 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:38:50.404704 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:38:50.404882 [info ] [Thread-2 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m23:38:50.405067 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:38:50.405261 [info ] [Thread-3 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m23:38:50.405551 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m23:38:50.406764 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:38:50.406970 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m23:38:50.407135 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:38:50.407295 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:38:50.407491 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:38:50.408485 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:38:50.410251 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:38:50.410475 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:38:50.412073 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:38:50.412330 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m23:38:50.412537 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:38:50.412790 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:38:50.412949 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:38:50.413089 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:38:50.414106 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:38:50.414273 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m23:38:50.415239 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:38:50.415462 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:38:50.415777 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:38:50.415944 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m23:38:50.416119 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:38:50.416361 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:38:50.416533 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m23:38:50.416778 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:38:50.418888 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:38:50.419079 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:38:50.419234 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.420006 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:38:50.420721 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m23:38:50.421132 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m23:38:50.421431 [info ] [Thread-1 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.02s]
[0m23:38:50.421657 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:38:50.421818 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:38:50.421986 [info ] [Thread-1 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m23:38:50.422214 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m23:38:50.422377 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:38:50.422540 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m23:38:50.424365 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:38:50.424576 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:38:50.424779 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.424962 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m23:38:50.425126 [debug] [Thread-3 (]: SQL status: BEGIN in 0.008 seconds
[0m23:38:50.425281 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:38:50.425429 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:38:50.425591 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:38:50.425742 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.426811 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:38:50.426977 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.427494 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:38:50.427662 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m23:38:50.427795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:38:50.433894 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:38:50.434092 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:38:50.434243 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:38:50.435617 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.011 seconds
[0m23:38:50.436273 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m23:38:50.436421 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.009 seconds
[0m23:38:50.436567 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.009 seconds
[0m23:38:50.437173 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m23:38:50.437349 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m23:38:50.437866 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m23:38:50.438309 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m23:38:50.438172 [info ] [Thread-4 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m23:38:50.438553 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m23:38:50.438748 [info ] [Thread-3 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:38:50.439076 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:38:50.439325 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:38:50.439509 [info ] [Thread-2 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:38:50.439847 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:38:50.444214 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:38:50.444907 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m23:38:50.445444 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m23:38:50.445799 [info ] [Thread-1 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.02s]
[0m23:38:50.446062 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:38:50.446779 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.446934 [debug] [MainThread]: On master: BEGIN
[0m23:38:50.447064 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:38:50.454401 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:38:50.454641 [debug] [MainThread]: On master: COMMIT
[0m23:38:50.454768 [debug] [MainThread]: Using postgres connection "master"
[0m23:38:50.454941 [debug] [MainThread]: On master: COMMIT
[0m23:38:50.455366 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:38:50.455519 [debug] [MainThread]: On master: Close
[0m23:38:50.455720 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:38:50.455848 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m23:38:50.455954 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m23:38:50.456062 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m23:38:50.456237 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m23:38:50.456389 [info ] [MainThread]: 
[0m23:38:50.456530 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m23:38:50.457329 [debug] [MainThread]: Command end result
[0m23:38:50.470094 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:38:50.471181 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:38:50.474232 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:38:50.474391 [info ] [MainThread]: 
[0m23:38:50.474565 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:38:50.474695 [info ] [MainThread]: 
[0m23:38:50.474825 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m23:38:50.477019 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6944575, "process_in_blocks": "0", "process_kernel_time": 0.226714, "process_mem_max_rss": "136396800", "process_out_blocks": "0", "process_user_time": 1.114723}
[0m23:38:50.477259 [debug] [MainThread]: Command `dbt test` succeeded at 23:38:50.477223 after 0.69 seconds
[0m23:38:50.477422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10462bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10466f590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046fe7d0>]}
[0m23:38:50.477579 [debug] [MainThread]: Flushing usage events
[0m23:38:50.820295 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:39:01.856977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10721fb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107296210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107295fd0>]}


============================== 23:39:01.858940 | 22ca372e-b98b-4f96-ab5d-31a1a25903ac ==============================
[0m23:39:01.858940 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:39:01.859246 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt docs generate', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'debug': 'False', 'write_json': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80', 'introspect': 'True', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'log_format': 'default', 'fail_fast': 'False', 'warn_error': 'None', 'version_check': 'True', 'no_print': 'None', 'use_colors': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'use_experimental_parser': 'False', 'quiet': 'False'}
[0m23:39:01.943796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107762650>]}
[0m23:39:01.973955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102df8710>]}
[0m23:39:01.974420 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:39:02.016347 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:39:02.071701 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:39:02.071925 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:39:02.094782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077e3fd0>]}
[0m23:39:02.103230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110287950>]}
[0m23:39:02.103489 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:39:02.103647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107825190>]}
[0m23:39:02.104723 [info ] [MainThread]: 
[0m23:39:02.104885 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:39:02.105002 [info ] [MainThread]: 
[0m23:39:02.105231 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:39:02.107095 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:39:02.107291 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:39:02.107512 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:39:02.109380 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:39:02.130780 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:39:02.131033 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:39:02.131281 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:39:02.131486 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:39:02.131751 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:39:02.131946 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:39:02.132097 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:39:02.132245 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:39:02.132400 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:02.132541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:02.132719 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:02.132922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:02.158937 [debug] [ThreadPool]: SQL status: BEGIN in 0.026 seconds
[0m23:39:02.159332 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:39:02.159487 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:39:02.160014 [debug] [ThreadPool]: SQL status: BEGIN in 0.027 seconds
[0m23:39:02.160160 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m23:39:02.160310 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m23:39:02.160442 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:39:02.160566 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:39:02.160683 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:39:02.160824 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:39:02.160977 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:39:02.161127 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:39:02.164271 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m23:39:02.164412 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m23:39:02.164531 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:39:02.165172 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:39:02.165320 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m23:39:02.165931 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:39:02.166393 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:39:02.166894 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:39:02.167055 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:39:02.167405 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging)
[0m23:39:02.168886 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:39:02.169005 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:39:02.169114 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:39:02.169214 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:39:02.169340 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:39:02.169562 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_raw)
[0m23:39:02.170133 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:39:02.171136 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:39:02.171354 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:39:02.171472 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:39:02.183106 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:39:02.183280 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:39:02.183430 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:39:02.183543 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:39:02.183673 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:39:02.183846 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:39:02.186093 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:39:02.186253 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:39:02.186830 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:39:02.187236 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:39:02.187666 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:39:02.187792 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:39:02.190773 [debug] [MainThread]: Using postgres connection "master"
[0m23:39:02.190924 [debug] [MainThread]: On master: BEGIN
[0m23:39:02.191029 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:39:02.197162 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:39:02.197339 [debug] [MainThread]: Using postgres connection "master"
[0m23:39:02.197503 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:39:02.199754 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:39:02.201009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '22ca372e-b98b-4f96-ab5d-31a1a25903ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102b15d0>]}
[0m23:39:02.201233 [debug] [MainThread]: On master: ROLLBACK
[0m23:39:02.201577 [debug] [MainThread]: On master: Close
[0m23:39:02.203236 [debug] [Thread-1 (]: Began running node seed.dbt_service.fxf_data
[0m23:39:02.203407 [debug] [Thread-2 (]: Began running node seed.dbt_service.fxf_sample
[0m23:39:02.203568 [debug] [Thread-3 (]: Began running node seed.dbt_service.pdl_data
[0m23:39:02.203727 [debug] [Thread-4 (]: Began running node seed.dbt_service.pdl_sample
[0m23:39:02.203896 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now seed.dbt_service.fxf_data)
[0m23:39:02.204061 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now seed.dbt_service.fxf_sample)
[0m23:39:02.204231 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now seed.dbt_service.pdl_data)
[0m23:39:02.204378 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now seed.dbt_service.pdl_sample)
[0m23:39:02.204524 [debug] [Thread-1 (]: Began compiling node seed.dbt_service.fxf_data
[0m23:39:02.204675 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.fxf_sample
[0m23:39:02.204813 [debug] [Thread-3 (]: Began compiling node seed.dbt_service.pdl_data
[0m23:39:02.204945 [debug] [Thread-4 (]: Began compiling node seed.dbt_service.pdl_sample
[0m23:39:02.207796 [debug] [Thread-1 (]: Began executing node seed.dbt_service.fxf_data
[0m23:39:02.208495 [debug] [Thread-2 (]: Began executing node seed.dbt_service.fxf_sample
[0m23:39:02.209691 [debug] [Thread-3 (]: Began executing node seed.dbt_service.pdl_data
[0m23:39:02.234345 [debug] [Thread-4 (]: Began executing node seed.dbt_service.pdl_sample
[0m23:39:02.234790 [debug] [Thread-1 (]: Finished running node seed.dbt_service.fxf_data
[0m23:39:02.235226 [debug] [Thread-2 (]: Finished running node seed.dbt_service.fxf_sample
[0m23:39:02.235519 [debug] [Thread-3 (]: Finished running node seed.dbt_service.pdl_data
[0m23:39:02.235994 [debug] [Thread-4 (]: Finished running node seed.dbt_service.pdl_sample
[0m23:39:02.236341 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m23:39:02.236713 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_pdl_data
[0m23:39:02.236912 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.dbt_service.fxf_data, now model.dbt_service.raw_fxf_data)
[0m23:39:02.237066 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly seed.dbt_service.pdl_sample, now model.dbt_service.raw_pdl_data)
[0m23:39:02.237210 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m23:39:02.237332 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m23:39:02.238819 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m23:39:02.240123 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m23:39:02.240846 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_pdl_data
[0m23:39:02.241038 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m23:39:02.241439 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_pdl_data
[0m23:39:02.241711 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m23:39:02.242120 [debug] [Thread-3 (]: Began running node model.dbt_service.stg_pdl_data
[0m23:39:02.242284 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:39:02.242435 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:39:02.242647 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.dbt_service.pdl_data, now model.dbt_service.stg_pdl_data)
[0m23:39:02.242808 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:39:02.242997 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.dbt_service.fxf_sample, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m23:39:02.243172 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_pdl_data, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m23:39:02.243333 [debug] [Thread-3 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m23:39:02.243475 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_fxf_data, now model.dbt_service.raw_company_profiling)
[0m23:39:02.243621 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:39:02.243762 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:39:02.245339 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m23:39:02.245496 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:39:02.250308 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:39:02.252821 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:39:02.254334 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:39:02.254791 [debug] [Thread-3 (]: Began executing node model.dbt_service.stg_pdl_data
[0m23:39:02.254968 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:39:02.255282 [debug] [Thread-3 (]: Finished running node model.dbt_service.stg_pdl_data
[0m23:39:02.255415 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:39:02.255537 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:39:02.255787 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:39:02.256016 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_country_profiling
[0m23:39:02.256297 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:39:02.256745 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:39:02.256986 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:39:02.257204 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_pdl_data, now model.dbt_service.raw_country_profiling)
[0m23:39:02.257389 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m23:39:02.257574 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:39:02.257756 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now model.dbt_service.raw_data_profiling)
[0m23:39:02.257920 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_country_profiling
[0m23:39:02.258071 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.stg_fxf_data)
[0m23:39:02.258236 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m23:39:02.258389 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:39:02.259749 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_country_profiling"
[0m23:39:02.259898 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m23:39:02.260050 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:39:02.261315 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:39:02.262383 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m23:39:02.264991 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:39:02.265240 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_country_profiling
[0m23:39:02.265462 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:39:02.265754 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_country_profiling
[0m23:39:02.266043 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:39:02.266200 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m23:39:02.266356 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:39:02.266533 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:39:02.266708 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:39:02.266977 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m23:39:02.267215 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:39:02.267365 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_country_profiling, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m23:39:02.267518 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m23:39:02.267680 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:39:02.267952 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m23:39:02.268096 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:39:02.268235 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:39:02.268391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_fxf_data, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m23:39:02.268543 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now model.dbt_service.company_analysis)
[0m23:39:02.270146 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:39:02.271530 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:39:02.271674 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:39:02.271819 [debug] [Thread-4 (]: Began compiling node model.dbt_service.company_analysis
[0m23:39:02.273320 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:39:02.274616 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:39:02.274760 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:39:02.274908 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:39:02.275227 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:39:02.275395 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:39:02.275615 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:39:02.275787 [debug] [Thread-3 (]: Began running node model.dbt_service.location_analysis
[0m23:39:02.275929 [debug] [Thread-4 (]: Began executing node model.dbt_service.company_analysis
[0m23:39:02.276167 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:39:02.276324 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:39:02.276549 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66, now model.dbt_service.location_analysis)
[0m23:39:02.276801 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m23:39:02.276965 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_country_profiling
[0m23:39:02.277128 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now model.dbt_service.staging_company_profiling)
[0m23:39:02.277279 [debug] [Thread-3 (]: Began compiling node model.dbt_service.location_analysis
[0m23:39:02.277452 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:39:02.277656 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa, now model.dbt_service.staging_country_profiling)
[0m23:39:02.277810 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:39:02.279103 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m23:39:02.279271 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_data_profiling)
[0m23:39:02.279419 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_country_profiling
[0m23:39:02.280751 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:39:02.280930 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:39:02.282230 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_country_profiling"
[0m23:39:02.283721 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:39:02.283868 [debug] [Thread-3 (]: Began executing node model.dbt_service.location_analysis
[0m23:39:02.284240 [debug] [Thread-3 (]: Finished running node model.dbt_service.location_analysis
[0m23:39:02.284390 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:39:02.284563 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:39:02.284888 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:39:02.285035 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:39:02.285172 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_country_profiling
[0m23:39:02.285314 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.location_analysis, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m23:39:02.285469 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:39:02.285732 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:39:02.285971 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_country_profiling
[0m23:39:02.286112 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:39:02.286256 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_company_profiling, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m23:39:02.286417 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:39:02.286583 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:39:02.287956 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:39:02.288101 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:39:02.288253 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_data_profiling, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m23:39:02.288406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_country_profiling, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m23:39:02.289809 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:39:02.289959 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:39:02.290102 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:39:02.290244 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:39:02.292617 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:39:02.294032 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:39:02.294171 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:39:02.294414 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:39:02.294735 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:39:02.294914 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:39:02.295046 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:39:02.295231 [debug] [Thread-2 (]: Began running node model.dbt_service.data_overview
[0m23:39:02.295394 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m23:39:02.295528 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:39:02.295751 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:39:02.295903 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now model.dbt_service.data_overview)
[0m23:39:02.296051 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:39:02.296283 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:39:02.296434 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:39:02.296586 [debug] [Thread-2 (]: Began compiling node model.dbt_service.data_overview
[0m23:39:02.297970 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:39:02.298139 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:39:02.298306 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m23:39:02.299820 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:39:02.300012 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m23:39:02.300176 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:39:02.300336 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:39:02.300485 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:39:02.301855 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:39:02.302102 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:39:02.302234 [debug] [Thread-2 (]: Began executing node model.dbt_service.data_overview
[0m23:39:02.303644 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:39:02.304143 [debug] [Thread-2 (]: Finished running node model.dbt_service.data_overview
[0m23:39:02.304350 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:39:02.304782 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:39:02.304982 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:39:02.305227 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:39:02.305736 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:39:02.305865 [debug] [MainThread]: Connection 'test.dbt_service.unique_location_analysis_location.d8f9675ab7' was properly closed.
[0m23:39:02.305976 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:39:02.306074 [debug] [MainThread]: Connection 'test.dbt_service.unique_company_analysis_company.2a96b2dba1' was properly closed.
[0m23:39:02.306164 [debug] [MainThread]: Connection 'test.dbt_service.not_null_location_analysis_location.8bf3dfa482' was properly closed.
[0m23:39:02.307441 [debug] [MainThread]: Command end result
[0m23:39:02.347608 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:39:02.348616 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:39:02.351559 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:39:02.353086 [debug] [MainThread]: Acquiring new postgres connection 'generate_catalog'
[0m23:39:02.353257 [info ] [MainThread]: Building catalog
[0m23:39:02.357245 [debug] [ThreadPool]: Acquiring new postgres connection 'finny_db.information_schema'
[0m23:39:02.360326 [debug] [ThreadPool]: Using postgres connection "finny_db.information_schema"
[0m23:39:02.360456 [debug] [ThreadPool]: On finny_db.information_schema: BEGIN
[0m23:39:02.360560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:02.382774 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m23:39:02.383024 [debug] [ThreadPool]: Using postgres connection "finny_db.information_schema"
[0m23:39:02.383221 [debug] [ThreadPool]: On finny_db.information_schema: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "finny_db.information_schema"} */

    
    

    select
        'finny_db' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            when 'm' then 'MATERIALIZED VIEW'
            else 'BASE TABLE'
        end as table_type,
        tbl_desc.description as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        col_desc.description as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid
    left outer join pg_catalog.pg_description tbl_desc on (tbl_desc.objoid = tbl.oid and tbl_desc.objsubid = 0)
    left outer join pg_catalog.pg_description col_desc on (col_desc.objoid = tbl.oid and col_desc.objsubid = col.attnum)
    where ((upper(sch.nspname) = upper('public_marts') and
           upper(tbl.relname) = upper('location_analysis')) or (upper(sch.nspname) = upper('public') and
           upper(tbl.relname) = upper('pdl_data')) or (upper(sch.nspname) = upper('public') and
           upper(tbl.relname) = upper('fxf_sample')) or (upper(sch.nspname) = upper('public_raw') and
           upper(tbl.relname) = upper('raw_fxf_data')) or (upper(sch.nspname) = upper('public_staging_analysis') and
           upper(tbl.relname) = upper('staging_data_profiling')) or (upper(sch.nspname) = upper('public_marts') and
           upper(tbl.relname) = upper('data_overview')) or (upper(sch.nspname) = upper('public_staging_analysis') and
           upper(tbl.relname) = upper('company_analysis')) or (upper(sch.nspname) = upper('public') and
           upper(tbl.relname) = upper('fxf_data')) or (upper(sch.nspname) = upper('public_raw_analysis') and
           upper(tbl.relname) = upper('raw_country_profiling')) or (upper(sch.nspname) = upper('public_staging_analysis') and
           upper(tbl.relname) = upper('staging_country_profiling')) or (upper(sch.nspname) = upper('public_staging_analysis') and
           upper(tbl.relname) = upper('staging_company_profiling')) or (upper(sch.nspname) = upper('public_raw_analysis') and
           upper(tbl.relname) = upper('raw_company_profiling')) or (upper(sch.nspname) = upper('public') and
           upper(tbl.relname) = upper('pdl_sample')) or (upper(sch.nspname) = upper('public_staging') and
           upper(tbl.relname) = upper('stg_fxf_data')) or (upper(sch.nspname) = upper('public_raw_analysis') and
           upper(tbl.relname) = upper('raw_data_profiling')) or (upper(sch.nspname) = upper('public_staging') and
           upper(tbl.relname) = upper('stg_pdl_data')) or (upper(sch.nspname) = upper('public_raw') and
           upper(tbl.relname) = upper('raw_pdl_data')))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence in ('p', 'u') -- [p]ermanent table or [u]nlogged table. Exclude [t]emporary tables
      and tbl.relkind in ('r', 'v', 'f', 'p', 'm') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table, [m]aterialized view. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
[0m23:39:02.388733 [debug] [ThreadPool]: SQL status: SELECT 178 in 0.005 seconds
[0m23:39:02.391591 [debug] [ThreadPool]: On finny_db.information_schema: ROLLBACK
[0m23:39:02.392676 [debug] [ThreadPool]: On finny_db.information_schema: Close
[0m23:39:02.400401 [debug] [MainThread]: Wrote artifact CatalogArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/catalog.json
[0m23:39:02.411978 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:39:02.412710 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:39:02.412846 [info ] [MainThread]: Catalog written to /Users/mahmoud/Workspace/finny/dbt_service/target/catalog.json
[0m23:39:02.415060 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 0.592376, "process_in_blocks": "0", "process_kernel_time": 0.190675, "process_mem_max_rss": "137936896", "process_out_blocks": "0", "process_user_time": 1.034113}
[0m23:39:02.415333 [debug] [MainThread]: Command `dbt docs generate` succeeded at 23:39:02.415296 after 0.59 seconds
[0m23:39:02.415493 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m23:39:02.415645 [debug] [MainThread]: Connection 'finny_db.information_schema' was properly closed.
[0m23:39:02.415802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072d2050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058cfe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100aa3690>]}
[0m23:39:02.416007 [debug] [MainThread]: Flushing usage events
[0m23:39:02.649937 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:44:28.568348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109547810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095c7550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095c7c90>]}


============================== 23:44:28.571196 | 43340c98-21b6-48d5-9e12-81674d3336cc ==============================
[0m23:44:28.571196 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:44:28.571526 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'write_json': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'no_print': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'partial_parse': 'True', 'use_colors': 'True', 'introspect': 'True', 'quiet': 'False', 'version_check': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'target_path': 'None', 'printer_width': '80'}
[0m23:44:28.702299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a17350>]}
[0m23:44:28.732649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106be9010>]}
[0m23:44:28.733339 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:44:28.779526 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:44:28.843747 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m23:44:28.844063 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m23:44:28.844215 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/raw_analysis/raw_location_profiling.sql
[0m23:44:28.844338 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/raw_analysis/raw_country_profiling.sql
[0m23:44:28.844450 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/staging_analysis/staging_country_profiling.sql
[0m23:44:28.972174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a382e50>]}
[0m23:44:29.038076 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:44:29.039414 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:44:29.052492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c35890>]}
[0m23:44:29.052760 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:44:29.052924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a656d50>]}
[0m23:44:29.053975 [info ] [MainThread]: 
[0m23:44:29.054210 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:44:29.054374 [info ] [MainThread]: 
[0m23:44:29.054692 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:44:29.056360 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:44:29.056683 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:44:29.061528 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:44:29.085503 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:44:29.085735 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:44:29.085912 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:44:29.086090 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:44:29.086235 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:44:29.086378 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:44:29.086511 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:29.086632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:29.086749 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:29.123231 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:44:29.123438 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:44:29.123699 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.037 seconds
[0m23:44:29.124959 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:44:29.126283 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:44:29.128274 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:44:29.131586 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m23:44:29.131843 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m23:44:29.135551 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:44:29.135837 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:44:29.136150 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:44:29.137575 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:44:29.137778 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:44:29.138616 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:44:29.139437 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:44:29.139581 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:44:29.139703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:29.139819 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:44:29.139930 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:44:29.140039 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:29.140333 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:29.140458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:29.153633 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:44:29.153881 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:44:29.154040 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:44:29.157190 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m23:44:29.157382 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m23:44:29.157570 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:44:29.157693 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m23:44:29.157919 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:44:29.158281 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:44:29.158521 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:44:29.158654 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m23:44:29.158813 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:44:29.159026 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:44:29.159707 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:44:29.160294 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:44:29.160575 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m23:44:29.161752 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:44:29.161903 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:44:29.162018 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:29.163409 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m23:44:29.163725 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m23:44:29.163928 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:44:29.164499 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:44:29.165358 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:44:29.165885 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:44:29.166589 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:44:29.167022 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:44:29.167253 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:44:29.167615 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m23:44:29.169182 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:44:29.169796 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:44:29.169976 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:29.170387 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:44:29.170536 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:44:29.170692 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:44:29.172869 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:44:29.173589 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:44:29.174125 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:44:29.176188 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m23:44:29.176379 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:44:29.176534 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:44:29.178038 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m23:44:29.178710 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:44:29.179177 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:44:29.182139 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.182315 [debug] [MainThread]: On master: BEGIN
[0m23:44:29.182430 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:44:29.189304 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:44:29.189528 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.189707 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:44:29.192349 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:44:29.193914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa8f890>]}
[0m23:44:29.194176 [debug] [MainThread]: On master: ROLLBACK
[0m23:44:29.194568 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.194754 [debug] [MainThread]: On master: BEGIN
[0m23:44:29.195437 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:44:29.195616 [debug] [MainThread]: On master: COMMIT
[0m23:44:29.195731 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.195834 [debug] [MainThread]: On master: COMMIT
[0m23:44:29.196196 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:44:29.196394 [debug] [MainThread]: On master: Close
[0m23:44:29.198649 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:44:29.198912 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:44:29.199356 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:44:29.199193 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:44:29.199623 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_location_profiling
[0m23:44:29.199871 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:44:29.200153 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:44:29.200400 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.company_analysis)
[0m23:44:29.200591 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_location_profiling ........ [RUN]
[0m23:44:29.200785 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_company_profiling)
[0m23:44:29.200945 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_data_profiling)
[0m23:44:29.201087 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:44:29.201242 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_location_profiling)
[0m23:44:29.201376 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:44:29.201499 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:44:29.204923 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:44:29.205125 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m23:44:29.206673 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:44:29.208123 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:44:29.209934 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m23:44:29.210289 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:44:29.223054 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:44:29.223306 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:44:29.227513 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:44:29.229179 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:44:29.230657 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:44:29.230834 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_location_profiling
[0m23:44:29.232728 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m23:44:29.233124 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.233379 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.233565 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.233746 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:44:29.233935 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:44:29.234133 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:44:29.234306 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:44:29.234518 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:44:29.234739 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m23:44:29.234930 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:44:29.235109 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:44:29.235363 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:44:29.247563 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m23:44:29.247769 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m23:44:29.247974 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m23:44:29.248151 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m23:44:29.248286 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.248473 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.248601 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.248726 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:44:29.248919 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:44:29.249186 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:44:29.249441 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:44:29.249724 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

combined_country_summary as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        country,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_country_profile
        union all
        select * from pdl_country_profile
    ) combined
    group by country
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m23:44:29.250544 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near "combined_country_summary"
LINE 96: combined_country_summary as (
         ^

[0m23:44:29.250759 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: ROLLBACK
[0m23:44:29.251216 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: Close
[0m23:44:29.256620 [debug] [Thread-4 (]: Database Error in model raw_location_profiling (models/raw_analysis/raw_location_profiling.sql)
  syntax error at or near "combined_country_summary"
  LINE 96: combined_country_summary as (
           ^
  compiled code at target/run/dbt_service/models/raw_analysis/raw_location_profiling.sql
[0m23:44:29.257643 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa1c350>]}
[0m23:44:29.257978 [error] [Thread-4 (]: 4 of 8 ERROR creating sql table model public_raw_analysis.raw_location_profiling  [[31mERROR[0m in 0.06s]
[0m23:44:29.258226 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_location_profiling
[0m23:44:29.258397 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:44:29.258601 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.raw_location_profiling' to be skipped because of status 'error'.  Reason: Database Error in model raw_location_profiling (models/raw_analysis/raw_location_profiling.sql)
  syntax error at or near "combined_country_summary"
  LINE 96: combined_country_summary as (
           ^
  compiled code at target/run/dbt_service/models/raw_analysis/raw_location_profiling.sql.
[0m23:44:29.259041 [info ] [Thread-4 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:44:29.259893 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_location_profiling, now model.dbt_service.staging_company_profiling)
[0m23:44:29.260077 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:44:29.261832 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:44:29.262385 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:44:29.264085 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:44:29.264500 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.264651 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:44:29.264786 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:44:29.271171 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:44:29.271405 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.271935 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:44:29.315871 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.066 seconds
[0m23:44:29.320376 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.320636 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:44:29.321300 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.322642 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.322810 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:44:29.323243 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.329809 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:44:29.330011 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.330161 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:44:29.331531 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.334875 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:44:29.336747 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:44:29.336902 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:44:29.339002 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:44:29.340127 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:44:29.340411 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa53c50>]}
[0m23:44:29.340694 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.14s]
[0m23:44:29.340931 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:44:29.341096 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:44:29.341369 [info ] [Thread-1 (]: 6 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:44:29.341599 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_data_profiling)
[0m23:44:29.341768 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:44:29.343400 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:44:29.343818 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:44:29.345239 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:44:29.345750 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.345910 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:44:29.346047 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:44:29.348364 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.098 seconds
[0m23:44:29.349798 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.349956 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:44:29.350443 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.351774 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.351963 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.102 seconds
[0m23:44:29.352176 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:44:29.352385 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:44:29.353857 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.354081 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.354298 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:44:29.354511 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.354774 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:44:29.355787 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:44:29.355974 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:44:29.356215 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.357436 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.357595 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:44:29.357786 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:44:29.358337 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.358980 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:44:29.359127 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.359287 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.360339 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:44:29.360486 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:44:29.360826 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:44:29.361029 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:44:29.361653 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.362710 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:44:29.362866 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:44:29.363175 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:44:29.363685 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:44:29.363845 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:44:29.364123 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab3c790>]}
[0m23:44:29.364499 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.16s]
[0m23:44:29.364770 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:44:29.364929 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:44:29.365159 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:44:29.365691 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:44:29.365887 [info ] [Thread-2 (]: 7 of 8 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:44:29.366198 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa2dc50>]}
[0m23:44:29.366394 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_location_profiling)
[0m23:44:29.366696 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.17s]
[0m23:44:29.366917 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:44:29.367188 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:44:29.368925 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:44:29.369118 [debug] [Thread-3 (]: Began running node model.dbt_service.data_overview
[0m23:44:29.369384 [info ] [Thread-3 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:44:29.369608 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.data_overview)
[0m23:44:29.369847 [debug] [Thread-3 (]: Began compiling node model.dbt_service.data_overview
[0m23:44:29.372495 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:44:29.372675 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:44:29.374258 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:44:29.374532 [debug] [Thread-3 (]: Began executing node model.dbt_service.data_overview
[0m23:44:29.382052 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:44:29.382275 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:44:29.382489 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:44:29.382632 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:44:29.382898 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:44:29.383046 [debug] [Thread-3 (]: On model.dbt_service.data_overview: BEGIN
[0m23:44:29.383223 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:44:29.389894 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m23:44:29.390108 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m23:44:29.390322 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:44:29.390494 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:44:29.390725 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:44:29.391078 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from country_data_quality
order by contact_count desc
  );
  
[0m23:44:29.391854 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 107: )
          ^

[0m23:44:29.392056 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: ROLLBACK
[0m23:44:29.392746 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:44:29.392954 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m23:44:29.395140 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:44:29.395324 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:44:29.396069 [debug] [Thread-2 (]: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:44:29.396267 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:44:29.396537 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa0bd10>]}
[0m23:44:29.397171 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:44:29.397479 [error] [Thread-2 (]: 7 of 8 ERROR creating sql table model public_staging_analysis.staging_location_profiling  [[31mERROR[0m in 0.03s]
[0m23:44:29.397720 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:44:29.397925 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:44:29.398058 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:44:29.398240 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.staging_location_profiling' to be skipped because of status 'error'.  Reason: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql.
[0m23:44:29.398958 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.400177 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:44:29.401532 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:44:29.401689 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:44:29.402148 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:44:29.402727 [debug] [Thread-3 (]: On model.dbt_service.data_overview: Close
[0m23:44:29.402989 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c58610>]}
[0m23:44:29.403270 [info ] [Thread-3 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:44:29.403535 [debug] [Thread-3 (]: Finished running node model.dbt_service.data_overview
[0m23:44:29.459216 [debug] [Thread-4 (]: SQL status: SELECT 271 in 0.187 seconds
[0m23:44:29.460795 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.461011 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m23:44:29.461595 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.462601 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.462787 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:44:29.463204 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.463797 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:44:29.463930 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.464052 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:44:29.465037 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.465947 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:44:29.466216 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:44:29.466372 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:44:29.467606 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:44:29.468221 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:44:29.468517 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab46110>]}
[0m23:44:29.468796 [info ] [Thread-4 (]: 5 of 8 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.21s]
[0m23:44:29.469065 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:44:29.526731 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.171 seconds
[0m23:44:29.528862 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.529110 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:44:29.529812 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:44:29.531103 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.531317 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:44:29.531806 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:44:29.532387 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:44:29.532588 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.532800 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:44:29.534384 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:44:29.536720 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:44:29.537134 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:44:29.537335 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:44:29.538674 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:44:29.539157 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:44:29.539442 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43340c98-21b6-48d5-9e12-81674d3336cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acb8450>]}
[0m23:44:29.539705 [info ] [Thread-1 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.20s]
[0m23:44:29.539935 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:44:29.540717 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.540901 [debug] [MainThread]: On master: BEGIN
[0m23:44:29.541025 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:44:29.546689 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:44:29.546861 [debug] [MainThread]: On master: COMMIT
[0m23:44:29.546982 [debug] [MainThread]: Using postgres connection "master"
[0m23:44:29.547085 [debug] [MainThread]: On master: COMMIT
[0m23:44:29.547472 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:44:29.547730 [debug] [MainThread]: On master: Close
[0m23:44:29.547957 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:44:29.548074 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:44:29.548179 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m23:44:29.548290 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:44:29.548398 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m23:44:29.548580 [info ] [MainThread]: 
[0m23:44:29.548733 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.49 seconds (0.49s).
[0m23:44:29.549359 [debug] [MainThread]: Command end result
[0m23:44:29.564853 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:44:29.565893 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:44:29.569228 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:44:29.569394 [info ] [MainThread]: 
[0m23:44:29.569567 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m23:44:29.569705 [info ] [MainThread]: 
[0m23:44:29.569882 [error] [MainThread]: [31mFailure in model raw_location_profiling (models/raw_analysis/raw_location_profiling.sql)[0m
[0m23:44:29.570053 [error] [MainThread]:   Database Error in model raw_location_profiling (models/raw_analysis/raw_location_profiling.sql)
  syntax error at or near "combined_country_summary"
  LINE 96: combined_country_summary as (
           ^
  compiled code at target/run/dbt_service/models/raw_analysis/raw_location_profiling.sql
[0m23:44:29.570190 [info ] [MainThread]: 
[0m23:44:29.570340 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/raw_analysis/raw_location_profiling.sql
[0m23:44:29.570467 [info ] [MainThread]: 
[0m23:44:29.570624 [error] [MainThread]: [31mFailure in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)[0m
[0m23:44:29.570785 [error] [MainThread]:   Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:44:29.570908 [info ] [MainThread]: 
[0m23:44:29.571060 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:44:29.571187 [info ] [MainThread]: 
[0m23:44:29.571326 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=8
[0m23:44:29.573577 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0428065, "process_in_blocks": "0", "process_kernel_time": 0.246647, "process_mem_max_rss": "140935168", "process_out_blocks": "0", "process_user_time": 1.245525}
[0m23:44:29.573808 [debug] [MainThread]: Command `dbt run` failed at 23:44:29.573769 after 1.04 seconds
[0m23:44:29.573998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf0350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095f3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095f3a50>]}
[0m23:44:29.574181 [debug] [MainThread]: Flushing usage events
[0m23:44:30.039331 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:45:02.671037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b2790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10732ffd0>]}


============================== 23:45:02.673591 | 375ad253-81d9-43c0-855c-d2e1f522d935 ==============================
[0m23:45:02.673591 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:45:02.673879 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'indirect_selection': 'eager', 'fail_fast': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_format': 'default', 'write_json': 'True', 'warn_error': 'None', 'empty': 'False', 'static_parser': 'True', 'no_print': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'use_colors': 'True'}
[0m23:45:02.773589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b19090>]}
[0m23:45:02.804080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043f59d0>]}
[0m23:45:02.804794 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:45:02.853589 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:45:02.919766 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:45:02.920138 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/raw_analysis/raw_location_profiling.sql
[0m23:45:03.046097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118624090>]}
[0m23:45:03.110961 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:45:03.111949 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:45:03.123291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db4f10>]}
[0m23:45:03.123563 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:45:03.123731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11874a410>]}
[0m23:45:03.124686 [info ] [MainThread]: 
[0m23:45:03.124833 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:45:03.124948 [info ] [MainThread]: 
[0m23:45:03.125150 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:45:03.126808 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:45:03.126997 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:45:03.127233 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:45:03.153501 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:45:03.153687 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:45:03.153851 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:45:03.154024 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:45:03.154170 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:45:03.154317 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:45:03.154451 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:03.154572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:03.154694 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:03.199984 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.045 seconds
[0m23:45:03.200170 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.046 seconds
[0m23:45:03.200334 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.046 seconds
[0m23:45:03.200867 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:45:03.201300 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:45:03.201679 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:45:03.202723 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:45:03.202928 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m23:45:03.205570 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:45:03.205871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m23:45:03.206124 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:45:03.207320 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:45:03.207515 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:45:03.208346 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:45:03.209172 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:45:03.209299 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:45:03.209423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:03.209552 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:45:03.209671 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:45:03.209785 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:03.209999 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:03.210114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:03.222010 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:45:03.222249 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:45:03.222380 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:45:03.222486 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m23:45:03.222652 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:45:03.222761 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:45:03.222872 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:45:03.222976 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:45:03.223099 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:45:03.223240 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:45:03.223375 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:45:03.223509 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:45:03.225177 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m23:45:03.225334 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:45:03.225460 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:45:03.225597 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m23:45:03.226273 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:45:03.226728 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:45:03.227122 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:45:03.227489 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:45:03.227963 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:45:03.228100 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:45:03.228242 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:45:03.228338 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:45:03.228604 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public)
[0m23:45:03.228821 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_raw_analysis)
[0m23:45:03.230478 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:45:03.231448 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:45:03.231585 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:45:03.231718 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:45:03.231831 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:03.231933 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:03.240635 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m23:45:03.240801 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:45:03.240934 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:45:03.242281 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m23:45:03.242497 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.001 seconds
[0m23:45:03.242658 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:45:03.243197 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:45:03.243351 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:45:03.243722 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:45:03.244638 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m23:45:03.245301 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:45:03.245830 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:45:03.248500 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.248685 [debug] [MainThread]: On master: BEGIN
[0m23:45:03.248798 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:45:03.256308 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:45:03.256559 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.256727 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:45:03.259035 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:45:03.260210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db5650>]}
[0m23:45:03.260463 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:03.260902 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.261147 [debug] [MainThread]: On master: BEGIN
[0m23:45:03.261810 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m23:45:03.261977 [debug] [MainThread]: On master: COMMIT
[0m23:45:03.262095 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.262194 [debug] [MainThread]: On master: COMMIT
[0m23:45:03.262543 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:45:03.262684 [debug] [MainThread]: On master: Close
[0m23:45:03.264352 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:45:03.264537 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:45:03.264892 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:45:03.264748 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:45:03.265084 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_location_profiling
[0m23:45:03.265270 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:45:03.265487 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:45:03.265678 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.company_analysis)
[0m23:45:03.265874 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_location_profiling ........ [RUN]
[0m23:45:03.266070 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_company_profiling)
[0m23:45:03.266241 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_data_profiling)
[0m23:45:03.266391 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:45:03.266547 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_location_profiling)
[0m23:45:03.266713 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:45:03.266856 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:45:03.270582 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:45:03.270757 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m23:45:03.272185 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:45:03.273545 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:45:03.274992 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m23:45:03.275276 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:45:03.275512 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:45:03.275668 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:45:03.281215 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_location_profiling
[0m23:45:03.292487 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:45:03.294208 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:45:03.295854 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:45:03.297510 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m23:45:03.298095 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.298385 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.298656 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.298801 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:45:03.298980 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:45:03.299126 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:45:03.299286 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:45:03.299422 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:45:03.299549 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m23:45:03.299668 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:45:03.299790 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:45:03.300050 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:45:03.312305 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m23:45:03.312503 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:45:03.312645 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m23:45:03.312775 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m23:45:03.312942 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.313075 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:45:03.313201 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.313323 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.313489 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:45:03.313761 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m23:45:03.314032 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:45:03.314276 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:45:03.391125 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.077 seconds
[0m23:45:03.395649 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.395854 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:45:03.396471 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.397683 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.397843 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:45:03.398308 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.404990 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:45:03.405225 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.405370 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:45:03.406772 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.409402 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:45:03.411400 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:45:03.411570 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:45:03.413093 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:45:03.414265 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:45:03.415224 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cefa90>]}
[0m23:45:03.415581 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.15s]
[0m23:45:03.415820 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:45:03.415991 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:45:03.416212 [info ] [Thread-1 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:45:03.416485 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_company_profiling)
[0m23:45:03.416662 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:45:03.418950 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:45:03.419342 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:45:03.420930 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:45:03.421380 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.421531 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:45:03.421667 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:45:03.424370 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.110 seconds
[0m23:45:03.425843 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.426012 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:45:03.426264 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.112 seconds
[0m23:45:03.427694 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.427889 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m23:45:03.428077 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:45:03.429314 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.429492 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m23:45:03.429793 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:45:03.430018 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.430193 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.430490 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:45:03.430772 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.431948 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.432635 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:45:03.432814 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:45:03.432987 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.433164 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:45:03.433504 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.434183 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:45:03.434344 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.434565 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.435838 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:45:03.436066 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:45:03.436411 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:45:03.436607 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:45:03.437148 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.438132 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:45:03.438282 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:45:03.438582 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:45:03.439170 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:45:03.439336 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:45:03.439625 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f05dd0>]}
[0m23:45:03.439912 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.17s]
[0m23:45:03.440146 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:45:03.440311 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:45:03.440451 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:45:03.440671 [info ] [Thread-2 (]: 6 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:45:03.441272 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:45:03.441496 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_data_profiling)
[0m23:45:03.441695 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:45:03.441912 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c6f050>]}
[0m23:45:03.443623 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:45:03.443917 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.18s]
[0m23:45:03.444191 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:45:03.444373 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:45:03.444703 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:45:03.444585 [info ] [Thread-3 (]: 7 of 8 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:45:03.446218 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:45:03.446407 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_location_profiling)
[0m23:45:03.446599 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:45:03.448123 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:45:03.448643 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.448799 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:45:03.448938 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:45:03.449077 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:45:03.451573 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:45:03.452288 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:45:03.452450 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:45:03.452580 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:45:03.458152 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m23:45:03.458343 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:45:03.458548 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.458707 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:45:03.458993 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:45:03.459385 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc

select * from fxf_country_profile
union all
select * from pdl_country_profile
union all
select * from country_data_quality
order by contact_count desc
  );
  
[0m23:45:03.460042 [debug] [Thread-3 (]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 107: )
          ^

[0m23:45:03.460216 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: ROLLBACK
[0m23:45:03.460580 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:45:03.465795 [debug] [Thread-3 (]: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:03.466099 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f7b450>]}
[0m23:45:03.466392 [error] [Thread-3 (]: 7 of 8 ERROR creating sql table model public_staging_analysis.staging_location_profiling  [[31mERROR[0m in 0.02s]
[0m23:45:03.466633 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:45:03.466797 [debug] [Thread-3 (]: Began running node model.dbt_service.data_overview
[0m23:45:03.466985 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.staging_location_profiling' to be skipped because of status 'error'.  Reason: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql.
[0m23:45:03.467185 [info ] [Thread-3 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:45:03.467676 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_location_profiling, now model.dbt_service.data_overview)
[0m23:45:03.467829 [debug] [Thread-3 (]: Began compiling node model.dbt_service.data_overview
[0m23:45:03.469779 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:45:03.470116 [debug] [Thread-3 (]: Began executing node model.dbt_service.data_overview
[0m23:45:03.477258 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:45:03.477645 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:45:03.477840 [debug] [Thread-3 (]: On model.dbt_service.data_overview: BEGIN
[0m23:45:03.477976 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:45:03.483725 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:45:03.483928 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:45:03.484103 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:45:03.485402 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:45:03.486704 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:45:03.486862 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:45:03.487289 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.487974 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:45:03.488117 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:45:03.488243 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m23:45:03.488891 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.490096 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:45:03.491265 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:45:03.491422 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:45:03.491858 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:45:03.492397 [debug] [Thread-3 (]: On model.dbt_service.data_overview: Close
[0m23:45:03.492666 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184e6150>]}
[0m23:45:03.492937 [info ] [Thread-3 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.02s]
[0m23:45:03.493166 [debug] [Thread-3 (]: Finished running node model.dbt_service.data_overview
[0m23:45:03.498179 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.184 seconds
[0m23:45:03.499596 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:45:03.499761 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m23:45:03.500317 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.500876 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:45:03.501029 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:45:03.501160 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:45:03.502092 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.503048 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m23:45:03.503411 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:45:03.503574 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m23:45:03.503989 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:45:03.504495 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: Close
[0m23:45:03.504769 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d9d490>]}
[0m23:45:03.505035 [info ] [Thread-4 (]: 4 of 8 OK created sql table model public_raw_analysis.raw_location_profiling ... [[32mSELECT 152[0m in 0.24s]
[0m23:45:03.505262 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_location_profiling
[0m23:45:03.611373 [debug] [Thread-1 (]: SQL status: SELECT 271 in 0.179 seconds
[0m23:45:03.613007 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.613182 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m23:45:03.613809 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.615030 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.615197 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:45:03.615674 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.616320 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:45:03.616486 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.616624 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:45:03.618121 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.619112 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:45:03.619401 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:45:03.619544 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:45:03.620806 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:45:03.621293 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:45:03.621640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e75c50>]}
[0m23:45:03.622109 [info ] [Thread-1 (]: 5 of 8 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.21s]
[0m23:45:03.622475 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:45:03.628983 [debug] [Thread-2 (]: SQL status: SELECT 2 in 0.169 seconds
[0m23:45:03.632322 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.632484 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:45:03.633073 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.634101 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.634244 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:45:03.634747 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:45:03.635333 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:45:03.635473 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.635596 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:45:03.636299 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:45:03.637283 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:45:03.637549 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:45:03.637681 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:45:03.639457 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:45:03.640074 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:45:03.640370 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '375ad253-81d9-43c0-855c-d2e1f522d935', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a013d10>]}
[0m23:45:03.640673 [info ] [Thread-2 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.20s]
[0m23:45:03.640907 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:45:03.641509 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.641641 [debug] [MainThread]: On master: BEGIN
[0m23:45:03.641756 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:45:03.647088 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m23:45:03.647289 [debug] [MainThread]: On master: COMMIT
[0m23:45:03.647428 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:03.647555 [debug] [MainThread]: On master: COMMIT
[0m23:45:03.647996 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:45:03.648246 [debug] [MainThread]: On master: Close
[0m23:45:03.648464 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:45:03.648591 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m23:45:03.648693 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:45:03.648801 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:45:03.648901 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m23:45:03.649079 [info ] [MainThread]: 
[0m23:45:03.649247 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.52 seconds (0.52s).
[0m23:45:03.649910 [debug] [MainThread]: Command end result
[0m23:45:03.666988 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:45:03.668069 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:45:03.671474 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:45:03.671643 [info ] [MainThread]: 
[0m23:45:03.671815 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:45:03.671946 [info ] [MainThread]: 
[0m23:45:03.672125 [error] [MainThread]: [31mFailure in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)[0m
[0m23:45:03.672297 [error] [MainThread]:   Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:03.672431 [info ] [MainThread]: 
[0m23:45:03.672586 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:03.672715 [info ] [MainThread]: 
[0m23:45:03.672858 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=8
[0m23:45:03.675199 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0404032, "process_in_blocks": "0", "process_kernel_time": 0.228913, "process_mem_max_rss": "141557760", "process_out_blocks": "0", "process_user_time": 1.233299}
[0m23:45:03.675465 [debug] [MainThread]: Command `dbt run` failed at 23:45:03.675423 after 1.04 seconds
[0m23:45:03.675658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d80350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118df4f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118df4410>]}
[0m23:45:03.675822 [debug] [MainThread]: Flushing usage events
[0m23:45:03.952837 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:45:22.206252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cf8f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d6bbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d6bf90>]}


============================== 23:45:22.208332 | 0264aad3-74f0-47db-9a6e-ef03fb4926f8 ==============================
[0m23:45:22.208332 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:45:22.208642 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'static_parser': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'empty': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'invocation_command': 'dbt run --select staging_analysis.staging_location_profiling', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'write_json': 'True', 'no_print': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'log_format': 'default'}
[0m23:45:22.296723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a31dc90>]}
[0m23:45:22.328802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cef3d0>]}
[0m23:45:22.329364 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:45:22.372835 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:45:22.429426 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:45:22.429753 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m23:45:22.559096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a38bd10>]}
[0m23:45:22.625731 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:45:22.626646 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:45:22.638232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab11cd0>]}
[0m23:45:22.638516 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:45:22.638677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b256090>]}
[0m23:45:22.639483 [info ] [MainThread]: 
[0m23:45:22.639640 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:45:22.639760 [info ] [MainThread]: 
[0m23:45:22.639990 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:45:22.640383 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:45:22.665205 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:45:22.665433 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:45:22.665557 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:22.687083 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m23:45:22.687776 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:45:22.689845 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:45:22.690114 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:45:22.692422 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:45:22.692643 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:45:22.692843 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:45:22.693700 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:45:22.693849 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:45:22.694762 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:45:22.695790 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:45:22.695928 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:45:22.696045 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:22.696161 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:45:22.696264 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:45:22.696362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:22.696554 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:22.696686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:22.709354 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:45:22.709517 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:45:22.709629 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:45:22.709748 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:45:22.709918 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:45:22.710043 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:45:22.710154 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:45:22.710269 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:45:22.710422 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:45:22.710530 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:45:22.710677 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:45:22.710828 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:45:22.714426 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m23:45:22.714589 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m23:45:22.715134 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:45:22.715534 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:45:22.715765 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:45:22.716123 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:45:22.716236 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:45:22.716351 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:45:22.716564 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_marts)
[0m23:45:22.716890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public)
[0m23:45:22.717686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:45:22.718001 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:45:22.718111 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.007 seconds
[0m23:45:22.718882 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:45:22.719004 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:45:22.719492 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:45:22.719754 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:45:22.719873 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:22.720119 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:22.720295 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:45:22.730959 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:45:22.731164 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:45:22.731364 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:45:22.731495 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:45:22.731631 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:45:22.731788 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:45:22.736457 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m23:45:22.736604 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.005 seconds
[0m23:45:22.737119 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:45:22.737535 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:45:22.737921 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:45:22.738081 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:45:22.741012 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.741144 [debug] [MainThread]: On master: BEGIN
[0m23:45:22.741243 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:45:22.749186 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m23:45:22.749338 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.749496 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:45:22.752792 [debug] [MainThread]: SQL status: SELECT 10 in 0.003 seconds
[0m23:45:22.753919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b23b410>]}
[0m23:45:22.754120 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:22.754474 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.754607 [debug] [MainThread]: On master: BEGIN
[0m23:45:22.755068 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:45:22.755205 [debug] [MainThread]: On master: COMMIT
[0m23:45:22.755311 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.755411 [debug] [MainThread]: On master: COMMIT
[0m23:45:22.755747 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:45:22.755895 [debug] [MainThread]: On master: Close
[0m23:45:22.757553 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:45:22.757802 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:45:22.758004 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.staging_location_profiling)
[0m23:45:22.758152 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:45:22.761641 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:45:22.762048 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:45:22.777722 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:45:22.778406 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:45:22.778645 [debug] [Thread-1 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:45:22.778780 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:45:22.789415 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m23:45:22.789682 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:45:22.789945 [debug] [Thread-1 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m23:45:22.791102 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 107: )
          ^

[0m23:45:22.791281 [debug] [Thread-1 (]: On model.dbt_service.staging_location_profiling: ROLLBACK
[0m23:45:22.791833 [debug] [Thread-1 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:45:22.796987 [debug] [Thread-1 (]: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:22.798037 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0264aad3-74f0-47db-9a6e-ef03fb4926f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4db450>]}
[0m23:45:22.798401 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model public_staging_analysis.staging_location_profiling  [[31mERROR[0m in 0.04s]
[0m23:45:22.798681 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:45:22.798919 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.staging_location_profiling' to be skipped because of status 'error'.  Reason: Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql.
[0m23:45:22.799764 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.799900 [debug] [MainThread]: On master: BEGIN
[0m23:45:22.800009 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:45:22.807902 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m23:45:22.808096 [debug] [MainThread]: On master: COMMIT
[0m23:45:22.808270 [debug] [MainThread]: Using postgres connection "master"
[0m23:45:22.808450 [debug] [MainThread]: On master: COMMIT
[0m23:45:22.809150 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m23:45:22.809396 [debug] [MainThread]: On master: Close
[0m23:45:22.809597 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:45:22.809719 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m23:45:22.809830 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m23:45:22.809931 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m23:45:22.810050 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m23:45:22.810178 [info ] [MainThread]: 
[0m23:45:22.810328 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m23:45:22.810609 [debug] [MainThread]: Command end result
[0m23:45:22.822769 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:45:22.823464 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:45:22.825669 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:45:22.825790 [info ] [MainThread]: 
[0m23:45:22.825936 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:45:22.826051 [info ] [MainThread]: 
[0m23:45:22.826193 [error] [MainThread]: [31mFailure in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)[0m
[0m23:45:22.826331 [error] [MainThread]:   Database Error in model staging_location_profiling (models/staging_analysis/staging_location_profiling.sql)
  syntax error at or near ")"
  LINE 107: )
            ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:22.826440 [info ] [MainThread]: 
[0m23:45:22.826567 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging_analysis/staging_location_profiling.sql
[0m23:45:22.826673 [info ] [MainThread]: 
[0m23:45:22.826787 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:45:22.829025 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.65663767, "process_in_blocks": "0", "process_kernel_time": 0.204005, "process_mem_max_rss": "142688256", "process_out_blocks": "0", "process_user_time": 1.09501}
[0m23:45:22.829379 [debug] [MainThread]: Command `dbt run` failed at 23:45:22.829337 after 0.66 seconds
[0m23:45:22.829575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102650190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d6bd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d6b750>]}
[0m23:45:22.829749 [debug] [MainThread]: Flushing usage events
[0m23:45:23.053728 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:46:02.046120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105443110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054bf690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054bfd50>]}


============================== 23:46:02.048987 | 2c828766-5802-4d0b-83e4-e53f291bbd25 ==============================
[0m23:46:02.048987 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:46:02.049333 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'static_parser': 'True', 'empty': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'write_json': 'True', 'warn_error': 'None', 'debug': 'False', 'partial_parse': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'target_path': 'None', 'fail_fast': 'False', 'introspect': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'quiet': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80'}
[0m23:46:02.149499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a56990>]}
[0m23:46:02.179791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a8a1d0>]}
[0m23:46:02.180389 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:46:02.226912 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:46:02.289781 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:46:02.290113 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m23:46:02.417052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106614950>]}
[0m23:46:02.481687 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:46:02.482754 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:46:02.494211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106265810>]}
[0m23:46:02.494494 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:46:02.494657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065aa510>]}
[0m23:46:02.495596 [info ] [MainThread]: 
[0m23:46:02.495744 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:46:02.495860 [info ] [MainThread]: 
[0m23:46:02.496066 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:46:02.497892 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:46:02.498111 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:46:02.502606 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:46:02.525242 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:46:02.525448 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:46:02.525602 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:46:02.525788 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:46:02.525944 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:46:02.526090 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:46:02.526232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:02.526380 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:02.526502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:02.565742 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.039 seconds
[0m23:46:02.565933 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.040 seconds
[0m23:46:02.566111 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.040 seconds
[0m23:46:02.566663 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:46:02.567100 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:46:02.567489 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:46:02.568608 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m23:46:02.568855 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m23:46:02.571197 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:46:02.571397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:46:02.571593 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:46:02.572381 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:46:02.572512 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:46:02.573419 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:46:02.574095 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:46:02.574209 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:46:02.574315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:02.574418 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:46:02.574520 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:46:02.574619 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:02.574805 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:02.574946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:02.588079 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:46:02.588227 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:46:02.588353 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:46:02.588465 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:46:02.588584 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:46:02.588695 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:46:02.588799 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:46:02.588925 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:46:02.589046 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:46:02.589174 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:46:02.589341 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:46:02.589521 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:46:02.592126 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m23:46:02.592255 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m23:46:02.592369 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m23:46:02.592477 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:46:02.592957 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:46:02.593355 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:46:02.593710 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:46:02.594054 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:46:02.594714 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:46:02.594839 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:46:02.594947 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:46:02.595048 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:46:02.595237 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m23:46:02.595662 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw)
[0m23:46:02.597298 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:46:02.597977 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:46:02.598088 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:46:02.598189 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:46:02.598291 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:02.598393 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:02.607306 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m23:46:02.607423 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:46:02.607535 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m23:46:02.607656 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:46:02.607782 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:46:02.607954 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:46:02.610446 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:46:02.610932 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:46:02.611073 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:46:02.611508 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:46:02.611636 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:46:02.612037 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:46:02.614316 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:02.614442 [debug] [MainThread]: On master: BEGIN
[0m23:46:02.614549 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:46:02.621812 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m23:46:02.621955 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:02.622113 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:46:02.624334 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:46:02.625415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106963890>]}
[0m23:46:02.625613 [debug] [MainThread]: On master: ROLLBACK
[0m23:46:02.625954 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:02.626091 [debug] [MainThread]: On master: BEGIN
[0m23:46:02.626603 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:46:02.626763 [debug] [MainThread]: On master: COMMIT
[0m23:46:02.626891 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:02.626998 [debug] [MainThread]: On master: COMMIT
[0m23:46:02.627295 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:46:02.627428 [debug] [MainThread]: On master: Close
[0m23:46:02.629466 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:46:02.629644 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:46:02.629982 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:46:02.629847 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:46:02.630184 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_location_profiling
[0m23:46:02.630361 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:46:02.630594 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:46:02.630817 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.company_analysis)
[0m23:46:02.631003 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_location_profiling ........ [RUN]
[0m23:46:02.631174 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_company_profiling)
[0m23:46:02.631308 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_data_profiling)
[0m23:46:02.631447 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:46:02.631583 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_location_profiling)
[0m23:46:02.631715 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:46:02.631852 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:46:02.635052 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:46:02.635214 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m23:46:02.636562 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:46:02.637931 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:46:02.639280 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m23:46:02.639582 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:46:02.655704 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:46:02.655947 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:46:02.656200 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_location_profiling
[0m23:46:02.656358 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:46:02.657827 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:46:02.658030 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.659351 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m23:46:02.660619 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:46:02.660815 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:46:02.661088 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.661273 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:02.661429 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.661583 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:46:02.661813 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.661960 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m23:46:02.662101 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:46:02.662238 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:46:02.662375 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:46:02.662573 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:46:02.674884 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:46:02.675053 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m23:46:02.675190 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m23:46:02.675336 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m23:46:02.675496 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.675641 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.675784 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.675931 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.676171 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m23:46:02.676472 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:46:02.676700 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:46:02.676926 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:46:02.752656 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.075 seconds
[0m23:46:02.757004 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.757188 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:46:02.757844 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.759238 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.759401 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:46:02.759910 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.766486 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:46:02.766672 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.766809 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:46:02.767996 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.770745 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:46:02.772725 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:46:02.772893 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:46:02.774220 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:02.775208 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:46:02.776097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b4fa10>]}
[0m23:46:02.776416 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.14s]
[0m23:46:02.776661 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:46:02.776982 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:46:02.777360 [info ] [Thread-1 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:46:02.777600 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_company_profiling)
[0m23:46:02.777768 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:46:02.779417 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:46:02.779838 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:46:02.781960 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:46:02.782323 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.782465 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:46:02.782603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:02.786484 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.109 seconds
[0m23:46:02.788032 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.788218 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.111 seconds
[0m23:46:02.788457 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:46:02.789880 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.790039 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m23:46:02.790227 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:46:02.790417 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.790589 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.790874 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:46:02.791135 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.792503 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.793704 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.793870 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:46:02.794031 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:46:02.794485 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.794640 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.795284 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:46:02.795941 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:46:02.796105 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.796262 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.796402 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:46:02.796538 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:46:02.797875 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.798027 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.799169 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:46:02.800093 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:46:02.800399 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:46:02.800692 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:46:02.800857 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:46:02.801008 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:46:02.801985 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:02.802119 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:02.802611 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:46:02.803056 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:46:02.803330 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069d0dd0>]}
[0m23:46:02.803549 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a39290>]}
[0m23:46:02.803821 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.17s]
[0m23:46:02.804087 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.17s]
[0m23:46:02.804324 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:46:02.804550 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:46:02.804787 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:46:02.805036 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:46:02.805257 [info ] [Thread-3 (]: 6 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:46:02.805466 [info ] [Thread-2 (]: 7 of 8 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:46:02.805655 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_data_profiling)
[0m23:46:02.805821 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_location_profiling)
[0m23:46:02.805989 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:46:02.806142 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:46:02.807897 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:46:02.809421 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:46:02.809968 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:46:02.810118 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:46:02.811609 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:46:02.814031 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:46:02.814559 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:46:02.814780 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:02.814962 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:46:02.815116 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:46:02.815261 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:46:02.815402 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:46:02.821701 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m23:46:02.821895 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:46:02.822075 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:46:02.822242 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:02.822525 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, state_region, city
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m23:46:02.822922 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:46:02.862195 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.185 seconds
[0m23:46:02.863748 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.863923 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m23:46:02.864432 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.865535 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.865682 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m23:46:02.866084 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.866701 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:46:02.866847 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.866987 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:46:02.867753 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.868779 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m23:46:02.869083 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:46:02.869238 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m23:46:02.870093 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:02.870637 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: Close
[0m23:46:02.870897 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b91d0>]}
[0m23:46:02.871164 [info ] [Thread-4 (]: 4 of 8 OK created sql table model public_raw_analysis.raw_location_profiling ... [[32mSELECT 152[0m in 0.24s]
[0m23:46:02.871397 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_location_profiling
[0m23:46:02.871562 [debug] [Thread-4 (]: Began running node model.dbt_service.data_overview
[0m23:46:02.871765 [info ] [Thread-4 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:46:02.871935 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_location_profiling, now model.dbt_service.data_overview)
[0m23:46:02.872078 [debug] [Thread-4 (]: Began compiling node model.dbt_service.data_overview
[0m23:46:02.873823 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:46:02.874184 [debug] [Thread-4 (]: Began executing node model.dbt_service.data_overview
[0m23:46:02.881750 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:46:02.882137 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:46:02.882287 [debug] [Thread-4 (]: On model.dbt_service.data_overview: BEGIN
[0m23:46:02.882417 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:46:02.888336 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:46:02.888593 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:46:02.888788 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:46:02.890025 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:46:02.891309 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:46:02.891445 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:46:02.891885 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.892388 [debug] [Thread-4 (]: On model.dbt_service.data_overview: COMMIT
[0m23:46:02.892525 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:46:02.892650 [debug] [Thread-4 (]: On model.dbt_service.data_overview: COMMIT
[0m23:46:02.893432 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.894390 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:46:02.895753 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:46:02.895977 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:46:02.896419 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:46:02.896988 [debug] [Thread-4 (]: On model.dbt_service.data_overview: Close
[0m23:46:02.897261 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a91150>]}
[0m23:46:02.897530 [info ] [Thread-4 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:46:02.897757 [debug] [Thread-4 (]: Finished running node model.dbt_service.data_overview
[0m23:46:02.976708 [debug] [Thread-1 (]: SQL status: SELECT 271 in 0.184 seconds
[0m23:46:02.978451 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.978638 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m23:46:02.979208 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.980452 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.980608 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:46:02.981097 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:02.981863 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:46:02.982100 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.982306 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:46:02.983860 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:02.985126 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:46:02.985406 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:46:02.985538 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:46:02.986636 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:02.987237 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:46:02.987510 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b1aa10>]}
[0m23:46:02.987784 [info ] [Thread-1 (]: 5 of 8 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.21s]
[0m23:46:02.988032 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:46:02.995225 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.172 seconds
[0m23:46:02.997963 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:02.998137 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:46:02.998703 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:03.000112 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:03.000335 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:46:03.000830 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:03.001384 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:46:03.001621 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:03.001859 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:46:03.002518 [debug] [Thread-3 (]: SQL status: COMMIT in 0.000 seconds
[0m23:46:03.003743 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:46:03.003912 [debug] [Thread-2 (]: SQL status: SELECT 152 in 0.181 seconds
[0m23:46:03.004188 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:46:03.005537 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:46:03.005731 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:46:03.005972 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m23:46:03.006572 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:46:03.007323 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:46:03.007508 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:46:03.007692 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:46:03.008379 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:46:03.008598 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:46:03.008947 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a6f890>]}
[0m23:46:03.009211 [info ] [Thread-3 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.20s]
[0m23:46:03.009451 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:46:03.009673 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:46:03.010579 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m23:46:03.010875 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:46:03.011018 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m23:46:03.011439 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.000 seconds
[0m23:46:03.011878 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:46:03.012091 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c828766-5802-4d0b-83e4-e53f291bbd25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac89d0>]}
[0m23:46:03.012334 [info ] [Thread-2 (]: 7 of 8 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.21s]
[0m23:46:03.012556 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:46:03.013177 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:03.013314 [debug] [MainThread]: On master: BEGIN
[0m23:46:03.013428 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:46:03.019277 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:46:03.019489 [debug] [MainThread]: On master: COMMIT
[0m23:46:03.019646 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:03.019793 [debug] [MainThread]: On master: COMMIT
[0m23:46:03.020204 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:46:03.020443 [debug] [MainThread]: On master: Close
[0m23:46:03.020654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:46:03.020783 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m23:46:03.020882 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m23:46:03.020983 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:46:03.021084 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:46:03.021250 [info ] [MainThread]: 
[0m23:46:03.021400 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.53 seconds (0.53s).
[0m23:46:03.022013 [debug] [MainThread]: Command end result
[0m23:46:03.037469 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:46:03.038502 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:46:03.041658 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:46:03.041812 [info ] [MainThread]: 
[0m23:46:03.041962 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:46:03.042075 [info ] [MainThread]: 
[0m23:46:03.042189 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m23:46:03.044385 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0343356, "process_in_blocks": "0", "process_kernel_time": 0.233321, "process_mem_max_rss": "142163968", "process_out_blocks": "0", "process_user_time": 1.216972}
[0m23:46:03.044744 [debug] [MainThread]: Command `dbt run` succeeded at 23:46:03.044697 after 1.03 seconds
[0m23:46:03.044918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054efcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a34350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100c0ac10>]}
[0m23:46:03.045082 [debug] [MainThread]: Flushing usage events
[0m23:46:03.425336 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:46:44.542686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cb2a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d22350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d2fe10>]}


============================== 23:46:44.545236 | 2be30f9c-ad6f-470e-bad7-a0b3024ee031 ==============================
[0m23:46:44.545236 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:46:44.545540 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'empty': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'write_json': 'True', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test', 'fail_fast': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'target_path': 'None', 'warn_error': 'None', 'printer_width': '80', 'no_print': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_experimental_parser': 'False'}
[0m23:46:44.643891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11428c510>]}
[0m23:46:44.674256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100f9450>]}
[0m23:46:44.675227 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:46:44.722232 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:46:44.786483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:46:44.786698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:46:44.809641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d27750>]}
[0m23:46:44.849337 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:46:44.850439 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:46:44.864815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151a6890>]}
[0m23:46:44.865058 [info ] [MainThread]: Found 13 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:46:44.865220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bb4d90>]}
[0m23:46:44.866285 [info ] [MainThread]: 
[0m23:46:44.866441 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:46:44.866557 [info ] [MainThread]: 
[0m23:46:44.866779 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:46:44.868729 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:46:44.868963 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:46:44.873888 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:46:44.874148 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:46:44.919359 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:46:44.919552 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:46:44.919709 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:46:44.919846 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:46:44.920002 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:46:44.920152 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:46:44.920281 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:46:44.920406 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:46:44.920542 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:44.920665 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:44.920778 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:44.920900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:44.963248 [debug] [ThreadPool]: SQL status: BEGIN in 0.042 seconds
[0m23:46:44.963448 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:46:44.963572 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:46:44.963735 [debug] [ThreadPool]: SQL status: BEGIN in 0.043 seconds
[0m23:46:44.963885 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:46:44.964033 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:46:44.964511 [debug] [ThreadPool]: SQL status: BEGIN in 0.044 seconds
[0m23:46:44.964643 [debug] [ThreadPool]: SQL status: BEGIN in 0.044 seconds
[0m23:46:44.964741 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:46:44.964847 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:46:44.964974 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:46:44.965108 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:46:44.967026 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:46:44.967594 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:46:44.967740 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.004 seconds
[0m23:46:44.968191 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:46:44.968340 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m23:46:44.968486 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:46:44.968588 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:46:44.969020 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:46:44.969388 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:46:44.969491 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:46:44.969724 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_raw_analysis)
[0m23:46:44.970800 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:46:44.970981 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_staging)
[0m23:46:44.971100 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:46:44.971231 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:46:44.971579 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:46:44.972290 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:46:44.973104 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:44.973232 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:46:44.973416 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:44.980246 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:46:44.980381 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:46:44.980526 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:46:44.980646 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:46:44.980788 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:46:44.980947 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:46:44.982804 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:46:44.982989 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m23:46:44.983509 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:46:44.983950 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:46:44.984353 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:46:44.984480 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:46:44.987292 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:44.987425 [debug] [MainThread]: On master: BEGIN
[0m23:46:44.987526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:46:44.993002 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m23:46:44.993143 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:44.993296 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:46:44.995462 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:46:44.996531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2be30f9c-ad6f-470e-bad7-a0b3024ee031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114348050>]}
[0m23:46:44.996728 [debug] [MainThread]: On master: ROLLBACK
[0m23:46:44.997073 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:44.997198 [debug] [MainThread]: On master: BEGIN
[0m23:46:44.997665 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:46:44.997820 [debug] [MainThread]: On master: COMMIT
[0m23:46:44.997938 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:44.998068 [debug] [MainThread]: On master: COMMIT
[0m23:46:44.998382 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:46:44.998515 [debug] [MainThread]: On master: Close
[0m23:46:45.000089 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:46:45.000359 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:46:45.000660 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:46:45.000521 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m23:46:45.000873 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:46:45.001033 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m23:46:45.001198 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m23:46:45.001379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m23:46:45.001530 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m23:46:45.001705 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m23:46:45.001864 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m23:46:45.002019 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:46:45.002172 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m23:46:45.002315 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:46:45.002454 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:46:45.009051 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:46:45.009237 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:46:45.010677 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:46:45.012037 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:46:45.013710 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:46:45.014038 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:46:45.021613 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:46:45.021810 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:46:45.021993 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:46:45.022126 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:46:45.023174 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:46:45.024082 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:46:45.024990 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:46:45.025146 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:46:45.025471 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:46:45.025626 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m23:46:45.025794 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:46:45.025954 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m23:46:45.026123 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:46:45.026270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:45.026414 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m23:46:45.026564 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:46:45.026706 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m23:46:45.026919 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:46:45.027107 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:46:45.043098 [debug] [Thread-4 (]: SQL status: BEGIN in 0.016 seconds
[0m23:46:45.043254 [debug] [Thread-2 (]: SQL status: BEGIN in 0.016 seconds
[0m23:46:45.043392 [debug] [Thread-3 (]: SQL status: BEGIN in 0.017 seconds
[0m23:46:45.043510 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m23:46:45.043644 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:46:45.043778 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:46:45.043905 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:46:45.044025 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:46:45.044161 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.044303 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.044443 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.044584 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.046230 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:46:45.046378 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:46:45.048021 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m23:46:45.048511 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m23:46:45.049563 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m23:46:45.049694 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m23:46:45.049926 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m23:46:45.050118 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m23:46:45.050407 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:46:45.050585 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:46:45.050745 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:46:45.050964 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:46:45.051140 [info ] [Thread-2 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m23:46:45.051308 [info ] [Thread-1 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m23:46:45.051485 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m23:46:45.051638 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m23:46:45.051770 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:46:45.051899 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:46:45.053688 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:46:45.053863 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.009 seconds
[0m23:46:45.053994 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.009 seconds
[0m23:46:45.055555 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:46:45.056393 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m23:46:45.056974 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m23:46:45.057285 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:46:45.058400 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:46:45.058567 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m23:46:45.058735 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:46:45.058865 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m23:46:45.060098 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:46:45.059165 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.06s]
[0m23:46:45.060542 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:46:45.060379 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.06s]
[0m23:46:45.060805 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:46:45.060959 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m23:46:45.061172 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:46:45.061321 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:46:45.061499 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:46:45.061683 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:46:45.061839 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:46:45.061991 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m23:46:45.062141 [info ] [Thread-4 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m23:46:45.062509 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:45.062383 [info ] [Thread-3 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m23:46:45.062722 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m23:46:45.062955 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m23:46:45.063120 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:46:45.063269 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:46:45.064973 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:46:45.067572 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:46:45.068134 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:46:45.069334 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:46:45.069489 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:46:45.069659 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m23:46:45.069823 [debug] [Thread-2 (]: SQL status: BEGIN in 0.008 seconds
[0m23:46:45.072068 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:46:45.072309 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:46:45.072493 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:46:45.072675 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:46:45.072915 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.073114 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.073305 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:46:45.073463 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m23:46:45.073660 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m23:46:45.073802 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:46:45.073940 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:46:45.078197 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.005 seconds
[0m23:46:45.078882 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m23:46:45.079054 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.005 seconds
[0m23:46:45.079606 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m23:46:45.079749 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m23:46:45.080151 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m23:46:45.080035 [info ] [Thread-1 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.080595 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:46:45.080407 [info ] [Thread-2 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.080811 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:46:45.081038 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:46:45.081209 [info ] [Thread-1 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m23:46:45.081390 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:46:45.081569 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m23:46:45.081720 [info ] [Thread-2 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m23:46:45.081889 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:46:45.082053 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m23:46:45.083858 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:46:45.084148 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:46:45.086179 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:46:45.086368 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m23:46:45.086557 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m23:46:45.086724 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:46:45.086920 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:46:45.087072 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:46:45.087255 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.087411 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:46:45.087559 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:46:45.088691 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:46:45.089780 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:46:45.089947 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:46:45.090737 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m23:46:45.090922 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:46:45.091114 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m23:46:45.091271 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:46:45.091402 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:45.091538 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m23:46:45.091690 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m23:46:45.092267 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:46:45.092122 [info ] [Thread-3 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.092654 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:46:45.092836 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:46:45.092992 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:46:45.093182 [info ] [Thread-3 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m23:46:45.093838 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m23:46:45.094026 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m23:46:45.094212 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:46:45.096005 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:46:45.096149 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m23:46:45.096477 [info ] [Thread-4 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.096785 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:46:45.096952 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:46:45.097136 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:46:45.098250 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:46:45.098468 [info ] [Thread-4 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m23:46:45.098776 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m23:46:45.098948 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m23:46:45.099093 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m23:46:45.099257 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:46:45.099414 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:46:45.099592 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:46:45.099743 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:46:45.101430 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:46:45.101664 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.101861 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m23:46:45.102039 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.102310 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:46:45.102606 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:46:45.103735 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:46:45.103954 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:46:45.104675 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m23:46:45.104940 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:46:45.105136 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m23:46:45.105286 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m23:46:45.105458 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:46:45.105766 [info ] [Thread-1 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.02s]
[0m23:46:45.106254 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:46:45.106427 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:46:45.106615 [info ] [Thread-1 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m23:46:45.106801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m23:46:45.106946 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:46:45.108625 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:46:45.109021 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:46:45.109222 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m23:46:45.110385 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:46:45.110576 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:46:45.110790 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.111148 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:46:45.111338 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m23:46:45.111492 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:46:45.111655 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:46:45.111806 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:46:45.112021 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.113776 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.011 seconds
[0m23:46:45.114515 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m23:46:45.114885 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m23:46:45.115163 [info ] [Thread-2 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.115426 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:46:45.117698 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:46:45.117887 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:46:45.118049 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:46:45.121442 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:46:45.122143 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m23:46:45.122434 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:46:45.122602 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m23:46:45.123229 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m23:46:45.123515 [info ] [Thread-3 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.123852 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:46:45.124023 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m23:46:45.124344 [info ] [Thread-4 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:46:45.124650 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:46:45.127941 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:46:45.128646 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m23:46:45.129257 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m23:46:45.129573 [info ] [Thread-1 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.02s]
[0m23:46:45.129908 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:46:45.130754 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:45.130877 [debug] [MainThread]: On master: BEGIN
[0m23:46:45.130978 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:46:45.137488 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:46:45.137786 [debug] [MainThread]: On master: COMMIT
[0m23:46:45.137919 [debug] [MainThread]: Using postgres connection "master"
[0m23:46:45.138035 [debug] [MainThread]: On master: COMMIT
[0m23:46:45.138446 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:46:45.138628 [debug] [MainThread]: On master: Close
[0m23:46:45.138849 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:46:45.138978 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m23:46:45.139105 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m23:46:45.139211 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m23:46:45.139312 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m23:46:45.139479 [info ] [MainThread]: 
[0m23:46:45.139642 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:46:45.140514 [debug] [MainThread]: Command end result
[0m23:46:45.153001 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:46:45.154019 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:46:45.156971 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:46:45.157137 [info ] [MainThread]: 
[0m23:46:45.157282 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:46:45.157396 [info ] [MainThread]: 
[0m23:46:45.157510 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m23:46:45.159829 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6535627, "process_in_blocks": "0", "process_kernel_time": 0.211509, "process_mem_max_rss": "136282112", "process_out_blocks": "0", "process_user_time": 1.103927}
[0m23:46:45.160220 [debug] [MainThread]: Command `dbt test` succeeded at 23:46:45.160168 after 0.65 seconds
[0m23:46:45.160410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e00190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d2f8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e37690>]}
[0m23:46:45.160594 [debug] [MainThread]: Flushing usage events
[0m23:46:45.440161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:51:56.816215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ecf0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f4f950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f4ff90>]}


============================== 23:51:56.818963 | 2579b094-f04b-4d7f-b032-2879379b1dda ==============================
[0m23:51:56.818963 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:51:56.819318 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'partial_parse': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'invocation_command': 'dbt run --select staging_analysis.location_standardization', 'no_print': 'None', 'static_parser': 'True', 'introspect': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'printer_width': '80', 'write_json': 'True', 'log_format': 'default', 'empty': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True'}
[0m23:51:56.947343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107527090>]}
[0m23:51:56.977343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045ae810>]}
[0m23:51:56.977998 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:51:57.024363 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:51:57.088968 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 2 files changed.
[0m23:51:57.089310 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/location_standardization.sql
[0m23:51:57.089483 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/raw_analysis/raw_location_profiling.sql
[0m23:51:57.089635 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m23:51:57.219566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f8f750>]}
[0m23:51:57.285837 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:51:57.287009 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:51:57.299174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075c7150>]}
[0m23:51:57.299436 [info ] [MainThread]: Found 14 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:51:57.299600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f42f10>]}
[0m23:51:57.300322 [info ] [MainThread]: 
[0m23:51:57.300472 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:51:57.300588 [info ] [MainThread]: 
[0m23:51:57.300785 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:51:57.301141 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:51:57.327647 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:51:57.327869 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:51:57.327997 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:57.358855 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m23:51:57.359476 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:51:57.361697 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m23:51:57.361919 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:51:57.364341 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:51:57.364508 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:51:57.364694 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:51:57.365621 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:51:57.365793 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:51:57.366718 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:51:57.367379 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:51:57.367492 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:51:57.367618 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:57.367762 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:51:57.367891 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:51:57.368002 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:57.368209 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:57.368367 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:57.382107 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:51:57.382307 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:51:57.382429 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:51:57.382532 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:51:57.382633 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m23:51:57.382752 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:51:57.382882 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:51:57.383011 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:51:57.383132 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:51:57.383264 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:51:57.383400 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:51:57.383584 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:51:57.387960 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m23:51:57.388168 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.004 seconds
[0m23:51:57.388380 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m23:51:57.388495 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m23:51:57.389116 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:51:57.389571 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:51:57.389965 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:51:57.390355 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:51:57.390892 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:51:57.391033 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:51:57.391145 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:51:57.391250 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:51:57.391495 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging_analysis)
[0m23:51:57.391703 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw)
[0m23:51:57.393848 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:51:57.394599 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:51:57.394730 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:51:57.394866 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:51:57.394989 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:57.395093 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:57.401698 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:51:57.401925 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:51:57.402085 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:51:57.402258 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:51:57.402401 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:51:57.402554 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:51:57.404480 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m23:51:57.404633 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:51:57.405131 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:51:57.405528 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:51:57.405890 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:51:57.406033 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:51:57.409145 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.409319 [debug] [MainThread]: On master: BEGIN
[0m23:51:57.409427 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:51:57.415064 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:51:57.415253 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.415434 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:51:57.417154 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:51:57.418437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10801c510>]}
[0m23:51:57.418672 [debug] [MainThread]: On master: ROLLBACK
[0m23:51:57.419073 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.419204 [debug] [MainThread]: On master: BEGIN
[0m23:51:57.419717 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:51:57.419849 [debug] [MainThread]: On master: COMMIT
[0m23:51:57.419966 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.420073 [debug] [MainThread]: On master: COMMIT
[0m23:51:57.420410 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:51:57.420554 [debug] [MainThread]: On master: Close
[0m23:51:57.422809 [debug] [Thread-1 (]: Began running node model.dbt_service.location_standardization
[0m23:51:57.423088 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging_analysis.location_standardization .. [RUN]
[0m23:51:57.423330 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.location_standardization)
[0m23:51:57.423505 [debug] [Thread-1 (]: Began compiling node model.dbt_service.location_standardization
[0m23:51:57.427196 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.location_standardization"
[0m23:51:57.427959 [debug] [Thread-1 (]: Began executing node model.dbt_service.location_standardization
[0m23:51:57.444220 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.location_standardization"
[0m23:51:57.445000 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:51:57.445273 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: BEGIN
[0m23:51:57.445413 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:51:57.453338 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m23:51:57.453572 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:51:57.453796 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */

  
    

  create  table "finny_db"."public_staging_analysis"."location_standardization__dbt_tmp"
  
  
    as
  
  (
    -- Location standardization mapping
-- This model provides consistent state/region mapping to ISO 2-letter codes



with state_mapping as (
    select state_name, iso_code from (
        values
        ('California', 'CA'),
        ('Texas', 'TX'),
        ('Colorado', 'CO'),
        ('Georgia', 'GA'),
        ('New York', 'NY'),
        ('Illinois', 'IL'),
        ('Washington', 'WA'),
        ('Tennessee', 'TN'),
        ('Massachusetts', 'MA'),
        ('Florida', 'FL'),
        ('North Carolina', 'NC'),
        ('Virginia', 'VA'),
        ('Pennsylvania', 'PA'),
        ('Ohio', 'OH'),
        ('Michigan', 'MI'),
        ('Arizona', 'AZ'),
        ('Nevada', 'NV'),
        ('Oregon', 'OR'),
        ('Utah', 'UT'),
        ('Wisconsin', 'WI'),
        ('Minnesota', 'MN'),
        ('Maryland', 'MD'),
        ('Connecticut', 'CT'),
        ('New Jersey', 'NJ'),
        ('Indiana', 'IN'),
        ('Missouri', 'MO'),
        ('Alabama', 'AL'),
        ('Louisiana', 'LA'),
        ('Kentucky', 'KY'),
        ('South Carolina', 'SC'),
        ('Iowa', 'IA'),
        ('Arkansas', 'AR'),
        ('Kansas', 'KS'),
        ('Nebraska', 'NE'),
        ('Oklahoma', 'OK'),
        ('Mississippi', 'MS'),
        ('Delaware', 'DE'),
        ('Rhode Island', 'RI'),
        ('New Hampshire', 'NH'),
        ('Vermont', 'VT'),
        ('Maine', 'ME'),
        ('Montana', 'MT'),
        ('North Dakota', 'ND'),
        ('South Dakota', 'SD'),
        ('Wyoming', 'WY'),
        ('Idaho', 'ID'),
        ('Alaska', 'AK'),
        ('Hawaii', 'HI'),
        ('West Virginia', 'WV'),
        ('New Mexico', 'NM'),
        ('District of Columbia', 'DC'),
        ('Washington DC', 'DC'),
        ('Washington D.C.', 'DC')
    ) as t(state_name, iso_code)
),

standardization_rules as (
    select 
        state_name,
        iso_code,
        -- Create case-insensitive matching
        upper(state_name) as state_name_upper,
        upper(iso_code) as iso_code_upper
    from state_mapping
    
    union all
    
    -- Add entries where ISO codes map to themselves
    select 
        iso_code as state_name,
        iso_code,
        upper(iso_code) as state_name_upper,
        upper(iso_code) as iso_code_upper
    from state_mapping
)

select * from standardization_rules
  );
  
[0m23:51:57.457532 [debug] [Thread-1 (]: SQL status: SELECT 106 in 0.004 seconds
[0m23:51:57.461645 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:51:57.461809 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */
alter table "finny_db"."public_staging_analysis"."location_standardization__dbt_tmp" rename to "location_standardization"
[0m23:51:57.462399 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:51:57.467715 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: COMMIT
[0m23:51:57.467892 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:51:57.468029 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: COMMIT
[0m23:51:57.468714 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:51:57.471858 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."location_standardization__dbt_backup"
[0m23:51:57.473725 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:51:57.473885 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */
drop table if exists "finny_db"."public_staging_analysis"."location_standardization__dbt_backup" cascade
[0m23:51:57.474975 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:51:57.475863 [debug] [Thread-1 (]: On model.dbt_service.location_standardization: Close
[0m23:51:57.476736 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2579b094-f04b-4d7f-b032-2879379b1dda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084cd4d0>]}
[0m23:51:57.477022 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging_analysis.location_standardization  [[32mSELECT 106[0m in 0.05s]
[0m23:51:57.477266 [debug] [Thread-1 (]: Finished running node model.dbt_service.location_standardization
[0m23:51:57.477848 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.477970 [debug] [MainThread]: On master: BEGIN
[0m23:51:57.478123 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:51:57.483226 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m23:51:57.483398 [debug] [MainThread]: On master: COMMIT
[0m23:51:57.483520 [debug] [MainThread]: Using postgres connection "master"
[0m23:51:57.483628 [debug] [MainThread]: On master: COMMIT
[0m23:51:57.484027 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:51:57.484138 [debug] [MainThread]: On master: Close
[0m23:51:57.484318 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:51:57.484424 [debug] [MainThread]: Connection 'model.dbt_service.location_standardization' was properly closed.
[0m23:51:57.484514 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m23:51:57.484617 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m23:51:57.484715 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m23:51:57.484836 [info ] [MainThread]: 
[0m23:51:57.484981 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m23:51:57.485253 [debug] [MainThread]: Command end result
[0m23:51:57.497426 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:51:57.498395 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:51:57.501287 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:51:57.501463 [info ] [MainThread]: 
[0m23:51:57.501647 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:51:57.501770 [info ] [MainThread]: 
[0m23:51:57.501900 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:51:57.503931 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7283977, "process_in_blocks": "0", "process_kernel_time": 0.220023, "process_mem_max_rss": "139804672", "process_out_blocks": "0", "process_user_time": 1.080067}
[0m23:51:57.504141 [debug] [MainThread]: Command `dbt run` succeeded at 23:51:57.504108 after 0.73 seconds
[0m23:51:57.504298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1024b0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f4fd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10257ea10>]}
[0m23:51:57.504440 [debug] [MainThread]: Flushing usage events
[0m23:51:57.816310 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:52:04.229819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160f7850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116137510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116137c10>]}


============================== 23:52:04.231896 | 10f282b6-57cd-4102-bc43-371373704e46 ==============================
[0m23:52:04.231896 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:52:04.232219 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_cache_events': 'False', 'quiet': 'False', 'empty': 'False', 'version_check': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'warn_error': 'None', 'introspect': 'True', 'write_json': 'True', 'partial_parse': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'debug': 'False', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+', 'log_format': 'default', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs'}
[0m23:52:04.316630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116184690>]}
[0m23:52:04.347474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115a133d0>]}
[0m23:52:04.348035 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:52:04.392412 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:52:04.447532 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:52:04.447755 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:52:04.470700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166d3ed0>]}
[0m23:52:04.511583 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:52:04.512832 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:52:04.520157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e74d90>]}
[0m23:52:04.520646 [info ] [MainThread]: Found 14 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:52:04.520869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116fbf690>]}
[0m23:52:04.522125 [info ] [MainThread]: 
[0m23:52:04.522313 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:52:04.522425 [info ] [MainThread]: 
[0m23:52:04.522632 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:52:04.524366 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:52:04.524687 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:52:04.530352 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:52:04.573546 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:52:04.573756 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:52:04.573945 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:52:04.574085 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:52:04.574238 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:52:04.574378 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:52:04.574500 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:04.574615 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:04.574715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:04.605259 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m23:52:04.605468 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m23:52:04.605636 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m23:52:04.606179 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:52:04.606619 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:52:04.607007 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:52:04.608082 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m23:52:04.608298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m23:52:04.610757 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:52:04.610968 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m23:52:04.611718 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:52:04.611946 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:52:04.612174 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:52:04.612980 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:52:04.613118 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:52:04.614140 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:52:04.614266 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:04.614379 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:52:04.614484 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:04.614594 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:52:04.614778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:04.614982 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:04.627419 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:52:04.627555 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:52:04.627704 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:52:04.627820 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:52:04.627941 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:52:04.628060 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:52:04.628169 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:52:04.628296 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:52:04.628433 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:52:04.628549 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:52:04.628670 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:52:04.628878 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:52:04.632958 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m23:52:04.633134 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m23:52:04.633724 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:52:04.633890 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m23:52:04.634045 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.005 seconds
[0m23:52:04.634414 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:52:04.634950 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:52:04.635091 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:52:04.635508 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:52:04.635814 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_staging)
[0m23:52:04.636331 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:52:04.636457 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:52:04.636562 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:52:04.637445 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:52:04.637686 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_marts)
[0m23:52:04.638852 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:52:04.639854 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:52:04.640042 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:04.640174 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:52:04.640386 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:04.650934 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:52:04.651104 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:52:04.651259 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:52:04.651366 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:52:04.651500 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:52:04.651639 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:52:04.653515 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:52:04.654115 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:52:04.654251 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m23:52:04.654714 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:52:04.654870 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:52:04.655209 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:52:04.657576 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:04.657704 [debug] [MainThread]: On master: BEGIN
[0m23:52:04.657798 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:52:04.663406 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:52:04.663559 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:04.663726 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:52:04.666144 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:52:04.667176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116591b10>]}
[0m23:52:04.667373 [debug] [MainThread]: On master: ROLLBACK
[0m23:52:04.667750 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:04.667876 [debug] [MainThread]: On master: BEGIN
[0m23:52:04.668347 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:52:04.668471 [debug] [MainThread]: On master: COMMIT
[0m23:52:04.668577 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:04.668688 [debug] [MainThread]: On master: COMMIT
[0m23:52:04.669030 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:52:04.669174 [debug] [MainThread]: On master: Close
[0m23:52:04.670368 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:52:04.670534 [debug] [Thread-2 (]: Began running node model.dbt_service.location_standardization
[0m23:52:04.670875 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:52:04.671038 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:52:04.670739 [info ] [Thread-1 (]: 1 of 9 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:52:04.671263 [info ] [Thread-2 (]: 2 of 9 START sql table model public_staging_analysis.location_standardization .. [RUN]
[0m23:52:04.671453 [info ] [Thread-3 (]: 3 of 9 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:52:04.671633 [info ] [Thread-4 (]: 4 of 9 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:52:04.671823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.company_analysis)
[0m23:52:04.671967 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.location_standardization)
[0m23:52:04.672106 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_company_profiling)
[0m23:52:04.672244 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_data_profiling)
[0m23:52:04.672388 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:52:04.672522 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_standardization
[0m23:52:04.672652 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:52:04.672780 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:52:04.676263 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:52:04.677423 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_standardization"
[0m23:52:04.679011 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:52:04.680371 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:52:04.681805 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:52:04.681968 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:52:04.682103 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:52:04.682248 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_standardization
[0m23:52:04.697832 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:52:04.700274 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:52:04.701598 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:52:04.703034 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_standardization"
[0m23:52:04.703656 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.703846 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.704029 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.704181 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:52:04.704360 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:52:04.704524 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.704663 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:52:04.704817 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:52:04.704952 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:52:04.705087 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: BEGIN
[0m23:52:04.705230 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:52:04.705533 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:52:04.719385 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m23:52:04.719540 [debug] [Thread-3 (]: SQL status: BEGIN in 0.015 seconds
[0m23:52:04.719718 [debug] [Thread-2 (]: SQL status: BEGIN in 0.014 seconds
[0m23:52:04.719864 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.719994 [debug] [Thread-4 (]: SQL status: BEGIN in 0.015 seconds
[0m23:52:04.720148 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.720280 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.720446 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:52:04.720606 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.720790 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:52:04.721036 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */

  
    

  create  table "finny_db"."public_staging_analysis"."location_standardization__dbt_tmp"
  
  
    as
  
  (
    -- Location standardization mapping
-- This model provides consistent state/region mapping to ISO 2-letter codes



with state_mapping as (
    select state_name, iso_code from (
        values
        ('California', 'CA'),
        ('Texas', 'TX'),
        ('Colorado', 'CO'),
        ('Georgia', 'GA'),
        ('New York', 'NY'),
        ('Illinois', 'IL'),
        ('Washington', 'WA'),
        ('Tennessee', 'TN'),
        ('Massachusetts', 'MA'),
        ('Florida', 'FL'),
        ('North Carolina', 'NC'),
        ('Virginia', 'VA'),
        ('Pennsylvania', 'PA'),
        ('Ohio', 'OH'),
        ('Michigan', 'MI'),
        ('Arizona', 'AZ'),
        ('Nevada', 'NV'),
        ('Oregon', 'OR'),
        ('Utah', 'UT'),
        ('Wisconsin', 'WI'),
        ('Minnesota', 'MN'),
        ('Maryland', 'MD'),
        ('Connecticut', 'CT'),
        ('New Jersey', 'NJ'),
        ('Indiana', 'IN'),
        ('Missouri', 'MO'),
        ('Alabama', 'AL'),
        ('Louisiana', 'LA'),
        ('Kentucky', 'KY'),
        ('South Carolina', 'SC'),
        ('Iowa', 'IA'),
        ('Arkansas', 'AR'),
        ('Kansas', 'KS'),
        ('Nebraska', 'NE'),
        ('Oklahoma', 'OK'),
        ('Mississippi', 'MS'),
        ('Delaware', 'DE'),
        ('Rhode Island', 'RI'),
        ('New Hampshire', 'NH'),
        ('Vermont', 'VT'),
        ('Maine', 'ME'),
        ('Montana', 'MT'),
        ('North Dakota', 'ND'),
        ('South Dakota', 'SD'),
        ('Wyoming', 'WY'),
        ('Idaho', 'ID'),
        ('Alaska', 'AK'),
        ('Hawaii', 'HI'),
        ('West Virginia', 'WV'),
        ('New Mexico', 'NM'),
        ('District of Columbia', 'DC'),
        ('Washington DC', 'DC'),
        ('Washington D.C.', 'DC')
    ) as t(state_name, iso_code)
),

standardization_rules as (
    select 
        state_name,
        iso_code,
        -- Create case-insensitive matching
        upper(state_name) as state_name_upper,
        upper(iso_code) as iso_code_upper
    from state_mapping
    
    union all
    
    -- Add entries where ISO codes map to themselves
    select 
        iso_code as state_name,
        iso_code,
        upper(iso_code) as state_name_upper,
        upper(iso_code) as iso_code_upper
    from state_mapping
)

select * from standardization_rules
  );
  
[0m23:52:04.721306 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:52:04.726330 [debug] [Thread-2 (]: SQL status: SELECT 106 in 0.005 seconds
[0m23:52:04.730743 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.730955 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */
alter table "finny_db"."public_staging_analysis"."location_standardization" rename to "location_standardization__dbt_backup"
[0m23:52:04.731476 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.732653 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.732800 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */
alter table "finny_db"."public_staging_analysis"."location_standardization__dbt_tmp" rename to "location_standardization"
[0m23:52:04.733207 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.739836 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: COMMIT
[0m23:52:04.740064 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.740210 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: COMMIT
[0m23:52:04.741332 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:04.743864 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."location_standardization__dbt_backup"
[0m23:52:04.745794 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_standardization"
[0m23:52:04.745979 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_standardization"} */
drop table if exists "finny_db"."public_staging_analysis"."location_standardization__dbt_backup" cascade
[0m23:52:04.747274 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:04.748322 [debug] [Thread-2 (]: On model.dbt_service.location_standardization: Close
[0m23:52:04.749123 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175b51d0>]}
[0m23:52:04.749440 [info ] [Thread-2 (]: 2 of 9 OK created sql table model public_staging_analysis.location_standardization  [[32mSELECT 106[0m in 0.08s]
[0m23:52:04.749688 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_standardization
[0m23:52:04.749859 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:52:04.750130 [info ] [Thread-2 (]: 5 of 9 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:52:04.750351 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.location_standardization, now model.dbt_service.staging_company_profiling)
[0m23:52:04.750514 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:52:04.752318 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:52:04.752915 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:52:04.754419 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:52:04.754730 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.754869 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:52:04.755001 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:52:04.761024 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m23:52:04.761331 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.761620 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:52:04.787144 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.066 seconds
[0m23:52:04.788758 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.788925 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:52:04.789441 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.791544 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.791718 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:52:04.792282 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.792966 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:52:04.793126 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.793266 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:52:04.794150 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:04.795189 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:52:04.795486 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:52:04.795634 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:52:04.797228 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:04.797798 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:52:04.798062 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11761ded0>]}
[0m23:52:04.798337 [info ] [Thread-1 (]: 1 of 9 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.13s]
[0m23:52:04.798566 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:52:04.798729 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:52:04.798948 [info ] [Thread-1 (]: 6 of 9 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:52:04.799174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_data_profiling)
[0m23:52:04.799328 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:52:04.800970 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:52:04.801342 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:52:04.802837 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:52:04.803131 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.803268 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:52:04.803396 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:52:04.809022 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:52:04.809256 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.809532 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:52:04.823516 [debug] [Thread-3 (]: SQL status: SELECT 180 in 0.102 seconds
[0m23:52:04.825076 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.825248 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.104 seconds
[0m23:52:04.825417 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:52:04.826703 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.826912 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:52:04.827277 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.827549 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.828878 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.829983 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.830145 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:52:04.830298 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:52:04.830781 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.830922 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.831525 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:52:04.832083 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:52:04.832236 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.832376 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.832511 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:52:04.832653 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:52:04.833581 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:04.833787 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:04.834773 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:52:04.835681 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:52:04.836022 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:52:04.836316 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:52:04.836466 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:52:04.836615 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:52:04.837679 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:04.838186 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:52:04.838342 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:52:04.838905 [debug] [Thread-3 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:52:04.839061 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117604a50>]}
[0m23:52:04.839320 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175e4e10>]}
[0m23:52:04.839534 [info ] [Thread-4 (]: 4 of 9 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.17s]
[0m23:52:04.839835 [info ] [Thread-3 (]: 3 of 9 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.17s]
[0m23:52:04.840070 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:52:04.840283 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:52:04.840457 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_location_profiling
[0m23:52:04.840658 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:52:04.840850 [info ] [Thread-4 (]: 7 of 9 START sql table model public_raw_analysis.raw_location_profiling ........ [RUN]
[0m23:52:04.841084 [info ] [Thread-3 (]: 8 of 9 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:52:04.841307 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.raw_location_profiling)
[0m23:52:04.841481 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_location_profiling)
[0m23:52:04.841638 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m23:52:04.841787 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:52:04.844648 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m23:52:04.846491 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:52:04.847075 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_location_profiling
[0m23:52:04.847221 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:52:04.848770 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m23:52:04.850257 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:52:04.850680 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:04.850843 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m23:52:04.851003 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:04.851155 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:52:04.851360 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:52:04.851652 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:52:04.857746 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m23:52:04.857933 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:52:04.858117 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:04.858274 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:04.858561 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(ls.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public_staging_analysis"."location_standardization" ls
        on upper(flp.state_region_raw) = ls.state_name_upper
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(ls.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public_staging_analysis"."location_standardization" ls
        on upper(plp.state_region_raw) = ls.state_name_upper
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m23:52:04.859031 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data with ISO code standardization
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(ls.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public_staging_analysis"."location_standardization" ls
        on upper(flp.state_region_raw) = ls.state_name_upper
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(ls.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public_staging_analysis"."location_standardization" ls
        on upper(plp.state_region_raw) = ls.state_name_upper
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m23:52:04.945044 [debug] [Thread-2 (]: SQL status: SELECT 271 in 0.183 seconds
[0m23:52:04.946650 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.946816 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m23:52:04.947444 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.948683 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.948840 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:52:04.949296 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.949986 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:52:04.950139 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.950274 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:52:04.951790 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:04.952858 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:52:04.953211 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:52:04.953369 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:52:04.954853 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:04.955609 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:52:04.955908 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116edd390>]}
[0m23:52:04.956187 [info ] [Thread-2 (]: 5 of 9 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.21s]
[0m23:52:04.956417 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:52:04.956573 [debug] [Thread-2 (]: Began running node model.dbt_service.data_overview
[0m23:52:04.956844 [info ] [Thread-2 (]: 9 of 9 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:52:04.957122 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_company_profiling, now model.dbt_service.data_overview)
[0m23:52:04.957275 [debug] [Thread-2 (]: Began compiling node model.dbt_service.data_overview
[0m23:52:04.959110 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:52:04.959663 [debug] [Thread-2 (]: Began executing node model.dbt_service.data_overview
[0m23:52:04.967386 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:52:04.967826 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:52:04.967975 [debug] [Thread-2 (]: On model.dbt_service.data_overview: BEGIN
[0m23:52:04.968106 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:52:04.974272 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m23:52:04.974500 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:52:04.974687 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:52:04.975917 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:52:04.977181 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:52:04.977337 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:52:04.977831 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:04.978404 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m23:52:04.978544 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:52:04.978676 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m23:52:04.978918 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.169 seconds
[0m23:52:04.981147 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.981326 [debug] [Thread-2 (]: SQL status: COMMIT in 0.003 seconds
[0m23:52:04.981510 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:52:04.982448 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:52:04.983824 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:52:04.983996 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:52:04.984169 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:52:04.985554 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.985753 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:52:04.986011 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:52:04.986485 [debug] [Thread-2 (]: On model.dbt_service.data_overview: Close
[0m23:52:04.986627 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:52:04.986949 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1174a6a90>]}
[0m23:52:04.987599 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:52:04.988077 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.987918 [info ] [Thread-2 (]: 9 of 9 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:52:04.988294 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:52:04.988521 [debug] [Thread-2 (]: Finished running node model.dbt_service.data_overview
[0m23:52:04.989106 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m23:52:04.990249 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:52:04.990567 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:52:04.990717 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:52:04.991832 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:04.992489 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:52:04.992753 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1174ad710>]}
[0m23:52:04.993016 [info ] [Thread-1 (]: 6 of 9 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.19s]
[0m23:52:04.993281 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:52:05.023582 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.164 seconds
[0m23:52:05.025299 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:05.025467 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m23:52:05.026046 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:05.027158 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:05.027307 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m23:52:05.027734 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:05.028387 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:52:05.028542 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:05.028678 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:52:05.029532 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:05.030510 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m23:52:05.030800 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:52:05.030941 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m23:52:05.032218 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:05.032689 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: Close
[0m23:52:05.032954 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1176bff50>]}
[0m23:52:05.033219 [info ] [Thread-4 (]: 7 of 9 OK created sql table model public_raw_analysis.raw_location_profiling ... [[32mSELECT 152[0m in 0.19s]
[0m23:52:05.033451 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_location_profiling
[0m23:52:05.037434 [debug] [Thread-3 (]: SQL status: SELECT 152 in 0.178 seconds
[0m23:52:05.039047 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:05.039203 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m23:52:05.039736 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:05.040842 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:05.041007 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m23:52:05.041456 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:52:05.041988 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:52:05.042157 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:05.042330 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:52:05.043057 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:52:05.044116 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m23:52:05.044447 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:52:05.044594 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m23:52:05.045951 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:52:05.046465 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:52:05.046757 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10f282b6-57cd-4102-bc43-371373704e46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1176b4f90>]}
[0m23:52:05.047015 [info ] [Thread-3 (]: 8 of 9 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.21s]
[0m23:52:05.047235 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:52:05.047888 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:05.048018 [debug] [MainThread]: On master: BEGIN
[0m23:52:05.048132 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:52:05.053909 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:52:05.054107 [debug] [MainThread]: On master: COMMIT
[0m23:52:05.054238 [debug] [MainThread]: Using postgres connection "master"
[0m23:52:05.054349 [debug] [MainThread]: On master: COMMIT
[0m23:52:05.054657 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:52:05.054842 [debug] [MainThread]: On master: Close
[0m23:52:05.055022 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:52:05.055134 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:52:05.055248 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:52:05.055349 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m23:52:05.055445 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m23:52:05.055616 [info ] [MainThread]: 
[0m23:52:05.055767 [info ] [MainThread]: Finished running 8 table models, 1 view model in 0 hours 0 minutes and 0.53 seconds (0.53s).
[0m23:52:05.056435 [debug] [MainThread]: Command end result
[0m23:52:05.072419 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:52:05.073486 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:52:05.076499 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:52:05.076655 [info ] [MainThread]: 
[0m23:52:05.076831 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:52:05.076962 [info ] [MainThread]: 
[0m23:52:05.077106 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m23:52:05.078670 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.88421583, "process_in_blocks": "0", "process_kernel_time": 0.185047, "process_mem_max_rss": "138903552", "process_out_blocks": "0", "process_user_time": 1.145457}
[0m23:52:05.078920 [debug] [MainThread]: Command `dbt run` succeeded at 23:52:05.078883 after 0.88 seconds
[0m23:52:05.079097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10475c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11615fa50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11615ffd0>]}
[0m23:52:05.079264 [debug] [MainThread]: Flushing usage events
[0m23:52:05.358431 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:53:26.304184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103ef050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110458ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110467d50>]}


============================== 23:53:26.306435 | fcd4592e-b672-4000-9dbe-51725a722019 ==============================
[0m23:53:26.306435 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:53:26.306724 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'use_experimental_parser': 'False', 'use_colors': 'True', 'debug': 'False', 'target_path': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'partial_parse': 'True', 'printer_width': '80', 'version_check': 'True', 'no_print': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'quiet': 'False', 'invocation_command': 'dbt test', 'introspect': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'static_parser': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_cache_events': 'False', 'empty': 'None'}
[0m23:53:26.394243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104644d0>]}
[0m23:53:26.424719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042f5310>]}
[0m23:53:26.425236 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:53:26.468572 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:53:26.524276 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:53:26.524500 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:53:26.547890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111470210>]}
[0m23:53:26.588604 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:53:26.589529 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:53:26.600918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11133c950>]}
[0m23:53:26.601204 [info ] [MainThread]: Found 14 models, 4 seeds, 13 data tests, 2 sources, 449 macros
[0m23:53:26.601359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1112a9d90>]}
[0m23:53:26.602459 [info ] [MainThread]: 
[0m23:53:26.602624 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:53:26.602744 [info ] [MainThread]: 
[0m23:53:26.602990 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:53:26.605101 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:53:26.605350 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:53:26.610527 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:53:26.611672 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:53:26.657854 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:53:26.658103 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:53:26.658247 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:53:26.658387 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:53:26.658575 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:53:26.658702 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:53:26.658819 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:53:26.658932 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:53:26.659049 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:26.659171 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:26.659288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:26.659408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:26.682855 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m23:53:26.683059 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m23:53:26.683250 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:53:26.683426 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:53:26.683607 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:53:26.683771 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:53:26.683955 [debug] [ThreadPool]: SQL status: BEGIN in 0.025 seconds
[0m23:53:26.684062 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:53:26.684176 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:53:26.684353 [debug] [ThreadPool]: SQL status: BEGIN in 0.025 seconds
[0m23:53:26.684451 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:53:26.684566 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:53:26.687587 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m23:53:26.687724 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:53:26.687857 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m23:53:26.688452 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:53:26.688894 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:53:26.689294 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:53:26.689766 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m23:53:26.689890 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:53:26.690270 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:53:26.690365 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:53:26.690475 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:53:26.690702 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public)
[0m23:53:26.691091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_raw)
[0m23:53:26.692700 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:53:26.692797 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:53:26.693508 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:53:26.693623 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:53:26.693803 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:53:26.694082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:53:26.694201 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:53:26.701684 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:53:26.701878 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:53:26.702028 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:53:26.702152 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:53:26.702300 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:53:26.702434 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:53:26.704501 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:53:26.704637 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.002 seconds
[0m23:53:26.705164 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:53:26.705596 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:53:26.705969 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:53:26.706122 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:53:26.709508 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.709698 [debug] [MainThread]: On master: BEGIN
[0m23:53:26.709808 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:53:26.715521 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:53:26.715719 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.715890 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:53:26.718191 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:53:26.719324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcd4592e-b672-4000-9dbe-51725a722019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111324190>]}
[0m23:53:26.719528 [debug] [MainThread]: On master: ROLLBACK
[0m23:53:26.719868 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.719992 [debug] [MainThread]: On master: BEGIN
[0m23:53:26.720442 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:53:26.720568 [debug] [MainThread]: On master: COMMIT
[0m23:53:26.720672 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.720778 [debug] [MainThread]: On master: COMMIT
[0m23:53:26.721087 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:53:26.721221 [debug] [MainThread]: On master: Close
[0m23:53:26.722652 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:53:26.722878 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:53:26.723047 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:53:26.723177 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:53:26.723331 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m23:53:26.723531 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m23:53:26.723691 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m23:53:26.723843 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m23:53:26.724024 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m23:53:26.724174 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m23:53:26.724308 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m23:53:26.724453 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m23:53:26.724618 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:53:26.724756 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:53:26.724888 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:53:26.725013 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:53:26.731733 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:53:26.733230 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:53:26.734635 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:53:26.736108 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:53:26.737409 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:53:26.737572 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:53:26.737705 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:53:26.737864 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:53:26.745892 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:53:26.747128 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:53:26.748070 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:53:26.748990 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:53:26.749623 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:53:26.749810 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:53:26.750052 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m23:53:26.750254 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:53:26.750400 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:53:26.750552 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m23:53:26.750710 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:53:26.750850 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m23:53:26.750980 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m23:53:26.751111 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:53:26.751363 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:53:26.751511 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:53:26.764948 [debug] [Thread-3 (]: SQL status: BEGIN in 0.014 seconds
[0m23:53:26.765123 [debug] [Thread-2 (]: SQL status: BEGIN in 0.014 seconds
[0m23:53:26.765318 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m23:53:26.765450 [debug] [Thread-4 (]: SQL status: BEGIN in 0.014 seconds
[0m23:53:26.765580 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:53:26.765715 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:53:26.765844 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:53:26.765972 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:53:26.766112 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.766256 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.766399 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.766546 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.768509 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:53:26.768643 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:53:26.770375 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m23:53:26.770914 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m23:53:26.771573 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m23:53:26.771711 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m23:53:26.771956 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m23:53:26.772403 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:53:26.772158 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m23:53:26.772630 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:53:26.772808 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.006 seconds
[0m23:53:26.772981 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:53:26.773112 [info ] [Thread-1 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m23:53:26.773721 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m23:53:26.773918 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:53:26.774038 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.007 seconds
[0m23:53:26.774222 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m23:53:26.774391 [info ] [Thread-2 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m23:53:26.774901 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m23:53:26.775027 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m23:53:26.775156 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:53:26.775292 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m23:53:26.775543 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:53:26.777506 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:53:26.777680 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m23:53:26.777851 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:53:26.778064 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:53:26.780105 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:53:26.778392 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:53:26.780463 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:53:26.780700 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:53:26.781049 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:53:26.781309 [info ] [Thread-3 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m23:53:26.781561 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:53:26.782908 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:53:26.783138 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:53:26.783374 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m23:53:26.784498 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:53:26.784770 [info ] [Thread-4 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m23:53:26.785012 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:53:26.785300 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:53:26.785539 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m23:53:26.787587 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:53:26.787766 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m23:53:26.787930 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:53:26.788081 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:53:26.788256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:53:26.788412 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m23:53:26.788554 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:53:26.791004 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:53:26.791254 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:53:26.793412 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:53:26.794059 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:53:26.795104 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:53:26.795456 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:53:26.795716 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m23:53:26.795894 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:53:26.796039 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:53:26.796185 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m23:53:26.796390 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:53:26.799395 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m23:53:26.799583 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:53:26.799727 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.800532 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m23:53:26.800729 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:53:26.800886 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.802790 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:53:26.803492 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m23:53:26.803718 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:53:26.804323 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m23:53:26.804492 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m23:53:26.804636 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m23:53:26.804788 [debug] [Thread-4 (]: SQL status: BEGIN in 0.008 seconds
[0m23:53:26.805217 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:53:26.805364 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m23:53:26.805092 [info ] [Thread-1 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.805556 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:53:26.805722 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:53:26.806022 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:53:26.806255 [info ] [Thread-2 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.806447 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.806669 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:53:26.806921 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:53:26.807111 [info ] [Thread-1 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m23:53:26.807290 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:53:26.807496 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m23:53:26.807679 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:53:26.807860 [info ] [Thread-2 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m23:53:26.808080 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:53:26.808752 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m23:53:26.808927 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m23:53:26.810956 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:53:26.811120 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.004 seconds
[0m23:53:26.811296 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:53:26.811897 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m23:53:26.812062 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m23:53:26.813714 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:53:26.814088 [info ] [Thread-4 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.814300 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:53:26.814558 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:53:26.814696 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m23:53:26.815872 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:53:26.816032 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:53:26.816230 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:53:26.817512 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:53:26.817749 [info ] [Thread-3 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.817935 [info ] [Thread-4 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m23:53:26.818224 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:53:26.818421 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:53:26.818622 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m23:53:26.818781 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:53:26.818947 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:53:26.819130 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m23:53:26.819292 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:53:26.819455 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m23:53:26.819597 [info ] [Thread-3 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m23:53:26.819747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:53:26.821322 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:53:26.821513 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:53:26.821693 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m23:53:26.822135 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:53:26.822344 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:53:26.824207 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:53:26.825324 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:53:26.825684 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:53:26.825830 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m23:53:26.825979 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:53:26.826151 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:53:26.827198 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:53:26.827614 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:53:26.827751 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m23:53:26.827938 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:53:26.828353 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m23:53:26.828496 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m23:53:26.828651 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:53:26.828805 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:53:26.828966 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.829147 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.830095 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:53:26.830817 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m23:53:26.831219 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m23:53:26.831562 [info ] [Thread-1 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.02s]
[0m23:53:26.831795 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:53:26.831953 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:53:26.832120 [info ] [Thread-1 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m23:53:26.832359 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m23:53:26.832535 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:53:26.834315 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:53:26.834915 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:53:26.835084 [debug] [Thread-4 (]: SQL status: BEGIN in 0.009 seconds
[0m23:53:26.835233 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m23:53:26.837469 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:53:26.837685 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:53:26.837867 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:53:26.838083 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.838263 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.838494 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:53:26.838657 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m23:53:26.838806 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:53:26.839378 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:53:26.840016 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m23:53:26.840415 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m23:53:26.840668 [info ] [Thread-2 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.840890 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:53:26.847742 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m23:53:26.847971 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:53:26.848120 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:53:26.848815 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:53:26.849437 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m23:53:26.849839 [debug] [Thread-3 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m23:53:26.850220 [info ] [Thread-3 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.850453 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.012 seconds
[0m23:53:26.850740 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:53:26.851336 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m23:53:26.851760 [debug] [Thread-4 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m23:53:26.852013 [info ] [Thread-4 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.852230 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:53:26.857988 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:53:26.858674 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m23:53:26.859189 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m23:53:26.859433 [info ] [Thread-1 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.03s]
[0m23:53:26.859643 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:53:26.860244 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.860379 [debug] [MainThread]: On master: BEGIN
[0m23:53:26.860497 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:53:26.866610 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:53:26.866856 [debug] [MainThread]: On master: COMMIT
[0m23:53:26.866966 [debug] [MainThread]: Using postgres connection "master"
[0m23:53:26.867076 [debug] [MainThread]: On master: COMMIT
[0m23:53:26.867438 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:53:26.867575 [debug] [MainThread]: On master: Close
[0m23:53:26.867781 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:53:26.867908 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m23:53:26.868024 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m23:53:26.868146 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m23:53:26.868243 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m23:53:26.868397 [info ] [MainThread]: 
[0m23:53:26.868540 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:53:26.869392 [debug] [MainThread]: Command end result
[0m23:53:26.881802 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:53:26.882857 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:53:26.886238 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:53:26.886410 [info ] [MainThread]: 
[0m23:53:26.886575 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:53:26.886702 [info ] [MainThread]: 
[0m23:53:26.886837 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m23:53:26.888193 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6204471, "process_in_blocks": "0", "process_kernel_time": 0.187354, "process_mem_max_rss": "136986624", "process_out_blocks": "0", "process_user_time": 1.076932}
[0m23:53:26.888396 [debug] [MainThread]: Command `dbt test` succeeded at 23:53:26.888361 after 0.62 seconds
[0m23:53:26.888544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f2bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100ffe7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f6f590>]}
[0m23:53:26.888718 [debug] [MainThread]: Flushing usage events
[0m23:53:27.184661 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:56:17.388873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c6b2e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c724ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c73fbd0>]}


============================== 23:56:17.391270 | 81028af0-8d9e-4c59-9483-91e43afc3115 ==============================
[0m23:56:17.391270 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:56:17.391547 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'no_print': 'None', 'log_format': 'default', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'target_path': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'introspect': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt seed', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'cache_selected_only': 'False', 'write_json': 'True', 'printer_width': '80'}
[0m23:56:17.480056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0d0690>]}
[0m23:56:17.509677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070f9390>]}
[0m23:56:17.510179 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:56:17.552895 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:56:17.611090 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 2 files changed.
[0m23:56:17.611509 [debug] [MainThread]: Partial parsing: added file: dbt_service://seeds/state_iso_mapping.csv
[0m23:56:17.611708 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/staging_analysis/location_standardization.sql
[0m23:56:17.611845 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m23:56:17.611971 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/raw_analysis/raw_location_profiling.sql
[0m23:56:17.834347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11807c390>]}
[0m23:56:17.874009 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:56:17.874896 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:56:17.882021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c54bb90>]}
[0m23:56:17.882272 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m23:56:17.882437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cbffa50>]}
[0m23:56:17.883367 [info ] [MainThread]: 
[0m23:56:17.883518 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:56:17.883634 [info ] [MainThread]: 
[0m23:56:17.883830 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:56:17.885458 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:56:17.908388 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:56:17.908688 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:56:17.908843 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:56:17.930321 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m23:56:17.931054 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:56:17.932036 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m23:56:17.932256 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:56:17.932487 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m23:56:17.934942 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:56:17.935185 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:56:17.936052 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:56:17.937185 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:56:17.937319 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:56:17.938025 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:56:17.938147 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:56:17.938258 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:56:17.938375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:56:17.938484 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:56:17.938588 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:56:17.938692 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:56:17.938909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:56:17.951839 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:56:17.952015 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:56:17.952130 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:56:17.952228 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:56:17.952324 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m23:56:17.952436 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:56:17.952546 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:56:17.952684 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:56:17.952813 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:56:17.952953 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:56:17.953099 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:56:17.953282 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:56:17.957196 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m23:56:17.957819 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:56:17.957960 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.005 seconds
[0m23:56:17.958467 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:56:17.958636 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m23:56:17.959040 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:56:17.959155 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:56:17.959374 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m23:56:17.959492 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:56:17.960633 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:56:17.960799 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:56:17.960934 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.007 seconds
[0m23:56:17.961195 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw)
[0m23:56:17.961336 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:56:17.962224 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:56:17.963226 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:56:17.963371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:56:17.963527 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:56:17.963762 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:56:17.963903 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:56:17.970035 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m23:56:17.970233 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m23:56:17.970431 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:56:17.970606 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:56:17.970766 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:56:17.970912 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:56:17.972475 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m23:56:17.972660 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m23:56:17.973241 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:56:17.973675 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:56:17.974088 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:56:17.974237 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:56:17.976945 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:17.977094 [debug] [MainThread]: On master: BEGIN
[0m23:56:17.977199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:56:17.982856 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:56:17.983082 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:17.983269 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:56:17.985327 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:56:17.986450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e5f0050>]}
[0m23:56:17.986656 [debug] [MainThread]: On master: ROLLBACK
[0m23:56:17.987437 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:17.987548 [debug] [MainThread]: On master: BEGIN
[0m23:56:17.988581 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m23:56:17.988708 [debug] [MainThread]: On master: COMMIT
[0m23:56:17.988817 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:17.988916 [debug] [MainThread]: On master: COMMIT
[0m23:56:17.989710 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m23:56:17.989819 [debug] [MainThread]: On master: Close
[0m23:56:17.990900 [debug] [Thread-1 (]: Began running node seed.dbt_service.fxf_data
[0m23:56:17.991080 [debug] [Thread-2 (]: Began running node seed.dbt_service.fxf_sample
[0m23:56:17.991401 [debug] [Thread-3 (]: Began running node seed.dbt_service.pdl_data
[0m23:56:17.991269 [info ] [Thread-1 (]: 1 of 5 START seed file public.fxf_data ......................................... [RUN]
[0m23:56:17.991622 [debug] [Thread-4 (]: Began running node seed.dbt_service.pdl_sample
[0m23:56:17.991781 [info ] [Thread-2 (]: 2 of 5 START seed file public.fxf_sample ....................................... [RUN]
[0m23:56:17.991978 [info ] [Thread-3 (]: 3 of 5 START seed file public.pdl_data ......................................... [RUN]
[0m23:56:17.992192 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now seed.dbt_service.fxf_data)
[0m23:56:17.992433 [info ] [Thread-4 (]: 4 of 5 START seed file public.pdl_sample ....................................... [RUN]
[0m23:56:17.992632 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now seed.dbt_service.fxf_sample)
[0m23:56:17.992792 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now seed.dbt_service.pdl_data)
[0m23:56:17.992951 [debug] [Thread-1 (]: Began compiling node seed.dbt_service.fxf_data
[0m23:56:17.993191 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now seed.dbt_service.pdl_sample)
[0m23:56:17.993340 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.fxf_sample
[0m23:56:17.993478 [debug] [Thread-3 (]: Began compiling node seed.dbt_service.pdl_data
[0m23:56:17.993621 [debug] [Thread-1 (]: Began executing node seed.dbt_service.fxf_data
[0m23:56:17.993764 [debug] [Thread-4 (]: Began compiling node seed.dbt_service.pdl_sample
[0m23:56:17.993893 [debug] [Thread-2 (]: Began executing node seed.dbt_service.fxf_sample
[0m23:56:17.994019 [debug] [Thread-3 (]: Began executing node seed.dbt_service.pdl_data
[0m23:56:18.000448 [debug] [Thread-4 (]: Began executing node seed.dbt_service.pdl_sample
[0m23:56:18.015432 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m23:56:18.079278 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: BEGIN
[0m23:56:18.138008 [debug] [Thread-4 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m23:56:18.138225 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:56:18.205282 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: BEGIN
[0m23:56:18.354533 [debug] [Thread-2 (]: SQL status: BEGIN in 0.216 seconds
[0m23:56:18.379291 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:56:18.383760 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:18.390425 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m23:56:18.440697 [debug] [Thread-4 (]: SQL status: BEGIN in 0.061 seconds
[0m23:56:18.519542 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: BEGIN
[0m23:56:18.522383 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:18.522610 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.fxf_sample"} */
truncate table "finny_db"."public"."fxf_sample"
[0m23:56:18.522761 [debug] [Thread-4 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m23:56:18.522909 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:56:18.523054 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: BEGIN
[0m23:56:18.523248 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.pdl_sample"} */
truncate table "finny_db"."public"."pdl_sample"
[0m23:56:18.523497 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:56:18.529109 [debug] [Thread-2 (]: SQL status: TRUNCATE TABLE in 0.006 seconds
[0m23:56:18.537286 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m23:56:18.537521 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m23:56:18.537668 [debug] [Thread-4 (]: SQL status: TRUNCATE TABLE in 0.014 seconds
[0m23:56:18.537805 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: 
          insert into "finny_db"."public"."fxf_sample" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m23:56:18.537958 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:18.541299 [debug] [Thread-4 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m23:56:18.541458 [debug] [Thread-3 (]: SQL status: BEGIN in 0.018 seconds
[0m23:56:18.541703 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.fxf_data"} */
truncate table "finny_db"."public"."fxf_data"
[0m23:56:18.541848 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: 
          insert into "finny_db"."public"."pdl_sample" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m23:56:18.541979 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:18.542196 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.pdl_data"} */
truncate table "finny_db"."public"."pdl_data"
[0m23:56:18.542410 [debug] [Thread-2 (]: SQL status: INSERT 0 5 in 0.001 seconds
[0m23:56:18.544505 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.dbt_service.fxf_sample"
[0m23:56:18.544667 [debug] [Thread-4 (]: SQL status: INSERT 0 5 in 0.003 seconds
[0m23:56:18.544999 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.dbt_service.pdl_sample"
[0m23:56:18.550601 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: COMMIT
[0m23:56:18.550774 [debug] [Thread-3 (]: SQL status: TRUNCATE TABLE in 0.008 seconds
[0m23:56:18.550897 [debug] [Thread-1 (]: SQL status: TRUNCATE TABLE in 0.009 seconds
[0m23:56:18.551405 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: COMMIT
[0m23:56:18.551579 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.fxf_sample"
[0m23:56:18.600843 [debug] [Thread-4 (]: Using postgres connection "seed.dbt_service.pdl_sample"
[0m23:56:18.657460 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: COMMIT
[0m23:56:18.700784 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: COMMIT
[0m23:56:18.777196 [debug] [Thread-2 (]: SQL status: COMMIT in 0.033 seconds
[0m23:56:18.804941 [debug] [Thread-4 (]: SQL status: COMMIT in 0.014 seconds
[0m23:56:18.830313 [debug] [Thread-2 (]: On seed.dbt_service.fxf_sample: Close
[0m23:56:18.866292 [debug] [Thread-4 (]: On seed.dbt_service.pdl_sample: Close
[0m23:56:18.996084 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e5f9a10>]}
[0m23:56:19.008547 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d578990>]}
[0m23:56:19.063645 [info ] [Thread-2 (]: 2 of 5 OK loaded seed file public.fxf_sample ................................... [[32mINSERT 5[0m in 0.93s]
[0m23:56:19.106571 [info ] [Thread-4 (]: 4 of 5 OK loaded seed file public.pdl_sample ................................... [[32mINSERT 5[0m in 0.98s]
[0m23:56:19.155964 [debug] [Thread-2 (]: Finished running node seed.dbt_service.fxf_sample
[0m23:56:19.179434 [debug] [Thread-4 (]: Finished running node seed.dbt_service.pdl_sample
[0m23:56:19.223296 [debug] [Thread-2 (]: Began running node seed.dbt_service.state_iso_mapping
[0m23:56:19.315589 [info ] [Thread-2 (]: 5 of 5 START seed file public.state_iso_mapping ................................ [RUN]
[0m23:56:19.363839 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.dbt_service.fxf_sample, now seed.dbt_service.state_iso_mapping)
[0m23:56:19.418554 [debug] [Thread-2 (]: Began compiling node seed.dbt_service.state_iso_mapping
[0m23:56:19.447164 [debug] [Thread-2 (]: Began executing node seed.dbt_service.state_iso_mapping
[0m23:56:19.561502 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.state_iso_mapping"
[0m23:56:19.611246 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: BEGIN
[0m23:56:19.661627 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:56:19.735732 [debug] [Thread-2 (]: SQL status: BEGIN in 0.074 seconds
[0m23:56:19.773657 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.state_iso_mapping"
[0m23:56:19.817713 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "seed.dbt_service.state_iso_mapping"} */

    create table "finny_db"."public"."state_iso_mapping" ("state_name" text,"iso_code" text)
  
[0m23:56:19.865571 [debug] [Thread-2 (]: SQL status: CREATE TABLE in 0.017 seconds
[0m23:56:19.934445 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.state_iso_mapping"
[0m23:56:19.984217 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: 
          insert into "finny_db"."public"."state_iso_mapping" ("state_name", "iso_code") values
          (%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%s),(%s,%...
[0m23:56:20.035448 [debug] [Thread-2 (]: SQL status: INSERT 0 104 in 0.013 seconds
[0m23:56:20.066057 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.dbt_service.state_iso_mapping"
[0m23:56:20.159869 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: COMMIT
[0m23:56:20.197815 [debug] [Thread-2 (]: Using postgres connection "seed.dbt_service.state_iso_mapping"
[0m23:56:20.248232 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: COMMIT
[0m23:56:20.301143 [debug] [Thread-2 (]: SQL status: COMMIT in 0.015 seconds
[0m23:56:20.338843 [debug] [Thread-2 (]: On seed.dbt_service.state_iso_mapping: Close
[0m23:56:20.388053 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13d717e50>]}
[0m23:56:20.425343 [info ] [Thread-2 (]: 5 of 5 OK loaded seed file public.state_iso_mapping ............................ [[32mINSERT 104[0m in 1.02s]
[0m23:56:20.481151 [debug] [Thread-2 (]: Finished running node seed.dbt_service.state_iso_mapping
[0m23:56:26.412519 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:26.431294 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:26.512432 [debug] [Thread-3 (]: SQL status: INSERT 0 10000 in 0.062 seconds
[0m23:56:26.523862 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:26.549264 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:26.623656 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.055 seconds
[0m23:56:34.446109 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:34.465198 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:34.537274 [debug] [Thread-3 (]: SQL status: INSERT 0 10000 in 0.053 seconds
[0m23:56:34.578281 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:34.597052 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:34.673735 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.058 seconds
[0m23:56:42.455173 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:42.474267 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:42.551317 [debug] [Thread-3 (]: SQL status: INSERT 0 10000 in 0.058 seconds
[0m23:56:42.569012 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:42.594364 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:42.672360 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.059 seconds
[0m23:56:50.362635 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:50.388051 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:50.464374 [debug] [Thread-3 (]: SQL status: INSERT 0 10000 in 0.058 seconds
[0m23:56:50.529273 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:50.554577 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:50.637980 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.058 seconds
[0m23:56:58.499049 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:58.518180 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:58.598562 [debug] [Thread-3 (]: SQL status: INSERT 0 10000 in 0.061 seconds
[0m23:56:58.613252 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:58.631866 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: 
          insert into "finny_db"."public"."pdl_data" ("pdl_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m23:56:58.657964 [debug] [Thread-3 (]: SQL status: INSERT 0 5 in 0.007 seconds
[0m23:56:58.668087 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:58.668712 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.dbt_service.pdl_data"
[0m23:56:58.668901 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m23:56:58.691194 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: COMMIT
[0m23:56:58.691424 [debug] [Thread-3 (]: Using postgres connection "seed.dbt_service.pdl_data"
[0m23:56:58.691568 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: COMMIT
[0m23:56:58.695459 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m23:56:58.695775 [debug] [Thread-3 (]: On seed.dbt_service.pdl_data: Close
[0m23:56:58.696046 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13c27ccd0>]}
[0m23:56:58.696276 [info ] [Thread-3 (]: 3 of 5 OK loaded seed file public.pdl_data ..................................... [[32mINSERT 50005[0m in 40.70s]
[0m23:56:58.696498 [debug] [Thread-3 (]: Finished running node seed.dbt_service.pdl_data
[0m23:56:58.730869 [debug] [Thread-1 (]: SQL status: INSERT 0 10000 in 0.062 seconds
[0m23:56:58.733734 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:58.733929 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: 
          insert into "finny_db"."public"."fxf_data" ("fxf_id", "name", "email", "company", "company_revenue", "title", "location") values
          (%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s)
      ...
[0m23:56:58.734547 [debug] [Thread-1 (]: SQL status: INSERT 0 5 in 0.000 seconds
[0m23:56:58.735146 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.dbt_service.fxf_data"
[0m23:56:58.736374 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: COMMIT
[0m23:56:58.736540 [debug] [Thread-1 (]: Using postgres connection "seed.dbt_service.fxf_data"
[0m23:56:58.736698 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: COMMIT
[0m23:56:58.740014 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m23:56:58.740381 [debug] [Thread-1 (]: On seed.dbt_service.fxf_data: Close
[0m23:56:58.740657 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81028af0-8d9e-4c59-9483-91e43afc3115', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6d8290>]}
[0m23:56:58.740890 [info ] [Thread-1 (]: 1 of 5 OK loaded seed file public.fxf_data ..................................... [[32mINSERT 50005[0m in 40.75s]
[0m23:56:58.741120 [debug] [Thread-1 (]: Finished running node seed.dbt_service.fxf_data
[0m23:56:58.741692 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:58.741850 [debug] [MainThread]: On master: BEGIN
[0m23:56:58.741975 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:56:58.748039 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:56:58.748274 [debug] [MainThread]: On master: COMMIT
[0m23:56:58.748404 [debug] [MainThread]: Using postgres connection "master"
[0m23:56:58.748513 [debug] [MainThread]: On master: COMMIT
[0m23:56:58.748902 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:56:58.749125 [debug] [MainThread]: On master: Close
[0m23:56:58.749347 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:56:58.749479 [debug] [MainThread]: Connection 'seed.dbt_service.fxf_data' was properly closed.
[0m23:56:58.749582 [debug] [MainThread]: Connection 'seed.dbt_service.state_iso_mapping' was properly closed.
[0m23:56:58.749689 [debug] [MainThread]: Connection 'seed.dbt_service.pdl_data' was properly closed.
[0m23:56:58.749788 [debug] [MainThread]: Connection 'seed.dbt_service.pdl_sample' was properly closed.
[0m23:56:58.749957 [info ] [MainThread]: 
[0m23:56:58.750106 [info ] [MainThread]: Finished running 5 seeds in 0 hours 0 minutes and 40.87 seconds (40.87s).
[0m23:56:58.750583 [debug] [MainThread]: Command end result
[0m23:56:58.762581 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:56:58.763528 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:56:58.766368 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:56:58.766548 [info ] [MainThread]: 
[0m23:56:58.766734 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:56:58.766872 [info ] [MainThread]: 
[0m23:56:58.767005 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m23:56:58.769096 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 41.420593, "process_in_blocks": "0", "process_kernel_time": 0.688143, "process_mem_max_rss": "321617920", "process_out_blocks": "0", "process_user_time": 41.376328}
[0m23:56:58.769337 [debug] [MainThread]: Command `dbt seed` succeeded at 23:56:58.769297 after 41.42 seconds
[0m23:56:58.769563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c6fb490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f7a910>]}
[0m23:56:58.769750 [debug] [MainThread]: Flushing usage events
[0m23:56:59.133817 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:57:05.833122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10962a5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a7f90>]}


============================== 23:57:05.835311 | 4df8853b-2d45-48e8-8091-e5fbe21d6903 ==============================
[0m23:57:05.835311 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:57:05.835620 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'write_json': 'True', 'printer_width': '80', 'debug': 'False', 'log_format': 'default', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'introspect': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'quiet': 'False', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select raw_analysis+ staging_analysis+'}
[0m23:57:05.926388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b03d50>]}
[0m23:57:05.956646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d6b3d0>]}
[0m23:57:05.957159 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:57:06.001742 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:57:06.064007 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:57:06.064271 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:57:06.090076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a54c210>]}
[0m23:57:06.133400 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:57:06.134344 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:57:06.141970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5ac050>]}
[0m23:57:06.142344 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m23:57:06.142531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5353d0>]}
[0m23:57:06.143740 [info ] [MainThread]: 
[0m23:57:06.143917 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:57:06.144043 [info ] [MainThread]: 
[0m23:57:06.144299 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:57:06.146506 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:57:06.146851 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:57:06.147108 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m23:57:06.204845 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:57:06.205086 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:57:06.205258 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m23:57:06.205432 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:57:06.205575 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:57:06.205708 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m23:57:06.205855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:06.206083 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:06.206262 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:06.236152 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.030 seconds
[0m23:57:06.236397 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m23:57:06.236591 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.030 seconds
[0m23:57:06.237267 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:57:06.237809 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:57:06.238251 [debug] [ThreadPool]: On list_finny_db: Close
[0m23:57:06.239408 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m23:57:06.239630 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m23:57:06.242703 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:57:06.242970 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m23:57:06.243339 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m23:57:06.244503 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:57:06.244785 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:57:06.245610 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:57:06.246845 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:57:06.247000 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:57:06.247132 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:06.247265 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:57:06.247376 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:57:06.247480 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:06.247713 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:06.247868 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:06.260133 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:57:06.260437 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:57:06.260643 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:57:06.260776 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:57:06.260902 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m23:57:06.261049 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:57:06.261172 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:57:06.261336 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:57:06.261491 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:57:06.261635 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:57:06.261787 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:57:06.262004 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:57:06.265208 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m23:57:06.265343 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:57:06.265515 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m23:57:06.265672 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m23:57:06.266349 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:57:06.266859 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:57:06.267287 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:57:06.267720 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:57:06.268344 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:57:06.268482 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:57:06.268624 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:57:06.268762 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:57:06.269008 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_marts)
[0m23:57:06.269714 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m23:57:06.271498 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:57:06.272261 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:57:06.272392 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:57:06.272556 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:57:06.272716 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:06.272845 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:06.283517 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:57:06.283702 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m23:57:06.283859 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:57:06.283982 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:57:06.284130 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:57:06.284293 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:57:06.287358 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m23:57:06.287499 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:57:06.288137 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:57:06.288533 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:57:06.289109 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:57:06.289304 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:57:06.292565 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.292735 [debug] [MainThread]: On master: BEGIN
[0m23:57:06.292839 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:06.300595 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m23:57:06.300865 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.301049 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:57:06.304795 [debug] [MainThread]: SQL status: SELECT 10 in 0.004 seconds
[0m23:57:06.306118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b60d6d0>]}
[0m23:57:06.306367 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:06.306922 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.307170 [debug] [MainThread]: On master: BEGIN
[0m23:57:06.307820 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m23:57:06.307953 [debug] [MainThread]: On master: COMMIT
[0m23:57:06.308071 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.308224 [debug] [MainThread]: On master: COMMIT
[0m23:57:06.308598 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:57:06.308752 [debug] [MainThread]: On master: Close
[0m23:57:06.310110 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m23:57:06.310303 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m23:57:06.310485 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_data_profiling
[0m23:57:06.310637 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_location_profiling
[0m23:57:06.310854 [info ] [Thread-1 (]: 1 of 8 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m23:57:06.311083 [info ] [Thread-2 (]: 2 of 8 START sql table model public_raw_analysis.raw_company_profiling ......... [RUN]
[0m23:57:06.311296 [info ] [Thread-3 (]: 3 of 8 START sql table model public_raw_analysis.raw_data_profiling ............ [RUN]
[0m23:57:06.311488 [info ] [Thread-4 (]: 4 of 8 START sql table model public_raw_analysis.raw_location_profiling ........ [RUN]
[0m23:57:06.311686 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.company_analysis)
[0m23:57:06.311837 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.raw_company_profiling)
[0m23:57:06.311984 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_data_profiling)
[0m23:57:06.312129 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_location_profiling)
[0m23:57:06.312273 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m23:57:06.312592 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m23:57:06.312750 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m23:57:06.312893 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m23:57:06.316843 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m23:57:06.318459 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m23:57:06.319956 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m23:57:06.321608 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m23:57:06.323157 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m23:57:06.323435 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m23:57:06.336317 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_data_profiling
[0m23:57:06.336538 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_location_profiling
[0m23:57:06.340669 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m23:57:06.342951 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m23:57:06.344291 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m23:57:06.345653 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m23:57:06.346346 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.346741 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.346976 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m23:57:06.347159 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.347433 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m23:57:06.347684 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.347952 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:57:06.348173 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m23:57:06.348380 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:06.348543 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m23:57:06.348801 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:57:06.349036 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:57:06.363848 [debug] [Thread-4 (]: SQL status: BEGIN in 0.015 seconds
[0m23:57:06.364072 [debug] [Thread-3 (]: SQL status: BEGIN in 0.015 seconds
[0m23:57:06.364222 [debug] [Thread-2 (]: SQL status: BEGIN in 0.016 seconds
[0m23:57:06.364380 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m23:57:06.364568 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.364720 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.364872 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.365011 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.365300 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m23:57:06.365659 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m23:57:06.366039 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m23:57:06.366315 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m23:57:06.447913 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.081 seconds
[0m23:57:06.452702 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.452947 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m23:57:06.453652 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.455304 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.455486 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m23:57:06.456143 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.463207 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:57:06.463441 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.463589 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m23:57:06.464902 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.467738 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m23:57:06.470185 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m23:57:06.470382 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m23:57:06.471929 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.473236 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m23:57:06.474304 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10691fc10>]}
[0m23:57:06.474704 [info ] [Thread-1 (]: 1 of 8 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.16s]
[0m23:57:06.474956 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m23:57:06.475133 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m23:57:06.475398 [info ] [Thread-1 (]: 5 of 8 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m23:57:06.475624 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_company_profiling)
[0m23:57:06.475794 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m23:57:06.477515 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m23:57:06.477890 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m23:57:06.479476 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m23:57:06.479857 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.480006 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m23:57:06.480144 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:06.481310 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.115 seconds
[0m23:57:06.482874 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.483067 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m23:57:06.483524 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.485518 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.485703 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.119 seconds
[0m23:57:06.485915 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m23:57:06.487457 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.487621 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m23:57:06.487813 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m23:57:06.487986 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.488139 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.488517 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m23:57:06.488838 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.489556 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:57:06.490891 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.491051 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.491242 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m23:57:06.491432 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m23:57:06.491903 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.492528 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:57:06.492706 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.492893 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.494099 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m23:57:06.494328 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m23:57:06.494691 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m23:57:06.494889 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m23:57:06.495502 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.496511 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m23:57:06.496656 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m23:57:06.496948 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m23:57:06.497524 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m23:57:06.497696 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m23:57:06.497990 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b618590>]}
[0m23:57:06.498279 [info ] [Thread-2 (]: 2 of 8 OK created sql table model public_raw_analysis.raw_company_profiling .... [[32mSELECT 180[0m in 0.19s]
[0m23:57:06.498521 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m23:57:06.498690 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_data_profiling
[0m23:57:06.498864 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.499060 [info ] [Thread-2 (]: 6 of 8 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m23:57:06.499591 [debug] [Thread-3 (]: On model.dbt_service.raw_data_profiling: Close
[0m23:57:06.499762 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.staging_data_profiling)
[0m23:57:06.500108 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m23:57:06.500307 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6d9410>]}
[0m23:57:06.502205 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m23:57:06.502513 [info ] [Thread-3 (]: 3 of 8 OK created sql table model public_raw_analysis.raw_data_profiling ....... [[32mSELECT 2[0m in 0.19s]
[0m23:57:06.502826 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_data_profiling
[0m23:57:06.503059 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_location_profiling
[0m23:57:06.503308 [info ] [Thread-3 (]: 7 of 8 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m23:57:06.503515 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_data_profiling
[0m23:57:06.503707 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_location_profiling)
[0m23:57:06.505258 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m23:57:06.505454 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m23:57:06.507481 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m23:57:06.507913 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.508102 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_location_profiling
[0m23:57:06.508307 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m23:57:06.509946 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m23:57:06.510232 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:57:06.510664 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.510831 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m23:57:06.510973 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:57:06.517073 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m23:57:06.517295 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m23:57:06.517527 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.517779 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.518101 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m23:57:06.518572 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m23:57:06.560619 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.194 seconds
[0m23:57:06.562616 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.562860 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m23:57:06.563660 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.566213 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.566405 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m23:57:06.566967 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.567750 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:57:06.567946 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.568112 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m23:57:06.569185 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.570678 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m23:57:06.571125 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m23:57:06.571327 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m23:57:06.572730 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.573360 [debug] [Thread-4 (]: On model.dbt_service.raw_location_profiling: Close
[0m23:57:06.573645 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6b4190>]}
[0m23:57:06.573919 [info ] [Thread-4 (]: 4 of 8 OK created sql table model public_raw_analysis.raw_location_profiling ... [[32mSELECT 152[0m in 0.26s]
[0m23:57:06.574150 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_location_profiling
[0m23:57:06.574317 [debug] [Thread-4 (]: Began running node model.dbt_service.data_overview
[0m23:57:06.574628 [info ] [Thread-4 (]: 8 of 8 START sql view model public_marts.data_overview ......................... [RUN]
[0m23:57:06.574876 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_location_profiling, now model.dbt_service.data_overview)
[0m23:57:06.575033 [debug] [Thread-4 (]: Began compiling node model.dbt_service.data_overview
[0m23:57:06.576990 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m23:57:06.577608 [debug] [Thread-4 (]: Began executing node model.dbt_service.data_overview
[0m23:57:06.585530 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m23:57:06.586033 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:57:06.586200 [debug] [Thread-4 (]: On model.dbt_service.data_overview: BEGIN
[0m23:57:06.586346 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:57:06.592656 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:57:06.592903 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:57:06.593086 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m23:57:06.594572 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m23:57:06.596158 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:57:06.596362 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m23:57:06.597272 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.598015 [debug] [Thread-4 (]: On model.dbt_service.data_overview: COMMIT
[0m23:57:06.598214 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:57:06.598366 [debug] [Thread-4 (]: On model.dbt_service.data_overview: COMMIT
[0m23:57:06.599167 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.600302 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m23:57:06.601720 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.data_overview"
[0m23:57:06.601897 [debug] [Thread-4 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m23:57:06.602475 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m23:57:06.603123 [debug] [Thread-4 (]: On model.dbt_service.data_overview: Close
[0m23:57:06.603440 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1c6750>]}
[0m23:57:06.603724 [info ] [Thread-4 (]: 8 of 8 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m23:57:06.603952 [debug] [Thread-4 (]: Finished running node model.dbt_service.data_overview
[0m23:57:06.674000 [debug] [Thread-1 (]: SQL status: SELECT 271 in 0.184 seconds
[0m23:57:06.675844 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.676037 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m23:57:06.676793 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.678171 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.678359 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m23:57:06.678913 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.679708 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:57:06.679890 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.680066 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m23:57:06.681803 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m23:57:06.683018 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m23:57:06.683384 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m23:57:06.683537 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m23:57:06.685062 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.685671 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m23:57:06.685992 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b308bd0>]}
[0m23:57:06.686313 [info ] [Thread-1 (]: 5 of 8 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.21s]
[0m23:57:06.686547 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m23:57:06.695120 [debug] [Thread-2 (]: SQL status: SELECT 2 in 0.176 seconds
[0m23:57:06.696806 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.696970 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m23:57:06.697648 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.698776 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.698919 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m23:57:06.699593 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m23:57:06.700385 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:57:06.700585 [debug] [Thread-3 (]: SQL status: SELECT 152 in 0.182 seconds
[0m23:57:06.701016 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.704078 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.704356 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m23:57:06.704561 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m23:57:06.705449 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.706667 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.706929 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m23:57:06.707224 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m23:57:06.708509 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m23:57:06.709152 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m23:57:06.709367 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m23:57:06.709559 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m23:57:06.710463 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:57:06.710714 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.710895 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m23:57:06.711680 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.711844 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m23:57:06.712452 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: Close
[0m23:57:06.713840 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m23:57:06.714251 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m23:57:06.714500 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bae6a10>]}
[0m23:57:06.714668 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m23:57:06.715061 [info ] [Thread-2 (]: 6 of 8 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.21s]
[0m23:57:06.715475 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_data_profiling
[0m23:57:06.716270 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m23:57:06.716853 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: Close
[0m23:57:06.717127 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df8853b-2d45-48e8-8091-e5fbe21d6903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5e5ad0>]}
[0m23:57:06.717389 [info ] [Thread-3 (]: 7 of 8 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.21s]
[0m23:57:06.717613 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_location_profiling
[0m23:57:06.718285 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.718419 [debug] [MainThread]: On master: BEGIN
[0m23:57:06.718535 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:57:06.724893 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:57:06.725113 [debug] [MainThread]: On master: COMMIT
[0m23:57:06.725245 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:06.725356 [debug] [MainThread]: On master: COMMIT
[0m23:57:06.725732 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:57:06.725858 [debug] [MainThread]: On master: Close
[0m23:57:06.726047 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:06.726157 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m23:57:06.726259 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m23:57:06.726378 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m23:57:06.726495 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m23:57:06.726690 [info ] [MainThread]: 
[0m23:57:06.726846 [info ] [MainThread]: Finished running 7 table models, 1 view model in 0 hours 0 minutes and 0.58 seconds (0.58s).
[0m23:57:06.727430 [debug] [MainThread]: Command end result
[0m23:57:06.742201 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:57:06.743485 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:57:06.746454 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:57:06.746606 [info ] [MainThread]: 
[0m23:57:06.746778 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:57:06.746909 [info ] [MainThread]: 
[0m23:57:06.747047 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m23:57:06.748492 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9504814, "process_in_blocks": "0", "process_kernel_time": 0.222803, "process_mem_max_rss": "139902976", "process_out_blocks": "0", "process_user_time": 1.197928}
[0m23:57:06.748730 [debug] [MainThread]: Command `dbt run` succeeded at 23:57:06.748691 after 0.95 seconds
[0m23:57:06.748895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028f0350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096d4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029bec10>]}
[0m23:57:06.749057 [debug] [MainThread]: Flushing usage events
[0m23:57:07.020817 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:57:26.658092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120029f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12006b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200a7f50>]}


============================== 23:57:26.660143 | 296fa253-4720-4d10-a4ab-e2c1af9a3f01 ==============================
[0m23:57:26.660143 [info ] [MainThread]: Running with dbt=1.10.13
[0m23:57:26.660437 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'introspect': 'True', 'write_json': 'True', 'partial_parse': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'log_format': 'default', 'debug': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'invocation_command': 'dbt test', 'target_path': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'empty': 'None', 'quiet': 'False'}
[0m23:57:26.748310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1205757d0>]}
[0m23:57:26.777983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9b3d0>]}
[0m23:57:26.778495 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m23:57:26.821861 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m23:57:26.878899 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:57:26.879134 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:57:26.902423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1210b6050>]}
[0m23:57:26.942257 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:57:26.943202 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:57:26.954593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213abfd0>]}
[0m23:57:26.954879 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m23:57:26.955041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207ddc90>]}
[0m23:57:26.956114 [info ] [MainThread]: 
[0m23:57:26.956277 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:57:26.956391 [info ] [MainThread]: 
[0m23:57:26.956611 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:57:26.958560 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m23:57:26.958812 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m23:57:26.964028 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m23:57:26.965055 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m23:57:27.013219 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:57:27.013461 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:57:27.013609 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:57:27.013763 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:57:27.013884 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m23:57:27.014019 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m23:57:27.014141 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m23:57:27.014255 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m23:57:27.014373 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:27.014482 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:27.014587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:27.014710 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:27.040255 [debug] [ThreadPool]: SQL status: BEGIN in 0.025 seconds
[0m23:57:27.040489 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m23:57:27.040636 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m23:57:27.044807 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m23:57:27.045491 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m23:57:27.045690 [debug] [ThreadPool]: SQL status: BEGIN in 0.031 seconds
[0m23:57:27.045824 [debug] [ThreadPool]: SQL status: BEGIN in 0.031 seconds
[0m23:57:27.045963 [debug] [ThreadPool]: SQL status: BEGIN in 0.032 seconds
[0m23:57:27.046113 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m23:57:27.046231 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m23:57:27.046342 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m23:57:27.046464 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m23:57:27.046619 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m23:57:27.046770 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m23:57:27.046910 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m23:57:27.047216 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_raw_analysis)
[0m23:57:27.048359 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:57:27.048469 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m23:57:27.048568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:27.050535 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m23:57:27.050677 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m23:57:27.051144 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m23:57:27.051537 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m23:57:27.051726 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m23:57:27.052252 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m23:57:27.052420 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m23:57:27.052558 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m23:57:27.052801 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m23:57:27.053172 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m23:57:27.054212 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:57:27.054868 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m23:57:27.055216 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:27.056106 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m23:57:27.056267 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m23:57:27.056432 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m23:57:27.058071 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m23:57:27.058728 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m23:57:27.059129 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m23:57:27.061133 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m23:57:27.061263 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m23:57:27.061395 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:57:27.063331 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m23:57:27.063809 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m23:57:27.064137 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m23:57:27.067285 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.067471 [debug] [MainThread]: On master: BEGIN
[0m23:57:27.067583 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:27.073411 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:57:27.073605 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.073812 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m23:57:27.075711 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m23:57:27.076828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '296fa253-4720-4d10-a4ab-e2c1af9a3f01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e1f410>]}
[0m23:57:27.077027 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:27.077529 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.077649 [debug] [MainThread]: On master: BEGIN
[0m23:57:27.078176 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m23:57:27.078298 [debug] [MainThread]: On master: COMMIT
[0m23:57:27.078437 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.078534 [debug] [MainThread]: On master: COMMIT
[0m23:57:27.078843 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:57:27.078968 [debug] [MainThread]: On master: Close
[0m23:57:27.080571 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:57:27.080838 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:57:27.081009 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:57:27.081181 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:57:27.081384 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m23:57:27.081574 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m23:57:27.081767 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m23:57:27.081931 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m23:57:27.082145 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m23:57:27.082286 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m23:57:27.082443 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m23:57:27.082576 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m23:57:27.082717 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:57:27.082846 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:57:27.082966 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:57:27.083086 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:57:27.090488 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:57:27.092766 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:57:27.094560 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:57:27.096290 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:57:27.097594 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:57:27.097760 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:57:27.105527 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:57:27.105692 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:57:27.105821 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:57:27.106807 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:57:27.107892 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:57:27.108821 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:57:27.109082 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:57:27.109365 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m23:57:27.109548 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:57:27.109699 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:57:27.109842 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:27.110009 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m23:57:27.110152 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:57:27.110302 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m23:57:27.110532 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:57:27.110679 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m23:57:27.110813 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:57:27.111025 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:57:27.122711 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.122892 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.123050 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m23:57:27.123184 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m23:57:27.123319 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m23:57:27.123451 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m23:57:27.123590 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.123747 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.123884 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m23:57:27.124029 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.124249 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m23:57:27.124418 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.125899 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:57:27.126039 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m23:57:27.127729 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m23:57:27.128267 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m23:57:27.129234 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m23:57:27.129375 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m23:57:27.129629 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m23:57:27.129896 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m23:57:27.130115 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m23:57:27.130325 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m23:57:27.130496 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:57:27.130698 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.006 seconds
[0m23:57:27.130880 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:57:27.131089 [info ] [Thread-2 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m23:57:27.131269 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.007 seconds
[0m23:57:27.131803 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m23:57:27.131961 [info ] [Thread-1 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m23:57:27.132155 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m23:57:27.132641 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m23:57:27.132837 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m23:57:27.133018 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:57:27.133163 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m23:57:27.133330 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:57:27.135080 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:57:27.135241 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m23:57:27.135642 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m23:57:27.137399 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:57:27.137997 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m23:57:27.137809 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.06s]
[0m23:57:27.138288 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:57:27.138445 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:57:27.138680 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m23:57:27.138811 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:57:27.138959 [info ] [Thread-4 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m23:57:27.140000 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:57:27.140166 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:57:27.141111 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:57:27.141273 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m23:57:27.141468 [info ] [Thread-3 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m23:57:27.141783 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:57:27.141984 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:57:27.142188 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m23:57:27.142348 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:57:27.144035 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:57:27.144220 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m23:57:27.144372 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:57:27.144524 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m23:57:27.144703 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:57:27.147180 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:57:27.147394 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:27.147548 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:57:27.150004 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:57:27.150504 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:57:27.151751 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:57:27.151965 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:57:27.152187 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m23:57:27.152337 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:57:27.152609 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:57:27.152772 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m23:57:27.152903 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:57:27.155295 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m23:57:27.155461 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m23:57:27.155631 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m23:57:27.155781 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m23:57:27.155934 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.156095 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.157910 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m23:57:27.158091 [debug] [Thread-3 (]: SQL status: BEGIN in 0.005 seconds
[0m23:57:27.158272 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m23:57:27.158436 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m23:57:27.158599 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m23:57:27.158773 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.158927 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:57:27.159083 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:57:27.159838 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m23:57:27.159981 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:57:27.160527 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m23:57:27.161073 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m23:57:27.161209 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m23:57:27.161638 [debug] [Thread-3 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m23:57:27.161792 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.003 seconds
[0m23:57:27.161921 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m23:57:27.161521 [info ] [Thread-1 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.03s]
[0m23:57:27.162637 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m23:57:27.162182 [info ] [Thread-3 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.02s]
[0m23:57:27.162921 [info ] [Thread-2 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.03s]
[0m23:57:27.163235 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m23:57:27.163487 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m23:57:27.163615 [debug] [Thread-4 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m23:57:27.163813 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m23:57:27.163984 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:57:27.164174 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:57:27.164554 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:57:27.164433 [info ] [Thread-4 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.02s]
[0m23:57:27.164757 [info ] [Thread-1 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m23:57:27.164931 [info ] [Thread-3 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m23:57:27.165096 [info ] [Thread-2 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m23:57:27.165303 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m23:57:27.165484 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m23:57:27.165651 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m23:57:27.165798 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m23:57:27.165952 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:57:27.166122 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:57:27.166259 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:57:27.166387 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:57:27.166524 [info ] [Thread-4 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m23:57:27.168540 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:57:27.170111 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:57:27.171574 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:57:27.171773 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m23:57:27.172242 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:57:27.173854 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:57:27.174219 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:57:27.174409 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:57:27.174554 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:57:27.175807 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:57:27.176769 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:57:27.177861 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:57:27.178045 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:57:27.179258 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:57:27.179456 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:57:27.179606 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:57:27.179792 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m23:57:27.179954 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:57:27.180097 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m23:57:27.180255 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m23:57:27.180408 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:57:27.180534 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m23:57:27.180666 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m23:57:27.180881 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m23:57:27.181014 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:27.181203 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m23:57:27.192172 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.192426 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m23:57:27.192596 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.192779 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.193003 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.193195 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m23:57:27.193389 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m23:57:27.193557 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m23:57:27.193713 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m23:57:27.193869 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.194036 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.194211 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.195067 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m23:57:27.195817 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m23:57:27.196220 [debug] [Thread-1 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m23:57:27.196551 [info ] [Thread-1 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.03s]
[0m23:57:27.196792 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m23:57:27.196941 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:57:27.197087 [info ] [Thread-1 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m23:57:27.197340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m23:57:27.197497 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:57:27.199306 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:57:27.199645 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:57:27.201639 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:57:27.201950 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:57:27.202096 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m23:57:27.202229 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:57:27.203678 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.011 seconds
[0m23:57:27.204300 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m23:57:27.204723 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m23:57:27.204906 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.011 seconds
[0m23:57:27.205070 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.011 seconds
[0m23:57:27.206028 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m23:57:27.205385 [info ] [Thread-3 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m23:57:27.206669 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m23:57:27.206974 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m23:57:27.207144 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m23:57:27.207687 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m23:57:27.207533 [info ] [Thread-2 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.04s]
[0m23:57:27.207956 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m23:57:27.208423 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m23:57:27.208236 [info ] [Thread-4 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m23:57:27.208622 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m23:57:27.208920 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m23:57:27.209087 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:57:27.219462 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.010 seconds
[0m23:57:27.220193 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m23:57:27.220671 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m23:57:27.220933 [info ] [Thread-1 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.02s]
[0m23:57:27.221161 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m23:57:27.221859 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.221985 [debug] [MainThread]: On master: BEGIN
[0m23:57:27.222110 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:57:27.228156 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m23:57:27.228350 [debug] [MainThread]: On master: COMMIT
[0m23:57:27.228542 [debug] [MainThread]: Using postgres connection "master"
[0m23:57:27.228654 [debug] [MainThread]: On master: COMMIT
[0m23:57:27.228941 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m23:57:27.229052 [debug] [MainThread]: On master: Close
[0m23:57:27.229211 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:27.229331 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m23:57:27.229438 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m23:57:27.229538 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m23:57:27.229634 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m23:57:27.229783 [info ] [MainThread]: 
[0m23:57:27.229930 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:57:27.230698 [debug] [MainThread]: Command end result
[0m23:57:27.243634 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m23:57:27.244677 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m23:57:27.248020 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m23:57:27.248171 [info ] [MainThread]: 
[0m23:57:27.248337 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:57:27.248469 [info ] [MainThread]: 
[0m23:57:27.248605 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m23:57:27.250274 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6271264, "process_in_blocks": "0", "process_kernel_time": 0.214964, "process_mem_max_rss": "139821056", "process_out_blocks": "0", "process_user_time": 1.100282}
[0m23:57:27.250490 [debug] [MainThread]: Command `dbt test` succeeded at 23:57:27.250454 after 0.63 seconds
[0m23:57:27.250653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010b0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120099d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010e7690>]}
[0m23:57:27.250821 [debug] [MainThread]: Flushing usage events
[0m23:57:27.465326 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:01:35.627509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114db3590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e008d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e25fd0>]}


============================== 00:01:35.630230 | f8056849-44b6-4395-8585-2edd2f3eb503 ==============================
[0m00:01:35.630230 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:01:35.630516 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'empty': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select staging', 'static_parser': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'indirect_selection': 'eager', 'version_check': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'warn_error': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m00:01:35.724780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115004c10>]}
[0m00:01:35.755803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a25950>]}
[0m00:01:35.756661 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:01:35.804978 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:01:35.870344 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:01:35.870820 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_fxf_data.sql
[0m00:01:35.871009 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_pdl_data.sql
[0m00:01:36.099312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115db4bd0>]}
[0m00:01:36.138765 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:01:36.140052 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:01:36.153045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116404490>]}
[0m00:01:36.153299 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m00:01:36.153452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1135865d0>]}
[0m00:01:36.154187 [info ] [MainThread]: 
[0m00:01:36.154332 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:01:36.154445 [info ] [MainThread]: 
[0m00:01:36.154641 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:01:36.156131 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:01:36.182471 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:01:36.182677 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:01:36.182808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:36.204894 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.022 seconds
[0m00:01:36.205573 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:01:36.206436 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m00:01:36.206667 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:01:36.208962 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:01:36.209130 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m00:01:36.209342 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m00:01:36.210133 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:01:36.210271 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:01:36.211001 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:01:36.211725 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:01:36.211851 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:01:36.211961 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:36.212115 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:01:36.212267 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:01:36.212383 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:36.212572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:36.212699 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:36.228120 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m00:01:36.228285 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m00:01:36.228404 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m00:01:36.228505 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:01:36.228603 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m00:01:36.228718 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:01:36.228819 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:01:36.228941 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:01:36.229059 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:01:36.229180 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:01:36.229314 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:01:36.229499 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:01:36.234415 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m00:01:36.234574 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.005 seconds
[0m00:01:36.235125 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:01:36.235287 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.006 seconds
[0m00:01:36.235832 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:01:36.235967 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.006 seconds
[0m00:01:36.236461 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:01:36.236586 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:01:36.237032 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:01:36.237161 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:01:36.237418 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw)
[0m00:01:36.237699 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:01:36.237812 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:01:36.238772 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:01:36.238975 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m00:01:36.240530 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:01:36.241608 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:01:36.241755 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:36.241875 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:01:36.242098 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:36.249830 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:01:36.250043 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:01:36.250227 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:01:36.250371 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:01:36.250520 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:01:36.250668 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:01:36.253071 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m00:01:36.253812 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:01:36.253989 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m00:01:36.254504 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:01:36.254656 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:01:36.255086 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:01:36.257775 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.257947 [debug] [MainThread]: On master: BEGIN
[0m00:01:36.258061 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:36.264071 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:01:36.264230 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.264406 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:01:36.267499 [debug] [MainThread]: SQL status: SELECT 10 in 0.003 seconds
[0m00:01:36.268741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1168235d0>]}
[0m00:01:36.268987 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:36.269388 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.269506 [debug] [MainThread]: On master: BEGIN
[0m00:01:36.270023 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:01:36.270169 [debug] [MainThread]: On master: COMMIT
[0m00:01:36.270289 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.270404 [debug] [MainThread]: On master: COMMIT
[0m00:01:36.270754 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:01:36.270871 [debug] [MainThread]: On master: Close
[0m00:01:36.272969 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m00:01:36.273142 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m00:01:36.273357 [info ] [Thread-1 (]: 1 of 2 START sql view model public_staging.stg_fxf_data ........................ [RUN]
[0m00:01:36.273581 [info ] [Thread-2 (]: 2 of 2 START sql view model public_staging.stg_pdl_data ........................ [RUN]
[0m00:01:36.273769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.stg_fxf_data)
[0m00:01:36.273916 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_pdl_data)
[0m00:01:36.274062 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m00:01:36.274205 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m00:01:36.278386 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m00:01:36.279586 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m00:01:36.280231 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m00:01:36.280414 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m00:01:36.294327 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m00:01:36.296226 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m00:01:36.296957 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.297226 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.297429 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m00:01:36.297583 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m00:01:36.297725 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:01:36.297853 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:01:36.309677 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m00:01:36.309858 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m00:01:36.310030 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.310199 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.310373 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m00:01:36.310564 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m00:01:36.313407 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m00:01:36.316417 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.316574 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m00:01:36.316749 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data" rename to "stg_pdl_data__dbt_backup"
[0m00:01:36.318044 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.318219 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data" rename to "stg_fxf_data__dbt_backup"
[0m00:01:36.318634 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:01:36.319786 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.319920 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:01:36.320044 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m00:01:36.321103 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.321280 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m00:01:36.321554 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:01:36.328042 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m00:01:36.328368 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:01:36.329086 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:01:36.329268 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.329415 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.329543 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:01:36.329692 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:01:36.331827 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m00:01:36.332022 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m00:01:36.335773 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m00:01:36.336738 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m00:01:36.338733 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:01:36.339035 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:01:36.339172 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m00:01:36.339355 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m00:01:36.342247 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.003 seconds
[0m00:01:36.343150 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m00:01:36.343556 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.004 seconds
[0m00:01:36.344376 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m00:01:36.345109 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115601950>]}
[0m00:01:36.345263 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8056849-44b6-4395-8585-2edd2f3eb503', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1162d0710>]}
[0m00:01:36.345621 [info ] [Thread-2 (]: 2 of 2 OK created sql view model public_staging.stg_pdl_data ................... [[32mCREATE VIEW[0m in 0.07s]
[0m00:01:36.346169 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m00:01:36.345970 [info ] [Thread-1 (]: 1 of 2 OK created sql view model public_staging.stg_fxf_data ................... [[32mCREATE VIEW[0m in 0.07s]
[0m00:01:36.346502 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m00:01:36.347089 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.347228 [debug] [MainThread]: On master: BEGIN
[0m00:01:36.347340 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:01:36.352578 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m00:01:36.352754 [debug] [MainThread]: On master: COMMIT
[0m00:01:36.352882 [debug] [MainThread]: Using postgres connection "master"
[0m00:01:36.352990 [debug] [MainThread]: On master: COMMIT
[0m00:01:36.353284 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:01:36.353418 [debug] [MainThread]: On master: Close
[0m00:01:36.353601 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:36.353724 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m00:01:36.353815 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m00:01:36.353907 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m00:01:36.353995 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m00:01:36.354141 [info ] [MainThread]: 
[0m00:01:36.354357 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m00:01:36.354759 [debug] [MainThread]: Command end result
[0m00:01:36.367216 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:01:36.368285 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:01:36.370690 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:01:36.370823 [info ] [MainThread]: 
[0m00:01:36.370995 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:01:36.371117 [info ] [MainThread]: 
[0m00:01:36.371259 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m00:01:36.373469 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.78210217, "process_in_blocks": "0", "process_kernel_time": 0.204854, "process_mem_max_rss": "143884288", "process_out_blocks": "0", "process_user_time": 1.197874}
[0m00:01:36.373714 [debug] [MainThread]: Command `dbt run` succeeded at 00:01:36.373679 after 0.78 seconds
[0m00:01:36.373875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e5fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cbc1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f769d0>]}
[0m00:01:36.374023 [debug] [MainThread]: Flushing usage events
[0m00:01:36.647321 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:02:10.557449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107912e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11009e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100abc10>]}


============================== 00:02:10.559767 | 3708fca7-e9ab-432e-82ea-e38a7fba2dae ==============================
[0m00:02:10.559767 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:02:10.560078 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'partial_parse': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'introspect': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'target_path': 'None', 'fail_fast': 'False', 'empty': 'False', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m00:02:10.646262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11017e410>]}
[0m00:02:10.676308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104665710>]}
[0m00:02:10.676786 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:02:10.721634 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:02:10.784914 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:02:10.785149 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:02:10.809446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106019d0>]}
[0m00:02:10.851093 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:02:10.851975 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:02:10.863024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11141bf10>]}
[0m00:02:10.863283 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m00:02:10.863448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110df9f10>]}
[0m00:02:10.864467 [info ] [MainThread]: 
[0m00:02:10.864630 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:02:10.864748 [info ] [MainThread]: 
[0m00:02:10.864972 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:02:10.866845 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:02:10.867044 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:02:10.867261 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:02:10.867544 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:02:10.915505 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:02:10.915692 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:02:10.915837 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:02:10.915991 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:02:10.916112 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:02:10.916251 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:02:10.916389 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:02:10.916520 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:02:10.916644 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:10.916758 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:10.916874 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:10.916988 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:10.951176 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.034 seconds
[0m00:02:10.951369 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m00:02:10.951528 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m00:02:10.952037 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:02:10.952154 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m00:02:10.952563 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:02:10.952936 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:02:10.954096 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:02:10.954577 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:02:10.954937 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:02:10.955150 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.964212 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.009 seconds
[0m00:02:10.964618 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:02:10.965503 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m00:02:10.965710 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m00:02:10.968163 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:02:10.968394 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m00:02:10.968581 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m00:02:10.969425 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:02:10.969558 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:02:10.970636 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:02:10.971328 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:02:10.971444 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:02:10.971552 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.971665 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:02:10.971772 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:02:10.971874 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.972075 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.972199 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.983779 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:02:10.983932 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:02:10.984054 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:02:10.984173 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:02:10.984312 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:02:10.984463 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:02:10.984581 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:02:10.984696 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:02:10.984834 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:02:10.984982 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:02:10.985128 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:02:10.985268 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:02:10.988454 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:02:10.988651 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:02:10.988846 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m00:02:10.989543 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:02:10.989695 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:02:10.990193 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:02:10.990639 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:02:10.991150 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:02:10.991295 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:02:10.991619 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_marts)
[0m00:02:10.991774 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:02:10.991880 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:02:10.992163 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:02:10.993062 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:02:10.993304 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_raw_analysis)
[0m00:02:10.994040 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:02:10.995447 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:02:10.995627 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:10.995760 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:02:10.995970 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:11.002855 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:02:11.003011 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:02:11.003163 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:02:11.003279 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:02:11.003422 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:02:11.003568 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:02:11.006786 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:02:11.007347 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:02:11.007472 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:02:11.007895 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:02:11.008024 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:02:11.008541 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:02:11.011052 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.011214 [debug] [MainThread]: On master: BEGIN
[0m00:02:11.011312 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:02:11.019645 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m00:02:11.019824 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.019986 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:02:11.022589 [debug] [MainThread]: SQL status: SELECT 6 in 0.002 seconds
[0m00:02:11.023454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111741210>]}
[0m00:02:11.023672 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:11.024050 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.024176 [debug] [MainThread]: On master: BEGIN
[0m00:02:11.024715 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:02:11.024850 [debug] [MainThread]: On master: COMMIT
[0m00:02:11.024959 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.025065 [debug] [MainThread]: On master: COMMIT
[0m00:02:11.026000 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.026123 [debug] [MainThread]: On master: Close
[0m00:02:11.028185 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m00:02:11.028355 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m00:02:11.028562 [info ] [Thread-1 (]: 1 of 13 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m00:02:11.028785 [info ] [Thread-2 (]: 2 of 13 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m00:02:11.028973 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_fxf_data)
[0m00:02:11.029157 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_pdl_data)
[0m00:02:11.029311 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m00:02:11.029451 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m00:02:11.032813 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m00:02:11.033979 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m00:02:11.034782 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m00:02:11.034939 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m00:02:11.049609 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m00:02:11.052267 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m00:02:11.052902 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.053118 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m00:02:11.053290 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.053432 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:11.053567 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m00:02:11.053794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:11.065071 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m00:02:11.065254 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m00:02:11.065419 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.065547 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.065686 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m00:02:11.065828 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m00:02:11.128597 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.063 seconds
[0m00:02:11.133462 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.133696 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.068 seconds
[0m00:02:11.133945 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m00:02:11.135781 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.136052 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m00:02:11.136547 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.137862 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.138052 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:02:11.138239 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m00:02:11.139678 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.139894 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m00:02:11.140128 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.146609 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m00:02:11.147989 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m00:02:11.148718 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m00:02:11.148904 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.149077 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.149242 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m00:02:11.149402 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m00:02:11.156939 [debug] [Thread-1 (]: SQL status: COMMIT in 0.007 seconds
[0m00:02:11.157164 [debug] [Thread-2 (]: SQL status: COMMIT in 0.008 seconds
[0m00:02:11.160305 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m00:02:11.161508 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m00:02:11.163991 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:02:11.164378 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:02:11.164565 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m00:02:11.164741 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m00:02:11.167212 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:02:11.167374 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:02:11.168554 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m00:02:11.169133 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m00:02:11.170829 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11192f750>]}
[0m00:02:11.171008 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f2c4d0>]}
[0m00:02:11.171358 [info ] [Thread-1 (]: 1 of 13 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.14s]
[0m00:02:11.171661 [info ] [Thread-2 (]: 2 of 13 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.14s]
[0m00:02:11.171942 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m00:02:11.172171 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m00:02:11.172491 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_fxf_data
[0m00:02:11.172733 [info ] [Thread-2 (]: 3 of 13 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m00:02:11.172994 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_company_profiling
[0m00:02:11.173179 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_pdl_data, now model.dbt_service.stg_fxf_data)
[0m00:02:11.173348 [debug] [Thread-4 (]: Began running node model.dbt_service.raw_data_profiling
[0m00:02:11.173515 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_location_profiling
[0m00:02:11.173725 [info ] [Thread-1 (]: 4 of 13 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m00:02:11.173918 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m00:02:11.174114 [info ] [Thread-4 (]: 5 of 13 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m00:02:11.174339 [info ] [Thread-3 (]: 6 of 13 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m00:02:11.174528 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_fxf_data, now model.dbt_service.raw_company_profiling)
[0m00:02:11.176171 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m00:02:11.176355 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_data_profiling)
[0m00:02:11.176535 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_location_profiling)
[0m00:02:11.176696 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m00:02:11.176897 [debug] [Thread-4 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m00:02:11.177185 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m00:02:11.179023 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m00:02:11.179275 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_fxf_data
[0m00:02:11.180759 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m00:02:11.183641 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m00:02:11.191684 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m00:02:11.192102 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_company_profiling
[0m00:02:11.193669 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m00:02:11.193825 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_location_profiling
[0m00:02:11.194104 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:02:11.194275 [debug] [Thread-4 (]: Began executing node model.dbt_service.raw_data_profiling
[0m00:02:11.195707 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m00:02:11.195891 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m00:02:11.197219 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m00:02:11.197384 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.197590 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:11.197806 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m00:02:11.197980 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.198231 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.198380 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:11.198524 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m00:02:11.198678 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m00:02:11.198893 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:02:11.199037 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:11.207987 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m00:02:11.208189 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m00:02:11.208331 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m00:02:11.208485 [debug] [Thread-4 (]: SQL status: BEGIN in 0.009 seconds
[0m00:02:11.208661 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.208820 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.208967 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:02:11.209132 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.209357 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m00:02:11.209707 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m00:02:11.210024 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m00:02:11.210277 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m00:02:11.211855 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:02:11.213390 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:02:11.213563 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m00:02:11.214130 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.214751 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:02:11.214918 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:02:11.215052 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:02:11.215728 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.216803 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m00:02:11.218483 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:02:11.218742 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m00:02:11.219369 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:02:11.220015 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: Close
[0m00:02:11.220292 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11195d390>]}
[0m00:02:11.220570 [info ] [Thread-2 (]: 3 of 13 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m00:02:11.220806 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_fxf_data
[0m00:02:11.220971 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m00:02:11.221190 [info ] [Thread-2 (]: 7 of 13 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m00:02:11.221361 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_fxf_data, now model.dbt_service.stg_pdl_data)
[0m00:02:11.221507 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m00:02:11.222941 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m00:02:11.223248 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m00:02:11.224692 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m00:02:11.225086 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:02:11.225244 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m00:02:11.225380 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:11.231238 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m00:02:11.231457 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:02:11.231635 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m00:02:11.232794 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:02:11.235139 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:02:11.235323 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m00:02:11.235943 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.236468 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:02:11.236604 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:02:11.236728 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:02:11.237317 [debug] [Thread-2 (]: SQL status: COMMIT in 0.000 seconds
[0m00:02:11.238383 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m00:02:11.238724 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:02:11.238876 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m00:02:11.239307 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:02:11.239879 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m00:02:11.240182 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10744cb50>]}
[0m00:02:11.240469 [info ] [Thread-2 (]: 7 of 13 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.02s]
[0m00:02:11.240697 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m00:02:11.241021 [debug] [Thread-2 (]: Began running node model.dbt_service.company_analysis
[0m00:02:11.241268 [info ] [Thread-2 (]: 8 of 13 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m00:02:11.241468 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_pdl_data, now model.dbt_service.company_analysis)
[0m00:02:11.241615 [debug] [Thread-2 (]: Began compiling node model.dbt_service.company_analysis
[0m00:02:11.243108 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m00:02:11.243631 [debug] [Thread-2 (]: Began executing node model.dbt_service.company_analysis
[0m00:02:11.245265 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m00:02:11.245715 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.245920 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: BEGIN
[0m00:02:11.246066 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:11.251629 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m00:02:11.251817 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.251997 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    array_agg(distinct location_item) as all_locations
from combined_companies,
     unnest(locations) as location_item
group by company
order by total_employees desc
  );
  
[0m00:02:11.301008 [debug] [Thread-1 (]: SQL status: SELECT 180 in 0.090 seconds
[0m00:02:11.302778 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.302951 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m00:02:11.303582 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.304916 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.305079 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m00:02:11.305367 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.095 seconds
[0m00:02:11.306738 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.306906 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:02:11.307099 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m00:02:11.307705 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m00:02:11.307924 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.308115 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m00:02:11.308441 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:02:11.309722 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.309883 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.310070 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m00:02:11.311095 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m00:02:11.311482 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:02:11.311636 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.311777 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m00:02:11.312390 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m00:02:11.312580 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.312744 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m00:02:11.313474 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.313923 [debug] [Thread-1 (]: On model.dbt_service.raw_company_profiling: Close
[0m00:02:11.314062 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.315375 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m00:02:11.315558 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119e4810>]}
[0m00:02:11.315934 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:02:11.316240 [info ] [Thread-1 (]: 4 of 13 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.14s]
[0m00:02:11.316441 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m00:02:11.316650 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_company_profiling
[0m00:02:11.316840 [debug] [Thread-1 (]: Began running node model.dbt_service.location_analysis
[0m00:02:11.317054 [info ] [Thread-1 (]: 9 of 13 START sql table model public_marts.location_analysis ................... [RUN]
[0m00:02:11.317232 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.location_analysis)
[0m00:02:11.317383 [debug] [Thread-1 (]: Began compiling node model.dbt_service.location_analysis
[0m00:02:11.319647 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m00:02:11.319800 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.003 seconds
[0m00:02:11.320344 [debug] [Thread-4 (]: On model.dbt_service.raw_data_profiling: Close
[0m00:02:11.320597 [debug] [Thread-1 (]: Began executing node model.dbt_service.location_analysis
[0m00:02:11.320741 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119ddb90>]}
[0m00:02:11.322407 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m00:02:11.322725 [info ] [Thread-4 (]: 5 of 13 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.14s]
[0m00:02:11.323031 [debug] [Thread-4 (]: Finished running node model.dbt_service.raw_data_profiling
[0m00:02:11.323212 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_company_profiling
[0m00:02:11.323437 [info ] [Thread-4 (]: 10 of 13 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m00:02:11.323618 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.323846 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_company_profiling)
[0m00:02:11.323998 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: BEGIN
[0m00:02:11.324148 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m00:02:11.324297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:11.325809 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m00:02:11.326349 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_company_profiling
[0m00:02:11.327892 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m00:02:11.328232 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.328412 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m00:02:11.328545 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:11.331686 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m00:02:11.331868 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.332044 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location
)

select
    location,
    contact_count,
    company_count,
    unique_titles,
    data_sources,
    round(avg_revenue::numeric, 2) as avg_revenue
from location_stats
order by contact_count desc
  );
  
[0m00:02:11.333757 [debug] [Thread-4 (]: SQL status: BEGIN in 0.005 seconds
[0m00:02:11.333922 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.334183 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m00:02:11.336329 [debug] [Thread-2 (]: SQL status: SELECT 91 in 0.084 seconds
[0m00:02:11.337980 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.338161 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m00:02:11.338777 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.340320 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.340510 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m00:02:11.341062 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.341738 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:02:11.341900 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.342034 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:02:11.342757 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.343973 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m00:02:11.344323 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:02:11.344481 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m00:02:11.345553 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.346180 [debug] [Thread-2 (]: On model.dbt_service.company_analysis: Close
[0m00:02:11.346440 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106d6090>]}
[0m00:02:11.346710 [info ] [Thread-2 (]: 8 of 13 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.10s]
[0m00:02:11.346936 [debug] [Thread-2 (]: Finished running node model.dbt_service.company_analysis
[0m00:02:11.347097 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_data_profiling
[0m00:02:11.347372 [info ] [Thread-2 (]: 11 of 13 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m00:02:11.347618 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_data_profiling)
[0m00:02:11.347779 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m00:02:11.349444 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m00:02:11.349837 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_data_profiling
[0m00:02:11.351409 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m00:02:11.352062 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.352204 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m00:02:11.352331 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:11.358160 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m00:02:11.358354 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.358629 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m00:02:11.377459 [debug] [Thread-3 (]: SQL status: SELECT 152 in 0.167 seconds
[0m00:02:11.379150 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.379320 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m00:02:11.380010 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:02:11.382025 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.382217 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m00:02:11.382753 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.383443 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m00:02:11.383597 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.383727 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m00:02:11.384450 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.385601 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m00:02:11.385920 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:02:11.386072 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m00:02:11.387171 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.387796 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: Close
[0m00:02:11.388091 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a1cbd0>]}
[0m00:02:11.388368 [info ] [Thread-3 (]: 6 of 13 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.21s]
[0m00:02:11.388600 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_location_profiling
[0m00:02:11.388767 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_location_profiling
[0m00:02:11.388976 [info ] [Thread-3 (]: 12 of 13 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m00:02:11.389148 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_location_profiling, now model.dbt_service.staging_location_profiling)
[0m00:02:11.389290 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m00:02:11.391205 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m00:02:11.391758 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_location_profiling
[0m00:02:11.393192 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m00:02:11.393519 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.393653 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m00:02:11.393788 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:02:11.399160 [debug] [Thread-3 (]: SQL status: BEGIN in 0.005 seconds
[0m00:02:11.399376 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.399698 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
            contacts_with_name, contacts_with_valid_email, email_validity_pct
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue,
    contacts_with_name, contacts_with_valid_email, email_validity_pct, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m00:02:11.451201 [debug] [Thread-1 (]: SQL status: SELECT 76 in 0.119 seconds
[0m00:02:11.452836 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.453002 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m00:02:11.453604 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.454973 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.455135 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m00:02:11.455637 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.456345 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:02:11.456502 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.456635 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:02:11.457238 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m00:02:11.458278 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m00:02:11.458610 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:02:11.458761 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m00:02:11.459635 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.460189 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: Close
[0m00:02:11.460431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106d8710>]}
[0m00:02:11.460691 [info ] [Thread-1 (]: 9 of 13 OK created sql table model public_marts.location_analysis .............. [[32mSELECT 76[0m in 0.14s]
[0m00:02:11.460919 [debug] [Thread-1 (]: Finished running node model.dbt_service.location_analysis
[0m00:02:11.461220 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m00:02:11.461495 [info ] [Thread-1 (]: 13 of 13 START sql view model public_marts.data_overview ....................... [RUN]
[0m00:02:11.461700 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.location_analysis, now model.dbt_service.data_overview)
[0m00:02:11.461850 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m00:02:11.463732 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m00:02:11.464369 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m00:02:11.465823 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m00:02:11.466103 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:02:11.466237 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m00:02:11.466362 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:11.472185 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m00:02:11.472401 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:02:11.472584 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m00:02:11.473841 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:02:11.475179 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:02:11.475362 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m00:02:11.475924 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.476421 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m00:02:11.476579 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:02:11.476716 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m00:02:11.477316 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m00:02:11.478208 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m00:02:11.479353 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:02:11.479496 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m00:02:11.479932 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:02:11.480452 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m00:02:11.480709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a63f10>]}
[0m00:02:11.480946 [info ] [Thread-1 (]: 13 of 13 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.02s]
[0m00:02:11.481156 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m00:02:11.573489 [debug] [Thread-4 (]: SQL status: SELECT 271 in 0.239 seconds
[0m00:02:11.575032 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.575207 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m00:02:11.575781 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.576955 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.577160 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m00:02:11.577632 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.578203 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:02:11.578344 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.578476 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:02:11.579423 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.580501 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m00:02:11.580819 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:02:11.580971 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m00:02:11.582054 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.582584 [debug] [Thread-4 (]: On model.dbt_service.staging_company_profiling: Close
[0m00:02:11.582844 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119dfbd0>]}
[0m00:02:11.583115 [info ] [Thread-4 (]: 10 of 13 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.26s]
[0m00:02:11.583347 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_company_profiling
[0m00:02:11.590245 [debug] [Thread-2 (]: SQL status: SELECT 2 in 0.231 seconds
[0m00:02:11.591744 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.591917 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m00:02:11.592483 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.593693 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.593845 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m00:02:11.594379 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.594961 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:02:11.595108 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.595239 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:02:11.596052 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.597117 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m00:02:11.597396 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:02:11.597536 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m00:02:11.598576 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.599120 [debug] [Thread-2 (]: On model.dbt_service.staging_data_profiling: Close
[0m00:02:11.599375 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111732290>]}
[0m00:02:11.599642 [info ] [Thread-2 (]: 11 of 13 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.25s]
[0m00:02:11.599971 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_data_profiling
[0m00:02:11.637167 [debug] [Thread-3 (]: SQL status: SELECT 152 in 0.237 seconds
[0m00:02:11.639468 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.639746 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m00:02:11.640441 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.642002 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.642300 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m00:02:11.642885 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:02:11.643511 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:02:11.643690 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.643850 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:02:11.644564 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m00:02:11.645555 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m00:02:11.646036 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:02:11.646351 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m00:02:11.647727 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:02:11.648327 [debug] [Thread-3 (]: On model.dbt_service.staging_location_profiling: Close
[0m00:02:11.648648 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3708fca7-e9ab-432e-82ea-e38a7fba2dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11171dbd0>]}
[0m00:02:11.649063 [info ] [Thread-3 (]: 12 of 13 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.26s]
[0m00:02:11.649423 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_location_profiling
[0m00:02:11.650205 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.650545 [debug] [MainThread]: On master: BEGIN
[0m00:02:11.650736 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:02:11.657105 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:02:11.657375 [debug] [MainThread]: On master: COMMIT
[0m00:02:11.657530 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:11.657697 [debug] [MainThread]: On master: COMMIT
[0m00:02:11.658011 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:02:11.658135 [debug] [MainThread]: On master: Close
[0m00:02:11.658349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:02:11.658482 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m00:02:11.658590 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m00:02:11.658702 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m00:02:11.658801 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m00:02:11.659000 [info ] [MainThread]: 
[0m00:02:11.659167 [info ] [MainThread]: Finished running 10 table models, 3 view models in 0 hours 0 minutes and 0.79 seconds (0.79s).
[0m00:02:11.660118 [debug] [MainThread]: Command end result
[0m00:02:11.676722 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:02:11.677564 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:02:11.681500 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:02:11.681705 [info ] [MainThread]: 
[0m00:02:11.681901 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:02:11.682052 [info ] [MainThread]: 
[0m00:02:11.682229 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m00:02:11.684222 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.1620938, "process_in_blocks": "0", "process_kernel_time": 0.208143, "process_mem_max_rss": "139231232", "process_out_blocks": "0", "process_user_time": 1.232253}
[0m00:02:11.684537 [debug] [MainThread]: Command `dbt run` succeeded at 00:02:11.684490 after 1.16 seconds
[0m00:02:11.684755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10255ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079136d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1025a3590>]}
[0m00:02:11.684951 [debug] [MainThread]: Flushing usage events
[0m00:02:11.958355 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:02:18.262542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b42ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b42b650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4a7f90>]}


============================== 00:02:18.264887 | 0f999bb9-6be6-4da6-9281-c1e6a7a7b60e ==============================
[0m00:02:18.264887 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:02:18.265236 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt test', 'debug': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'empty': 'None', 'introspect': 'True', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'cache_selected_only': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'use_colors': 'True'}
[0m00:02:18.351864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b903e10>]}
[0m00:02:18.384320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f195d0>]}
[0m00:02:18.384920 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:02:18.429843 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:02:18.495706 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:02:18.495974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:02:18.520860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c237c90>]}
[0m00:02:18.563085 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:02:18.564165 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:02:18.579995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c6abf10>]}
[0m00:02:18.580316 [info ] [MainThread]: Found 13 models, 5 seeds, 13 data tests, 2 sources, 449 macros
[0m00:02:18.580480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb4db90>]}
[0m00:02:18.581651 [info ] [MainThread]: 
[0m00:02:18.581818 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:02:18.581931 [info ] [MainThread]: 
[0m00:02:18.582171 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:02:18.584208 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m00:02:18.584427 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m00:02:18.584718 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:02:18.585617 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m00:02:18.632587 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:02:18.632779 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:02:18.632933 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:02:18.633085 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:02:18.633205 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:02:18.633334 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:02:18.633455 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:02:18.633564 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:02:18.633672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:18.633774 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:18.633876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:18.633978 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:18.659654 [debug] [ThreadPool]: SQL status: BEGIN in 0.026 seconds
[0m00:02:18.659843 [debug] [ThreadPool]: SQL status: BEGIN in 0.026 seconds
[0m00:02:18.660027 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:02:18.660151 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:02:18.660287 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:02:18.660427 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:02:18.661479 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m00:02:18.661636 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m00:02:18.661765 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:02:18.661892 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:02:18.662028 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:02:18.662178 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:02:18.665366 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m00:02:18.665900 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:02:18.666039 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m00:02:18.666507 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:02:18.666648 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:02:18.666802 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m00:02:18.666906 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:02:18.667311 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:02:18.667432 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:02:18.667842 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:02:18.668076 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public)
[0m00:02:18.668434 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_raw_analysis)
[0m00:02:18.669520 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:02:18.669639 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:02:18.669755 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:02:18.670559 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:02:18.670701 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:02:18.671369 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:02:18.671617 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:18.671730 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:18.678157 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:02:18.678297 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:02:18.678435 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:02:18.678556 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:02:18.678690 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:02:18.678849 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:02:18.680764 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m00:02:18.680891 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:02:18.681374 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:02:18.681797 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:02:18.682164 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:02:18.682297 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:02:18.685753 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.685979 [debug] [MainThread]: On master: BEGIN
[0m00:02:18.686099 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:02:18.691933 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:02:18.692080 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.692232 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:02:18.694795 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:02:18.696078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f999bb9-6be6-4da6-9281-c1e6a7a7b60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107785f50>]}
[0m00:02:18.696297 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:18.696662 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.696799 [debug] [MainThread]: On master: BEGIN
[0m00:02:18.697442 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:02:18.697643 [debug] [MainThread]: On master: COMMIT
[0m00:02:18.697758 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.697863 [debug] [MainThread]: On master: COMMIT
[0m00:02:18.699077 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m00:02:18.699286 [debug] [MainThread]: On master: Close
[0m00:02:18.701512 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:02:18.701707 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:02:18.701847 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:02:18.702138 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:02:18.701999 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m00:02:18.702358 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m00:02:18.702543 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m00:02:18.702721 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m00:02:18.702921 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m00:02:18.703077 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m00:02:18.703219 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m00:02:18.703360 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m00:02:18.703508 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:02:18.703660 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:02:18.703789 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:02:18.703925 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:02:18.710614 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:02:18.712137 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:02:18.713606 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:02:18.715376 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:02:18.716152 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:02:18.716353 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:02:18.716495 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:02:18.716644 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:02:18.724794 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:02:18.725824 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:02:18.726819 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:02:18.727704 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:02:18.728613 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:02:18.728783 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:02:18.728936 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:02:18.729073 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:02:18.729224 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m00:02:18.729361 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m00:02:18.729483 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m00:02:18.729612 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m00:02:18.729743 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:18.729870 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:18.729986 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:02:18.730103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:18.742436 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m00:02:18.742593 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m00:02:18.742732 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m00:02:18.742874 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:02:18.743014 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m00:02:18.743135 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:02:18.743265 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:02:18.743414 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.743548 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:02:18.743682 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.743823 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.744002 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.746630 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m00:02:18.746807 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m00:02:18.748466 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m00:02:18.748948 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m00:02:18.749574 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m00:02:18.749716 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m00:02:18.749984 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m00:02:18.750322 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m00:02:18.750494 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.006 seconds
[0m00:02:18.750797 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:02:18.750955 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.007 seconds
[0m00:02:18.751174 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:02:18.751745 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m00:02:18.751925 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:02:18.752459 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m00:02:18.752644 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:02:18.752845 [info ] [Thread-1 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m00:02:18.753078 [info ] [Thread-2 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m00:02:18.753248 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m00:02:18.753383 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m00:02:18.753591 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m00:02:18.753757 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m00:02:18.753998 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m00:02:18.754238 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m00:02:18.754408 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:02:18.754549 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:02:18.754757 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:02:18.754955 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:02:18.756810 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:02:18.758249 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:02:18.758426 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:02:18.758619 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:02:18.759099 [info ] [Thread-3 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m00:02:18.759358 [info ] [Thread-4 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m00:02:18.759520 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:02:18.759719 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m00:02:18.759869 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:02:18.760028 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m00:02:18.761014 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:02:18.761160 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:02:18.762037 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:02:18.762179 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:02:18.763767 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:02:18.766490 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:02:18.766695 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:02:18.766956 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:02:18.767244 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m00:02:18.767386 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:02:18.767567 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m00:02:18.767730 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:18.767861 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:02:18.769765 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:02:18.769930 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:18.771184 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:02:18.771836 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:02:18.772114 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:02:18.772292 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m00:02:18.772466 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m00:02:18.772598 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:18.772736 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:02:18.787328 [debug] [Thread-2 (]: SQL status: BEGIN in 0.020 seconds
[0m00:02:18.787548 [debug] [Thread-3 (]: SQL status: BEGIN in 0.015 seconds
[0m00:02:18.787724 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m00:02:18.787903 [debug] [Thread-4 (]: SQL status: BEGIN in 0.015 seconds
[0m00:02:18.788044 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:02:18.788207 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:02:18.788357 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:02:18.788522 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:02:18.788688 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.788834 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.788988 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m00:02:18.789138 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.790166 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.001 seconds
[0m00:02:18.790884 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m00:02:18.791265 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m00:02:18.791562 [info ] [Thread-4 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.03s]
[0m00:02:18.791779 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:02:18.791929 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:02:18.792131 [info ] [Thread-4 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m00:02:18.792367 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.003 seconds
[0m00:02:18.792582 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m00:02:18.792723 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m00:02:18.792861 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.004 seconds
[0m00:02:18.793433 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m00:02:18.793593 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:02:18.794051 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m00:02:18.794485 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m00:02:18.796403 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:02:18.796583 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m00:02:18.796934 [info ] [Thread-2 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.04s]
[0m00:02:18.797165 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:02:18.797403 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:02:18.797714 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:02:18.797860 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m00:02:18.797993 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m00:02:18.797583 [info ] [Thread-2 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m00:02:18.798989 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:02:18.799240 [info ] [Thread-3 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.04s]
[0m00:02:18.799445 [info ] [Thread-1 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.05s]
[0m00:02:18.799602 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m00:02:18.799845 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:02:18.800070 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:02:18.800220 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:02:18.800376 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:02:18.800533 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:02:18.800713 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:02:18.802481 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:02:18.802660 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m00:02:18.802828 [info ] [Thread-3 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m00:02:18.803031 [info ] [Thread-1 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m00:02:18.803226 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:18.803397 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m00:02:18.803568 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m00:02:18.803836 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:02:18.803996 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:02:18.804147 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:02:18.805260 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:02:18.806898 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:02:18.808230 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:02:18.808633 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:02:18.808862 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:02:18.809087 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m00:02:18.809259 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:02:18.810461 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:02:18.810643 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m00:02:18.810797 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:02:18.811805 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:02:18.812084 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:02:18.812457 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:02:18.812626 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.812827 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m00:02:18.813018 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:02:18.813210 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:02:18.813430 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m00:02:18.813557 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:02:18.817009 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.004 seconds
[0m00:02:18.817659 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m00:02:18.817987 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m00:02:18.818259 [info ] [Thread-4 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.03s]
[0m00:02:18.818474 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:02:18.818633 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:02:18.818802 [info ] [Thread-4 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m00:02:18.818965 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m00:02:18.819109 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:02:18.820782 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:02:18.821075 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:02:18.822856 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:02:18.823309 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:02:18.823522 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m00:02:18.823689 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m00:02:18.823833 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m00:02:18.824013 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m00:02:18.824181 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:02:18.824340 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:02:18.824500 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:02:18.824663 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:02:18.824829 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.825014 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.825181 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.832907 [debug] [Thread-4 (]: SQL status: BEGIN in 0.008 seconds
[0m00:02:18.833174 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:02:18.833339 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:02:18.839845 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.014 seconds
[0m00:02:18.840015 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.015 seconds
[0m00:02:18.840611 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m00:02:18.841033 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m00:02:18.841504 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m00:02:18.841669 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m00:02:18.841934 [info ] [Thread-3 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.04s]
[0m00:02:18.842374 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:02:18.842139 [info ] [Thread-2 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m00:02:18.842668 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:02:18.872951 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.048 seconds
[0m00:02:18.873674 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m00:02:18.874146 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m00:02:18.874410 [info ] [Thread-1 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.07s]
[0m00:02:18.874639 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:02:18.886548 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.053 seconds
[0m00:02:18.887211 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m00:02:18.887672 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m00:02:18.888005 [info ] [Thread-4 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.07s]
[0m00:02:18.888260 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:02:18.888849 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.888979 [debug] [MainThread]: On master: BEGIN
[0m00:02:18.889104 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:02:18.894757 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:02:18.894934 [debug] [MainThread]: On master: COMMIT
[0m00:02:18.895049 [debug] [MainThread]: Using postgres connection "master"
[0m00:02:18.895158 [debug] [MainThread]: On master: COMMIT
[0m00:02:18.895632 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:02:18.895863 [debug] [MainThread]: On master: Close
[0m00:02:18.896075 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:02:18.896207 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m00:02:18.896316 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m00:02:18.896422 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m00:02:18.896518 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m00:02:18.896677 [info ] [MainThread]: 
[0m00:02:18.896842 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m00:02:18.897679 [debug] [MainThread]: Command end result
[0m00:02:18.910916 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:02:18.911679 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:02:18.914694 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:02:18.914830 [info ] [MainThread]: 
[0m00:02:18.914986 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:02:18.915117 [info ] [MainThread]: 
[0m00:02:18.915248 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m00:02:18.917034 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6938371, "process_in_blocks": "0", "process_kernel_time": 0.192917, "process_mem_max_rss": "139542528", "process_out_blocks": "0", "process_user_time": 1.115483}
[0m00:02:18.917438 [debug] [MainThread]: Command `dbt test` succeeded at 00:02:18.917387 after 0.69 seconds
[0m00:02:18.917641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4a7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b42a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a56910>]}
[0m00:02:18.917856 [debug] [MainThread]: Flushing usage events
[0m00:02:19.136619 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:11:25.894452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d2a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122dab410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122dabb10>]}


============================== 00:11:25.897107 | 07672d6e-e9c6-4504-869f-966a84637a81 ==============================
[0m00:11:25.897107 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:11:25.897413 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'empty': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'write_json': 'True', 'warn_error': 'None', 'debug': 'False', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'version_check': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'target_path': 'None', 'printer_width': '80', 'log_cache_events': 'False', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs'}
[0m00:11:26.033261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120939310>]}
[0m00:11:26.064562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122e97750>]}
[0m00:11:26.065337 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:11:26.112301 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:11:26.178603 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 4 files changed.
[0m00:11:26.178930 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://seeds/pdl_sample.csv
[0m00:11:26.179066 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://seeds/fxf_sample.csv
[0m00:11:26.179268 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/location_analysis.sql
[0m00:11:26.179426 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_location_profiling.sql
[0m00:11:26.179575 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/company_analysis.sql
[0m00:11:26.179716 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_company_profiling.sql
[0m00:11:26.418606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b55f10>]}
[0m00:11:26.458574 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:11:26.459694 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:11:26.471784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124b24190>]}
[0m00:11:26.472053 [info ] [MainThread]: Found 13 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:11:26.472212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d28510>]}
[0m00:11:26.473166 [info ] [MainThread]: 
[0m00:11:26.473316 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:11:26.473438 [info ] [MainThread]: 
[0m00:11:26.473727 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:11:26.475473 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:11:26.475832 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:11:26.480763 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:11:26.481137 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:11:26.505686 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:11:26.505887 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:11:26.506038 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:11:26.506181 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:11:26.506342 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:11:26.506483 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:11:26.506612 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:11:26.506745 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:11:26.506879 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:26.507009 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:26.507118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:26.507247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:26.545306 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.038 seconds
[0m00:11:26.545494 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.038 seconds
[0m00:11:26.545655 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.039 seconds
[0m00:11:26.545825 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.039 seconds
[0m00:11:26.546367 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:11:26.546788 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:11:26.547153 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:11:26.547504 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:11:26.548495 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:11:26.548894 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:11:26.549013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.558307 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.009 seconds
[0m00:11:26.558759 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:11:26.559667 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m00:11:26.559878 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m00:11:26.562114 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:11:26.562314 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m00:11:26.562499 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m00:11:26.563261 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:11:26.563411 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:11:26.564212 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:11:26.565217 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:11:26.565363 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:11:26.565479 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.565597 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:11:26.565708 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:11:26.565813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.565998 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.566120 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.580905 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:11:26.581078 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:11:26.581223 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:11:26.581370 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:11:26.581501 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m00:11:26.581618 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:11:26.581741 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:11:26.581883 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:11:26.582015 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:11:26.582157 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:11:26.582313 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:11:26.582462 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:11:26.583412 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m00:11:26.584187 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:11:26.584407 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m00:11:26.584685 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m00:11:26.584814 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m00:11:26.585421 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:11:26.585555 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:11:26.585979 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:11:26.586375 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:11:26.586656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public)
[0m00:11:26.587117 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:11:26.587234 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:11:26.588145 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:11:26.588286 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:11:26.588515 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_raw)
[0m00:11:26.588870 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:11:26.590308 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:11:26.590494 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.590631 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:11:26.590848 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:26.598956 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:11:26.599169 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m00:11:26.599319 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:11:26.599433 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:11:26.599556 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:11:26.599716 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:11:26.602277 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:11:26.602851 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:11:26.602991 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:11:26.603412 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:11:26.603534 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:11:26.603750 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:11:26.606580 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:26.606791 [debug] [MainThread]: On master: BEGIN
[0m00:11:26.606900 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:11:26.612692 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:11:26.612965 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:26.613133 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:11:26.615748 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:11:26.617106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123c67dd0>]}
[0m00:11:26.617328 [debug] [MainThread]: On master: ROLLBACK
[0m00:11:26.617709 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:26.617832 [debug] [MainThread]: On master: BEGIN
[0m00:11:26.618339 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:11:26.618461 [debug] [MainThread]: On master: COMMIT
[0m00:11:26.618579 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:26.618750 [debug] [MainThread]: On master: COMMIT
[0m00:11:26.619090 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:11:26.619233 [debug] [MainThread]: On master: Close
[0m00:11:26.621329 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m00:11:26.621513 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m00:11:26.621738 [info ] [Thread-1 (]: 1 of 13 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m00:11:26.622011 [info ] [Thread-2 (]: 2 of 13 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m00:11:26.622213 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_fxf_data)
[0m00:11:26.622357 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_pdl_data)
[0m00:11:26.622504 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m00:11:26.622633 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m00:11:26.626160 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m00:11:26.627409 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m00:11:26.627818 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m00:11:26.627981 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m00:11:26.643179 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m00:11:26.644861 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m00:11:26.645404 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.645681 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.645852 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m00:11:26.646006 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m00:11:26.646148 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:26.646310 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:26.655345 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m00:11:26.655546 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m00:11:26.655725 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.655880 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.656052 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m00:11:26.656213 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m00:11:26.707114 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.051 seconds
[0m00:11:26.712857 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.056 seconds
[0m00:11:26.714137 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.715768 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.715959 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m00:11:26.716133 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m00:11:26.716926 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:11:26.717105 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:11:26.718447 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.719792 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.719988 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m00:11:26.720163 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m00:11:26.720959 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:11:26.727411 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m00:11:26.728425 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m00:11:26.729166 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m00:11:26.729360 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.729527 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.729688 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m00:11:26.729838 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m00:11:26.738599 [debug] [Thread-2 (]: SQL status: COMMIT in 0.009 seconds
[0m00:11:26.741834 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m00:11:26.742047 [debug] [Thread-1 (]: SQL status: COMMIT in 0.012 seconds
[0m00:11:26.744880 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m00:11:26.746066 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m00:11:26.746264 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m00:11:26.746586 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m00:11:26.746785 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m00:11:26.749136 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:11:26.750254 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m00:11:26.750416 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m00:11:26.751149 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m00:11:26.752535 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12343c190>]}
[0m00:11:26.752774 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124fa40d0>]}
[0m00:11:26.753110 [info ] [Thread-2 (]: 2 of 13 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.13s]
[0m00:11:26.753369 [info ] [Thread-1 (]: 1 of 13 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.13s]
[0m00:11:26.753705 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m00:11:26.753937 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m00:11:26.754314 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_pdl_data
[0m00:11:26.754583 [info ] [Thread-4 (]: 3 of 13 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m00:11:26.754752 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_company_profiling
[0m00:11:26.754904 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_data_profiling
[0m00:11:26.755069 [debug] [Thread-3 (]: Began running node model.dbt_service.raw_location_profiling
[0m00:11:26.755285 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_pdl_data)
[0m00:11:26.755512 [info ] [Thread-2 (]: 4 of 13 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m00:11:26.755741 [info ] [Thread-1 (]: 5 of 13 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m00:11:26.755955 [info ] [Thread-3 (]: 6 of 13 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m00:11:26.756127 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m00:11:26.756296 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_pdl_data, now model.dbt_service.raw_company_profiling)
[0m00:11:26.756465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_fxf_data, now model.dbt_service.raw_data_profiling)
[0m00:11:26.756633 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_location_profiling)
[0m00:11:26.758380 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m00:11:26.758598 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m00:11:26.758778 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m00:11:26.758960 [debug] [Thread-3 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m00:11:26.760664 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m00:11:26.762022 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m00:11:26.763809 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m00:11:26.764019 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_pdl_data
[0m00:11:26.772805 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m00:11:26.773059 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_company_profiling
[0m00:11:26.773423 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_data_profiling
[0m00:11:26.773607 [debug] [Thread-3 (]: Began executing node model.dbt_service.raw_location_profiling
[0m00:11:26.775256 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m00:11:26.776697 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m00:11:26.776898 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:11:26.778243 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m00:11:26.778503 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m00:11:26.778661 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.778863 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:26.779018 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.779247 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m00:11:26.779470 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.779781 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m00:11:26.779959 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:26.780116 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m00:11:26.780270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:26.780490 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:11:26.790016 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m00:11:26.790205 [debug] [Thread-3 (]: SQL status: BEGIN in 0.010 seconds
[0m00:11:26.790341 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m00:11:26.790493 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m00:11:26.790670 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.790840 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.790998 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:11:26.791154 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.791375 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m00:11:26.791730 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m00:11:26.792046 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m00:11:26.792294 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m00:11:26.793768 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:11:26.795309 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:11:26.795463 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m00:11:26.796046 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.796664 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:11:26.796846 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:11:26.796977 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m00:11:26.797667 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:26.798900 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m00:11:26.800262 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m00:11:26.800430 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m00:11:26.800958 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:11:26.801559 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: Close
[0m00:11:26.801828 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124eebb50>]}
[0m00:11:26.802102 [info ] [Thread-4 (]: 3 of 13 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m00:11:26.802331 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_pdl_data
[0m00:11:26.802505 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_fxf_data
[0m00:11:26.802723 [info ] [Thread-4 (]: 7 of 13 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m00:11:26.802897 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_pdl_data, now model.dbt_service.stg_fxf_data)
[0m00:11:26.803036 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m00:11:26.804410 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m00:11:26.804745 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_fxf_data
[0m00:11:26.806158 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m00:11:26.806447 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:11:26.806583 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m00:11:26.806711 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:26.812176 [debug] [Thread-4 (]: SQL status: BEGIN in 0.005 seconds
[0m00:11:26.812406 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:11:26.812593 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m00:11:26.813890 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:11:26.815418 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:11:26.815589 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m00:11:26.816090 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.816663 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:11:26.816817 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:11:26.816946 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m00:11:26.817636 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:26.819454 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m00:11:26.819796 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m00:11:26.819941 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m00:11:26.820387 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:11:26.820977 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: Close
[0m00:11:26.821247 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12580e750>]}
[0m00:11:26.821512 [info ] [Thread-4 (]: 7 of 13 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.02s]
[0m00:11:26.821740 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_fxf_data
[0m00:11:26.822084 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m00:11:26.822357 [info ] [Thread-4 (]: 8 of 13 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m00:11:26.822558 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_fxf_data, now model.dbt_service.company_analysis)
[0m00:11:26.822703 [debug] [Thread-4 (]: Began compiling node model.dbt_service.company_analysis
[0m00:11:26.824269 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m00:11:26.824578 [debug] [Thread-4 (]: Began executing node model.dbt_service.company_analysis
[0m00:11:26.826020 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m00:11:26.826318 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.826453 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: BEGIN
[0m00:11:26.826660 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:26.832380 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m00:11:26.832582 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.832786 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m00:11:26.881015 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.088 seconds
[0m00:11:26.882669 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.882845 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m00:11:26.883474 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.884822 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.885008 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.092 seconds
[0m00:11:26.885181 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m00:11:26.886542 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.886776 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m00:11:26.887021 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.887662 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m00:11:26.887816 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.887983 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:11:26.888127 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m00:11:26.889297 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.889489 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m00:11:26.889899 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.890047 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:26.890698 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m00:11:26.891822 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m00:11:26.891987 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.892303 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m00:11:26.892453 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m00:11:26.892611 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m00:11:26.893169 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m00:11:26.894082 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m00:11:26.894222 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:26.894508 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m00:11:26.895091 [debug] [Thread-2 (]: On model.dbt_service.raw_company_profiling: Close
[0m00:11:26.895265 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m00:11:26.895538 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124f06190>]}
[0m00:11:26.895810 [info ] [Thread-2 (]: 4 of 13 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.14s]
[0m00:11:26.896163 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_company_profiling
[0m00:11:26.896343 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:26.896539 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m00:11:26.897060 [debug] [Thread-1 (]: On model.dbt_service.raw_data_profiling: Close
[0m00:11:26.897327 [info ] [Thread-2 (]: 9 of 13 START sql table model public_marts.location_analysis ................... [RUN]
[0m00:11:26.897594 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_company_profiling, now model.dbt_service.location_analysis)
[0m00:11:26.897834 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m00:11:26.897967 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123c3b950>]}
[0m00:11:26.899534 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m00:11:26.899812 [info ] [Thread-1 (]: 5 of 13 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.14s]
[0m00:11:26.900075 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_data_profiling
[0m00:11:26.900242 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_company_profiling
[0m00:11:26.900457 [info ] [Thread-1 (]: 10 of 13 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m00:11:26.900629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_data_profiling, now model.dbt_service.staging_company_profiling)
[0m00:11:26.900784 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m00:11:26.900919 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m00:11:26.903264 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m00:11:26.904906 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m00:11:26.905322 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:26.905490 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_company_profiling
[0m00:11:26.905641 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m00:11:26.907138 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m00:11:26.907318 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:26.907981 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:26.908148 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m00:11:26.908277 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:26.913163 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m00:11:26.913375 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:26.913569 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources with structured location data



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location, city, state
),

state_summary as (
    select
        state,
        count(distinct location) as locations_in_state,
        count(distinct city) as cities_in_state,
        sum(contact_count) as total_contacts_in_state,
        sum(company_count) as total_companies_in_state,
        avg(avg_revenue) as avg_state_revenue
    from location_stats
    where state is not null
    group by state
)

select
    ls.location,
    ls.city,
    ls.state,
    ls.contact_count,
    ls.company_count,
    ls.unique_titles,
    ls.data_sources,
    round(ls.avg_revenue::numeric, 2) as avg_revenue,
    -- Add state-level context
    ss.total_contacts_in_state,
    ss.cities_in_state,
    round(100.0 * ls.contact_count / ss.total_contacts_in_state, 2) as pct_of_state_contacts
from location_stats ls
left join state_summary ss on ls.state = ss.state
order by ls.contact_count desc
  );
  
[0m00:11:26.913779 [debug] [Thread-1 (]: SQL status: BEGIN in 0.005 seconds
[0m00:11:26.913950 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:26.914232 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

company_quality_comparison as (
    select
        'quality_comparison' as source_table,
        'staging' as data_layer,
        company,
        sum(employee_count) as employee_count,
        sum(unique_employees) as unique_employees,
        sum(unique_titles) as unique_titles,
        sum(office_locations) as office_locations,
        sum(unique_cities) as unique_cities,
        sum(unique_states) as unique_states,
        sum(employees_with_revenue_info) as employees_with_revenue_info,
        sum(employees_missing_revenue) as employees_missing_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        avg(name_completeness_pct) as name_completeness_pct,
        avg(email_completeness_pct) as email_completeness_pct,
        avg(title_completeness_pct) as title_completeness_pct,
        avg(location_completeness_pct) as location_completeness_pct,
        avg(city_completeness_pct) as city_completeness_pct,
        avg(state_completeness_pct) as state_completeness_pct,
        sum(valid_email_count) as valid_email_count,
        avg(email_validity_pct) as email_validity_pct,
        sum(email_domain_count) as email_domain_count
    from (
        select * from fxf_company_profile
        union all
        select * from pdl_company_profile
    ) combined
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
union all  
select * from company_quality_comparison
order by employee_count desc
  );
  
[0m00:11:26.957174 [debug] [Thread-3 (]: SQL status: SELECT 152 in 0.165 seconds
[0m00:11:26.958728 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.958898 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m00:11:26.959500 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.960795 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.960958 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m00:11:26.961435 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.962099 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m00:11:26.962249 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.962381 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m00:11:26.962964 [debug] [Thread-3 (]: SQL status: COMMIT in 0.000 seconds
[0m00:11:26.963905 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m00:11:26.964180 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m00:11:26.964316 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m00:11:26.964952 [debug] [Thread-4 (]: SQL status: SELECT 91 in 0.132 seconds
[0m00:11:26.966322 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.966498 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:11:26.966707 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m00:11:26.967238 [debug] [Thread-3 (]: On model.dbt_service.raw_location_profiling: Close
[0m00:11:26.967566 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124f8eed0>]}
[0m00:11:26.967787 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.968010 [info ] [Thread-3 (]: 6 of 13 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.21s]
[0m00:11:26.969278 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.969507 [debug] [Thread-3 (]: Finished running node model.dbt_service.raw_location_profiling
[0m00:11:26.969697 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m00:11:26.969910 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_data_profiling
[0m00:11:26.970200 [info ] [Thread-3 (]: 11 of 13 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m00:11:26.970408 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_location_profiling, now model.dbt_service.staging_data_profiling)
[0m00:11:26.970566 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:26.970716 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m00:11:26.971359 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:11:26.973053 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m00:11:26.973232 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.973430 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:11:26.973906 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_data_profiling
[0m00:11:26.975585 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m00:11:26.975763 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m00:11:26.977686 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m00:11:26.977990 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:11:26.978138 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m00:11:26.978441 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:26.978590 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m00:11:26.978726 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:11:26.979144 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:26.979681 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: Close
[0m00:11:26.979985 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1258a8250>]}
[0m00:11:26.980334 [info ] [Thread-4 (]: 8 of 13 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.16s]
[0m00:11:26.980584 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m00:11:26.980756 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_location_profiling
[0m00:11:26.981047 [info ] [Thread-4 (]: 12 of 13 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m00:11:26.981302 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.company_analysis, now model.dbt_service.staging_location_profiling)
[0m00:11:26.981463 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m00:11:26.983079 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m00:11:26.983655 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_location_profiling
[0m00:11:26.985140 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m00:11:26.985524 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:26.985692 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m00:11:26.985861 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m00:11:26.986042 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:26.986215 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:26.986562 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

data_quality_comparison as (
    select
        'data_quality_summary' as source_table,
        'staging' as data_layer,
        (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile) as total_records,
        0 as unique_ids,
        0 as duplicate_ids,
        0 as non_null_names,
        0 as null_names,
        (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) as non_null_emails,
        (select sum(null_emails) from fxf_staging_profile union all select sum(null_emails) from pdl_staging_profile) as null_emails,
        0 as non_null_companies,
        0 as null_companies,
        0 as non_null_revenues,
        0 as null_revenues,
        0 as non_null_titles,
        0 as null_titles,
        0 as non_null_locations,
        0 as null_locations,
        0 as unique_companies,
        0 as unique_locations,
        0 as unique_titles,
        round(100.0 * (select sum(non_null_emails) from fxf_staging_profile union all select sum(non_null_emails) from pdl_staging_profile) / 
              (select sum(total_records) from fxf_staging_profile union all select sum(total_records) from pdl_staging_profile), 2) as email_completeness_pct,
        0 as revenue_completeness_pct,
        0 as avg_company_revenue,
        0 as max_company_revenue,
        0 as min_company_revenue
    limit 1
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
  );
  
[0m00:11:26.991606 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m00:11:26.991844 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:26.992117 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m00:11:27.051932 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.138 seconds
[0m00:11:27.053491 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:27.053650 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m00:11:27.054180 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.055257 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:27.055403 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m00:11:27.055822 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.056368 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:11:27.056507 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:27.056645 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:11:27.057299 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:27.058207 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m00:11:27.058484 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:11:27.058619 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m00:11:27.059504 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:27.060049 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m00:11:27.060300 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b89e90>]}
[0m00:11:27.060567 [info ] [Thread-2 (]: 9 of 13 OK created sql table model public_marts.location_analysis .............. [[32mSELECT 76[0m in 0.16s]
[0m00:11:27.060843 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m00:11:27.061085 [debug] [Thread-2 (]: Began running node model.dbt_service.data_overview
[0m00:11:27.061317 [info ] [Thread-2 (]: 13 of 13 START sql view model public_marts.data_overview ....................... [RUN]
[0m00:11:27.061566 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.location_analysis, now model.dbt_service.data_overview)
[0m00:11:27.061719 [debug] [Thread-2 (]: Began compiling node model.dbt_service.data_overview
[0m00:11:27.063426 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m00:11:27.063974 [debug] [Thread-2 (]: Began executing node model.dbt_service.data_overview
[0m00:11:27.065728 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m00:11:27.066049 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:11:27.066187 [debug] [Thread-2 (]: On model.dbt_service.data_overview: BEGIN
[0m00:11:27.066313 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:27.071574 [debug] [Thread-2 (]: SQL status: BEGIN in 0.005 seconds
[0m00:11:27.071771 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:11:27.071956 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m00:11:27.073286 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:11:27.074700 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:11:27.074887 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m00:11:27.075539 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.076217 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m00:11:27.076410 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:11:27.076575 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m00:11:27.077122 [debug] [Thread-2 (]: SQL status: COMMIT in 0.000 seconds
[0m00:11:27.078159 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m00:11:27.078466 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:11:27.078606 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m00:11:27.079029 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:11:27.079558 [debug] [Thread-2 (]: On model.dbt_service.data_overview: Close
[0m00:11:27.079801 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123e8ee50>]}
[0m00:11:27.080072 [info ] [Thread-2 (]: 13 of 13 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.02s]
[0m00:11:27.080301 [debug] [Thread-2 (]: Finished running node model.dbt_service.data_overview
[0m00:11:27.202718 [debug] [Thread-1 (]: SQL status: SELECT 271 in 0.288 seconds
[0m00:11:27.204881 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:27.205050 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m00:11:27.205700 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:11:27.207718 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:27.207870 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m00:11:27.208347 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.208919 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:11:27.209064 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:27.209197 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:11:27.210036 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:27.211107 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m00:11:27.211408 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:11:27.211557 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m00:11:27.212489 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:27.213011 [debug] [Thread-1 (]: On model.dbt_service.staging_company_profiling: Close
[0m00:11:27.213257 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1258d3810>]}
[0m00:11:27.213517 [info ] [Thread-1 (]: 10 of 13 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 271[0m in 0.31s]
[0m00:11:27.213742 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_company_profiling
[0m00:11:27.214473 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.228 seconds
[0m00:11:27.216260 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:27.216434 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m00:11:27.216981 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.218149 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:27.218288 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m00:11:27.218659 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.219201 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:11:27.219469 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:27.219655 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:11:27.220384 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:27.221316 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m00:11:27.221646 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:11:27.221789 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m00:11:27.222646 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:27.223122 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: Close
[0m00:11:27.223388 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123c72250>]}
[0m00:11:27.223645 [info ] [Thread-3 (]: 11 of 13 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.25s]
[0m00:11:27.223859 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_data_profiling
[0m00:11:27.256108 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.264 seconds
[0m00:11:27.258427 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:27.258607 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m00:11:27.259229 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.260425 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:27.260810 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m00:11:27.261392 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:11:27.262000 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:11:27.262206 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:27.262477 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:11:27.263288 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:11:27.264220 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m00:11:27.264580 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:11:27.264868 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m00:11:27.265986 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:11:27.266443 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: Close
[0m00:11:27.266681 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07672d6e-e9c6-4504-869f-966a84637a81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124fd6750>]}
[0m00:11:27.266950 [info ] [Thread-4 (]: 12 of 13 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.29s]
[0m00:11:27.267173 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_location_profiling
[0m00:11:27.267747 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:27.267882 [debug] [MainThread]: On master: BEGIN
[0m00:11:27.267994 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:11:27.275028 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m00:11:27.275195 [debug] [MainThread]: On master: COMMIT
[0m00:11:27.275308 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:27.275415 [debug] [MainThread]: On master: COMMIT
[0m00:11:27.275790 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:11:27.275915 [debug] [MainThread]: On master: Close
[0m00:11:27.276081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:11:27.276197 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m00:11:27.276299 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m00:11:27.276397 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m00:11:27.276491 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m00:11:27.276665 [info ] [MainThread]: 
[0m00:11:27.276834 [info ] [MainThread]: Finished running 10 table models, 3 view models in 0 hours 0 minutes and 0.80 seconds (0.80s).
[0m00:11:27.277845 [debug] [MainThread]: Command end result
[0m00:11:27.332943 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:11:27.333964 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:11:27.337299 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:11:27.337472 [info ] [MainThread]: 
[0m00:11:27.337672 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:11:27.337825 [info ] [MainThread]: 
[0m00:11:27.337976 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m00:11:27.340357 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.4823284, "process_in_blocks": "0", "process_kernel_time": 0.270655, "process_mem_max_rss": "143556608", "process_out_blocks": "0", "process_user_time": 1.490195}
[0m00:11:27.340630 [debug] [MainThread]: Command `dbt run` succeeded at 00:11:27.340588 after 1.48 seconds
[0m00:11:27.340831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104de8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb2910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e1f690>]}
[0m00:11:27.341009 [debug] [MainThread]: Flushing usage events
[0m00:11:27.756519 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:11:35.290461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e22050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e2f650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e2fd50>]}


============================== 00:11:35.292671 | 05e63070-3b47-4396-ab3d-5f4f2b3be2c5 ==============================
[0m00:11:35.292671 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:11:35.293024 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'introspect': 'True', 'invocation_command': 'dbt test', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'quiet': 'False', 'fail_fast': 'False', 'debug': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'cache_selected_only': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'static_parser': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'printer_width': '80', 'empty': 'None'}
[0m00:11:35.379322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f2b750>]}
[0m00:11:35.409931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cf2210>]}
[0m00:11:35.410445 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:11:35.454485 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:11:35.513081 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:11:35.513405 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:11:35.538331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cfc250>]}
[0m00:11:35.582140 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:11:35.583258 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:11:35.595067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a3f10>]}
[0m00:11:35.595362 [info ] [MainThread]: Found 13 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:11:35.595537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111be2890>]}
[0m00:11:35.596680 [info ] [MainThread]: 
[0m00:11:35.596843 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:11:35.596957 [info ] [MainThread]: 
[0m00:11:35.597192 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:11:35.599247 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m00:11:35.599495 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m00:11:35.599788 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:11:35.600905 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:11:35.652216 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:11:35.652430 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:11:35.652618 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:11:35.652752 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:11:35.652895 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:11:35.653032 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:11:35.653150 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:11:35.653265 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:11:35.653384 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:35.653497 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:35.653602 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:35.653723 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:11:35.680346 [debug] [ThreadPool]: SQL status: BEGIN in 0.027 seconds
[0m00:11:35.680534 [debug] [ThreadPool]: SQL status: BEGIN in 0.027 seconds
[0m00:11:35.680709 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:11:35.680844 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:11:35.680976 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:11:35.681119 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:11:35.683537 [debug] [ThreadPool]: SQL status: BEGIN in 0.030 seconds
[0m00:11:35.683654 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:11:35.683773 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:11:35.683918 [debug] [ThreadPool]: SQL status: BEGIN in 0.031 seconds
[0m00:11:35.684011 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:11:35.684124 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:11:35.684668 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:11:35.684797 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:11:35.685376 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:11:35.685841 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:11:35.686422 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:11:35.686577 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:11:35.686704 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:11:35.686821 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m00:11:35.687265 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:11:35.687498 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public)
[0m00:11:35.687943 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_staging_analysis)
[0m00:11:35.688323 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:11:35.689409 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:11:35.689520 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:11:35.690301 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:11:35.690460 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:11:35.690772 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:11:35.690890 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:11:35.690989 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:35.691097 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:11:35.701920 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m00:11:35.702146 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m00:11:35.702392 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:11:35.702512 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:11:35.702666 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:11:35.702846 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:11:35.704637 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:11:35.705348 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:11:35.705516 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m00:11:35.705920 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:11:35.706043 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:11:35.706682 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:11:35.709256 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.709409 [debug] [MainThread]: On master: BEGIN
[0m00:11:35.709533 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:11:35.715204 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:11:35.715370 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.715516 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:11:35.717691 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:11:35.719035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05e63070-3b47-4396-ab3d-5f4f2b3be2c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11299b550>]}
[0m00:11:35.719259 [debug] [MainThread]: On master: ROLLBACK
[0m00:11:35.719672 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.719781 [debug] [MainThread]: On master: BEGIN
[0m00:11:35.720292 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:11:35.720436 [debug] [MainThread]: On master: COMMIT
[0m00:11:35.720555 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.720673 [debug] [MainThread]: On master: COMMIT
[0m00:11:35.721047 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:11:35.721160 [debug] [MainThread]: On master: Close
[0m00:11:35.722521 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:11:35.722719 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:11:35.723039 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:11:35.723202 [debug] [Thread-4 (]: Began running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:11:35.722894 [info ] [Thread-1 (]: 1 of 13 START test not_null_company_analysis_company ........................... [RUN]
[0m00:11:35.723386 [info ] [Thread-2 (]: 2 of 13 START test not_null_location_analysis_location ......................... [RUN]
[0m00:11:35.723553 [info ] [Thread-3 (]: 3 of 13 START test not_null_raw_fxf_data_fxf_id ................................ [RUN]
[0m00:11:35.723720 [info ] [Thread-4 (]: 4 of 13 START test not_null_raw_pdl_data_pdl_id ................................ [RUN]
[0m00:11:35.723911 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now test.dbt_service.not_null_company_analysis_company.b4c562fde2)
[0m00:11:35.724065 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now test.dbt_service.not_null_location_analysis_location.8bf3dfa482)
[0m00:11:35.724201 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed)
[0m00:11:35.724346 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df)
[0m00:11:35.724477 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:11:35.724605 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:11:35.724730 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:11:35.724860 [debug] [Thread-4 (]: Began compiling node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:11:35.731603 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:11:35.733236 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:11:35.734837 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:11:35.736588 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:11:35.737235 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:11:35.743479 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:11:35.746061 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:11:35.747246 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:11:35.747489 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:11:35.747662 [debug] [Thread-4 (]: Began executing node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:11:35.748660 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:11:35.748858 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:11:35.749802 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:11:35.750042 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:11:35.750251 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: BEGIN
[0m00:11:35.750444 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:11:35.750631 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: BEGIN
[0m00:11:35.750791 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:35.750952 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: BEGIN
[0m00:11:35.751101 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:35.751252 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:11:35.751490 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:11:35.751690 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: BEGIN
[0m00:11:35.751887 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:35.767436 [debug] [Thread-3 (]: SQL status: BEGIN in 0.016 seconds
[0m00:11:35.767626 [debug] [Thread-2 (]: SQL status: BEGIN in 0.017 seconds
[0m00:11:35.767807 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m00:11:35.767970 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"
[0m00:11:35.768128 [debug] [Thread-4 (]: SQL status: BEGIN in 0.016 seconds
[0m00:11:35.768260 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"
[0m00:11:35.768407 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_company_analysis_company.b4c562fde2"
[0m00:11:35.768562 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.768710 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"
[0m00:11:35.768863 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_location_analysis_location.8bf3dfa482"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select location
from "finny_db"."public_marts"."location_analysis"
where location is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.769028 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_company_analysis_company.b4c562fde2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select company
from "finny_db"."public_staging_analysis"."company_analysis"
where company is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.769221 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.770925 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m00:11:35.771088 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.002 seconds
[0m00:11:35.772934 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: ROLLBACK
[0m00:11:35.773453 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: ROLLBACK
[0m00:11:35.774025 [debug] [Thread-2 (]: On test.dbt_service.not_null_location_analysis_location.8bf3dfa482: Close
[0m00:11:35.774245 [debug] [Thread-1 (]: On test.dbt_service.not_null_company_analysis_company.b4c562fde2: Close
[0m00:11:35.774846 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.006 seconds
[0m00:11:35.775013 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.006 seconds
[0m00:11:35.774648 [info ] [Thread-2 (]: 2 of 13 PASS not_null_location_analysis_location ............................... [[32mPASS[0m in 0.05s]
[0m00:11:35.775301 [info ] [Thread-1 (]: 1 of 13 PASS not_null_company_analysis_company ................................. [[32mPASS[0m in 0.05s]
[0m00:11:35.776134 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: ROLLBACK
[0m00:11:35.776601 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: ROLLBACK
[0m00:11:35.776818 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_location_analysis_location.8bf3dfa482
[0m00:11:35.777017 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_company_analysis_company.b4c562fde2
[0m00:11:35.777246 [debug] [Thread-2 (]: Began running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:11:35.777478 [debug] [Thread-1 (]: Began running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:11:35.777606 [debug] [Thread-3 (]: On test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed: Close
[0m00:11:35.777719 [debug] [Thread-4 (]: On test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df: Close
[0m00:11:35.777876 [info ] [Thread-2 (]: 5 of 13 START test not_null_stg_fxf_data_email ................................. [RUN]
[0m00:11:35.778056 [info ] [Thread-1 (]: 6 of 13 START test not_null_stg_fxf_data_fxf_id ................................ [RUN]
[0m00:11:35.778325 [info ] [Thread-3 (]: 3 of 13 PASS not_null_raw_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m00:11:35.778556 [info ] [Thread-4 (]: 4 of 13 PASS not_null_raw_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.05s]
[0m00:11:35.778734 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_location_analysis_location.8bf3dfa482, now test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1)
[0m00:11:35.778892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_company_analysis_company.b4c562fde2, now test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe)
[0m00:11:35.779102 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed
[0m00:11:35.779303 [debug] [Thread-4 (]: Finished running node test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df
[0m00:11:35.779437 [debug] [Thread-2 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:11:35.779573 [debug] [Thread-1 (]: Began compiling node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:11:35.779714 [debug] [Thread-3 (]: Began running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:11:35.779897 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:11:35.781937 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:11:35.783598 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:11:35.783758 [info ] [Thread-3 (]: 7 of 13 START test not_null_stg_pdl_data_pdl_id ................................ [RUN]
[0m00:11:35.783918 [info ] [Thread-4 (]: 8 of 13 START test unique_company_analysis_company ............................. [RUN]
[0m00:11:35.784351 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_fxf_data_fxf_id.a66d87cfed, now test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7)
[0m00:11:35.784660 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_raw_pdl_data_pdl_id.b8ed29d8df, now test.dbt_service.unique_company_analysis_company.2a96b2dba1)
[0m00:11:35.784869 [debug] [Thread-2 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:11:35.785006 [debug] [Thread-1 (]: Began executing node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:11:35.785144 [debug] [Thread-3 (]: Began compiling node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:11:35.785289 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:11:35.786538 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:11:35.787694 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:11:35.789629 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:11:35.792150 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:11:35.792655 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:11:35.792908 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:11:35.793070 [debug] [Thread-3 (]: Began executing node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:11:35.793248 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: BEGIN
[0m00:11:35.793402 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:11:35.793558 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: BEGIN
[0m00:11:35.794923 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:11:35.795102 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:35.797324 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:11:35.797525 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:35.798148 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:11:35.798354 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: BEGIN
[0m00:11:35.798545 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:11:35.798736 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:11:35.798905 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: BEGIN
[0m00:11:35.799114 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:35.814790 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m00:11:35.815038 [debug] [Thread-2 (]: SQL status: BEGIN in 0.020 seconds
[0m00:11:35.815239 [debug] [Thread-4 (]: SQL status: BEGIN in 0.016 seconds
[0m00:11:35.815436 [debug] [Thread-3 (]: SQL status: BEGIN in 0.017 seconds
[0m00:11:35.815603 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"
[0m00:11:35.815756 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"
[0m00:11:35.815920 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_company_analysis_company.2a96b2dba1"
[0m00:11:35.816074 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"
[0m00:11:35.816240 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select fxf_id
from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.816387 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select email
from "finny_db"."public_staging"."stg_fxf_data"
where email is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.816532 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_company_analysis_company.2a96b2dba1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    company as unique_field,
    count(*) as n_records

from "finny_db"."public_staging_analysis"."company_analysis"
where company is not null
group by company
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.816679 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pdl_id
from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is null



  
  
      
    ) dbt_internal_test
[0m00:11:35.817691 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.001 seconds
[0m00:11:35.818497 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: ROLLBACK
[0m00:11:35.818994 [debug] [Thread-4 (]: On test.dbt_service.unique_company_analysis_company.2a96b2dba1: Close
[0m00:11:35.819288 [info ] [Thread-4 (]: 8 of 13 PASS unique_company_analysis_company ................................... [[32mPASS[0m in 0.03s]
[0m00:11:35.819512 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_company_analysis_company.2a96b2dba1
[0m00:11:35.819673 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:11:35.819857 [info ] [Thread-4 (]: 9 of 13 START test unique_location_analysis_location ........................... [RUN]
[0m00:11:35.820056 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_company_analysis_company.2a96b2dba1, now test.dbt_service.unique_location_analysis_location.d8f9675ab7)
[0m00:11:35.820222 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m00:11:35.820399 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.003 seconds
[0m00:11:35.820558 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.004 seconds
[0m00:11:35.820685 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:11:35.821219 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: ROLLBACK
[0m00:11:35.821667 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: ROLLBACK
[0m00:11:35.822131 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: ROLLBACK
[0m00:11:35.824194 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:11:35.824736 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:11:35.824947 [debug] [Thread-3 (]: On test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7: Close
[0m00:11:35.825093 [debug] [Thread-2 (]: On test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1: Close
[0m00:11:35.825223 [debug] [Thread-1 (]: On test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe: Close
[0m00:11:35.826280 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:11:35.826585 [info ] [Thread-3 (]: 7 of 13 PASS not_null_stg_pdl_data_pdl_id ...................................... [[32mPASS[0m in 0.04s]
[0m00:11:35.826821 [info ] [Thread-2 (]: 5 of 13 PASS not_null_stg_fxf_data_email ....................................... [[32mPASS[0m in 0.05s]
[0m00:11:35.827089 [info ] [Thread-1 (]: 6 of 13 PASS not_null_stg_fxf_data_fxf_id ...................................... [[32mPASS[0m in 0.05s]
[0m00:11:35.827461 [debug] [Thread-3 (]: Finished running node test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7
[0m00:11:35.827737 [debug] [Thread-2 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1
[0m00:11:35.827984 [debug] [Thread-1 (]: Finished running node test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe
[0m00:11:35.828164 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:11:35.828345 [debug] [Thread-3 (]: Began running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:11:35.828545 [debug] [Thread-2 (]: Began running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:11:35.828742 [debug] [Thread-1 (]: Began running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:11:35.828902 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: BEGIN
[0m00:11:35.829068 [info ] [Thread-3 (]: 10 of 13 START test unique_raw_fxf_data_fxf_id ................................. [RUN]
[0m00:11:35.829248 [info ] [Thread-2 (]: 11 of 13 START test unique_raw_pdl_data_pdl_id ................................. [RUN]
[0m00:11:35.829428 [info ] [Thread-1 (]: 12 of 13 START test unique_stg_fxf_data_fxf_id ................................. [RUN]
[0m00:11:35.829588 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:35.829772 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_pdl_data_pdl_id.b9ad9908b7, now test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66)
[0m00:11:35.829949 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_email.547af3f2d1, now test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda)
[0m00:11:35.830109 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_service.not_null_stg_fxf_data_fxf_id.76da6d0fbe, now test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25)
[0m00:11:35.830316 [debug] [Thread-3 (]: Began compiling node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:11:35.830460 [debug] [Thread-2 (]: Began compiling node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:11:35.830606 [debug] [Thread-1 (]: Began compiling node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:11:35.832526 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:11:35.834081 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:11:35.835413 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:11:35.835869 [debug] [Thread-3 (]: Began executing node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:11:35.836110 [debug] [Thread-2 (]: Began executing node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:11:35.836329 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m00:11:35.837565 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:11:35.838718 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:11:35.838871 [debug] [Thread-1 (]: Began executing node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:11:35.839041 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_location_analysis_location.d8f9675ab7"
[0m00:11:35.840240 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:11:35.840466 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_location_analysis_location.d8f9675ab7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    location as unique_field,
    count(*) as n_records

from "finny_db"."public_marts"."location_analysis"
where location is not null
group by location
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.840701 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:11:35.840889 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:11:35.841154 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: BEGIN
[0m00:11:35.841323 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: BEGIN
[0m00:11:35.841510 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:11:35.841668 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:11:35.841808 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:11:35.841949 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: BEGIN
[0m00:11:35.842242 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:11:35.843190 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.002 seconds
[0m00:11:35.843947 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: ROLLBACK
[0m00:11:35.844448 [debug] [Thread-4 (]: On test.dbt_service.unique_location_analysis_location.d8f9675ab7: Close
[0m00:11:35.844720 [info ] [Thread-4 (]: 9 of 13 PASS unique_location_analysis_location ................................. [[32mPASS[0m in 0.02s]
[0m00:11:35.844947 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_location_analysis_location.d8f9675ab7
[0m00:11:35.845109 [debug] [Thread-4 (]: Began running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:11:35.845297 [info ] [Thread-4 (]: 13 of 13 START test unique_stg_pdl_data_pdl_id ................................. [RUN]
[0m00:11:35.845509 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.dbt_service.unique_location_analysis_location.d8f9675ab7, now test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa)
[0m00:11:35.845705 [debug] [Thread-4 (]: Began compiling node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:11:35.847425 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:11:35.847733 [debug] [Thread-4 (]: Began executing node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:11:35.849562 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:11:35.849825 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:11:35.849962 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: BEGIN
[0m00:11:35.850092 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:11:35.854208 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m00:11:35.854401 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m00:11:35.854588 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m00:11:35.854760 [debug] [Thread-1 (]: Using postgres connection "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"
[0m00:11:35.854930 [debug] [Thread-3 (]: Using postgres connection "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"
[0m00:11:35.855093 [debug] [Thread-2 (]: Using postgres connection "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"
[0m00:11:35.855277 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.855473 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    fxf_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_fxf_data"
where fxf_id is not null
group by fxf_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.855647 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_raw"."raw_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.857275 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m00:11:35.857436 [debug] [Thread-4 (]: Using postgres connection "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"
[0m00:11:35.857586 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pdl_id as unique_field,
    count(*) as n_records

from "finny_db"."public_staging"."stg_pdl_data"
where pdl_id is not null
group by pdl_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m00:11:35.867123 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.011 seconds
[0m00:11:35.867335 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.011 seconds
[0m00:11:35.867943 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: ROLLBACK
[0m00:11:35.868391 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: ROLLBACK
[0m00:11:35.868845 [debug] [Thread-2 (]: On test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda: Close
[0m00:11:35.868993 [debug] [Thread-3 (]: On test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66: Close
[0m00:11:35.869267 [info ] [Thread-2 (]: 11 of 13 PASS unique_raw_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.04s]
[0m00:11:35.869750 [debug] [Thread-2 (]: Finished running node test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda
[0m00:11:35.869543 [info ] [Thread-3 (]: 10 of 13 PASS unique_raw_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.04s]
[0m00:11:35.870070 [debug] [Thread-3 (]: Finished running node test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66
[0m00:11:35.897318 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.041 seconds
[0m00:11:35.898033 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: ROLLBACK
[0m00:11:35.898469 [debug] [Thread-1 (]: On test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25: Close
[0m00:11:35.898748 [info ] [Thread-1 (]: 12 of 13 PASS unique_stg_fxf_data_fxf_id ....................................... [[32mPASS[0m in 0.07s]
[0m00:11:35.899009 [debug] [Thread-1 (]: Finished running node test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25
[0m00:11:35.909379 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.052 seconds
[0m00:11:35.910072 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: ROLLBACK
[0m00:11:35.910551 [debug] [Thread-4 (]: On test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa: Close
[0m00:11:35.910850 [info ] [Thread-4 (]: 13 of 13 PASS unique_stg_pdl_data_pdl_id ....................................... [[32mPASS[0m in 0.07s]
[0m00:11:35.911136 [debug] [Thread-4 (]: Finished running node test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa
[0m00:11:35.911854 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.911982 [debug] [MainThread]: On master: BEGIN
[0m00:11:35.912082 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:11:35.918375 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:11:35.918602 [debug] [MainThread]: On master: COMMIT
[0m00:11:35.918735 [debug] [MainThread]: Using postgres connection "master"
[0m00:11:35.918845 [debug] [MainThread]: On master: COMMIT
[0m00:11:35.919197 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:11:35.919345 [debug] [MainThread]: On master: Close
[0m00:11:35.919556 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:11:35.919691 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_fxf_data_fxf_id.b81bfb9c25' was properly closed.
[0m00:11:35.919803 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_pdl_data_pdl_id.c28d33aeda' was properly closed.
[0m00:11:35.919904 [debug] [MainThread]: Connection 'test.dbt_service.unique_raw_fxf_data_fxf_id.db3ff52e66' was properly closed.
[0m00:11:35.920004 [debug] [MainThread]: Connection 'test.dbt_service.unique_stg_pdl_data_pdl_id.0d192d34aa' was properly closed.
[0m00:11:35.920149 [info ] [MainThread]: 
[0m00:11:35.920301 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m00:11:35.921147 [debug] [MainThread]: Command end result
[0m00:11:35.933202 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:11:35.934264 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:11:35.937489 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:11:35.937649 [info ] [MainThread]: 
[0m00:11:35.937799 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:11:35.937921 [info ] [MainThread]: 
[0m00:11:35.938043 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m00:11:35.939477 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6838791, "process_in_blocks": "0", "process_kernel_time": 0.201791, "process_mem_max_rss": "141574144", "process_out_blocks": "0", "process_user_time": 1.152637}
[0m00:11:35.939827 [debug] [MainThread]: Command `dbt test` succeeded at 00:11:35.939784 after 0.68 seconds
[0m00:11:35.940004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e5bf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028d7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029aa7d0>]}
[0m00:11:35.940209 [debug] [MainThread]: Flushing usage events
[0m00:11:36.160696 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:20:53.747973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107062550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070e3690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070e3dd0>]}


============================== 00:20:53.750750 | 2d849c67-4fcc-4c5f-b9bb-55e84596ea1a ==============================
[0m00:20:53.750750 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:20:53.751045 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select staging_analysis+', 'empty': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'no_print': 'None', 'use_colors': 'True', 'debug': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'log_format': 'default', 'partial_parse': 'True', 'printer_width': '80', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'quiet': 'False', 'introspect': 'True', 'target_path': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m00:20:53.849225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071337d0>]}
[0m00:20:53.878875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046aa050>]}
[0m00:20:53.879661 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:20:53.926635 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:20:53.991618 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:20:53.991972 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_data_profiling.sql
[0m00:20:53.992154 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_company_profiling.sql
[0m00:20:54.120076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10769f3d0>]}
[0m00:20:54.184770 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:20:54.186064 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:20:54.197390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107776610>]}
[0m00:20:54.197640 [info ] [MainThread]: Found 13 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:20:54.197826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ef1410>]}
[0m00:20:54.198647 [info ] [MainThread]: 
[0m00:20:54.198804 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:20:54.198922 [info ] [MainThread]: 
[0m00:20:54.199123 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:20:54.200725 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:20:54.200978 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:20:54.227731 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:20:54.227923 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:20:54.228112 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:20:54.228254 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:20:54.228394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:54.228522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:54.263547 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m00:20:54.263747 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.035 seconds
[0m00:20:54.264345 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:20:54.264794 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:20:54.265751 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m00:20:54.265933 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m00:20:54.268321 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:20:54.268502 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:20:54.268688 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m00:20:54.269535 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:20:54.269672 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:20:54.270751 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:20:54.271466 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:20:54.271588 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:20:54.271709 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:20:54.271833 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:20:54.271955 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:20:54.272073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:20:54.272247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:54.272360 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:54.288919 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m00:20:54.289123 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m00:20:54.289294 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m00:20:54.289423 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m00:20:54.289534 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:20:54.289667 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:20:54.289781 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:20:54.289895 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:20:54.290037 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:20:54.290188 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:20:54.290352 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:20:54.290511 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:20:54.295519 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m00:20:54.295707 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m00:20:54.296345 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:20:54.296485 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.006 seconds
[0m00:20:54.296878 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:20:54.297356 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:20:54.297505 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.007 seconds
[0m00:20:54.297664 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:20:54.298348 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:20:54.298564 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:20:54.298739 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:20:54.299138 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging_analysis)
[0m00:20:54.299612 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:20:54.299809 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_marts)
[0m00:20:54.301747 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:20:54.312813 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:20:54.316226 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:20:54.318033 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:20:54.318199 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:20:54.318327 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:20:54.330434 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:20:54.330598 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:20:54.330735 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:20:54.333201 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:20:54.333336 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:20:54.333464 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:20:54.334022 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m00:20:54.334618 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:20:54.334854 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m00:20:54.334982 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:20:54.335485 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:20:54.335966 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:20:54.338759 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.338904 [debug] [MainThread]: On master: BEGIN
[0m00:20:54.339011 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:20:54.345141 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:20:54.345319 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.345501 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:20:54.347305 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:20:54.348599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ebe750>]}
[0m00:20:54.348821 [debug] [MainThread]: On master: ROLLBACK
[0m00:20:54.349805 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.349910 [debug] [MainThread]: On master: BEGIN
[0m00:20:54.351441 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m00:20:54.351549 [debug] [MainThread]: On master: COMMIT
[0m00:20:54.351652 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.351753 [debug] [MainThread]: On master: COMMIT
[0m00:20:54.352301 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:20:54.352426 [debug] [MainThread]: On master: Close
[0m00:20:54.354245 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m00:20:54.354424 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_company_profiling
[0m00:20:54.354595 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_data_profiling
[0m00:20:54.354944 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_location_profiling
[0m00:20:54.354807 [info ] [Thread-1 (]: 1 of 5 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m00:20:54.355191 [info ] [Thread-2 (]: 2 of 5 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m00:20:54.355390 [info ] [Thread-3 (]: 3 of 5 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m00:20:54.355598 [info ] [Thread-4 (]: 4 of 5 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m00:20:54.355803 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.company_analysis)
[0m00:20:54.355959 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.staging_company_profiling)
[0m00:20:54.356193 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.staging_data_profiling)
[0m00:20:54.356363 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.staging_location_profiling)
[0m00:20:54.356517 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m00:20:54.356666 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m00:20:54.356811 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m00:20:54.356953 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m00:20:54.360387 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m00:20:54.361969 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m00:20:54.363335 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m00:20:54.364597 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m00:20:54.365046 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m00:20:54.365228 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_company_profiling
[0m00:20:54.371692 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_data_profiling
[0m00:20:54.378150 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_location_profiling
[0m00:20:54.381202 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m00:20:54.382754 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m00:20:54.384139 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m00:20:54.385444 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m00:20:54.385881 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.386093 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.386276 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:20:54.386474 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m00:20:54.386637 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m00:20:54.386805 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.386956 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m00:20:54.387103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:20:54.387249 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:20:54.387399 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m00:20:54.387537 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:20:54.387819 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:20:54.405807 [debug] [Thread-4 (]: SQL status: BEGIN in 0.018 seconds
[0m00:20:54.405996 [debug] [Thread-3 (]: SQL status: BEGIN in 0.019 seconds
[0m00:20:54.406130 [debug] [Thread-1 (]: SQL status: BEGIN in 0.019 seconds
[0m00:20:54.406247 [debug] [Thread-2 (]: SQL status: BEGIN in 0.019 seconds
[0m00:20:54.406398 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.406528 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.406666 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.406796 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:20:54.407008 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m00:20:54.407297 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m00:20:54.407562 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m00:20:54.407840 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
),

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m00:20:54.409601 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 81: select * from fxf_company_profile
         ^

[0m00:20:54.409753 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: ROLLBACK
[0m00:20:54.412413 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: Close
[0m00:20:54.417938 [debug] [Thread-2 (]: Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  syntax error at or near "select"
  LINE 81: select * from fxf_company_profile
           ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m00:20:54.418897 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085aa3d0>]}
[0m00:20:54.419203 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model public_staging_analysis.staging_company_profiling  [[31mERROR[0m in 0.06s]
[0m00:20:54.419446 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_company_profiling
[0m00:20:54.419659 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.staging_company_profiling' to be skipped because of status 'error'.  Reason: Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  syntax error at or near "select"
  LINE 81: select * from fxf_company_profile
           ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql.
[0m00:20:54.545783 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.138 seconds
[0m00:20:54.550749 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.550950 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m00:20:54.551587 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.552781 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.552931 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m00:20:54.553339 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.560015 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:20:54.560265 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.152 seconds
[0m00:20:54.560447 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.561811 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.561971 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:20:54.562148 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m00:20:54.562722 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.563857 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.564014 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m00:20:54.564170 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m00:20:54.567696 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m00:20:54.569774 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:20:54.569959 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:20:54.570155 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m00:20:54.570805 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:20:54.571002 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.571144 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:20:54.571760 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m00:20:54.572783 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m00:20:54.572935 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:20:54.573238 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:20:54.574225 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: Close
[0m00:20:54.574411 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m00:20:54.574744 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108715310>]}
[0m00:20:54.575087 [info ] [Thread-3 (]: 3 of 5 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.22s]
[0m00:20:54.575398 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_data_profiling
[0m00:20:54.575657 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:20:54.576230 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m00:20:54.576500 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10850cad0>]}
[0m00:20:54.576760 [info ] [Thread-1 (]: 1 of 5 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.22s]
[0m00:20:54.576982 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m00:20:54.577241 [debug] [Thread-2 (]: Began running node model.dbt_service.data_overview
[0m00:20:54.577435 [info ] [Thread-2 (]: 5 of 5 START sql view model public_marts.data_overview ......................... [RUN]
[0m00:20:54.577614 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_company_profiling, now model.dbt_service.data_overview)
[0m00:20:54.577761 [debug] [Thread-2 (]: Began compiling node model.dbt_service.data_overview
[0m00:20:54.579518 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m00:20:54.579864 [debug] [Thread-2 (]: Began executing node model.dbt_service.data_overview
[0m00:20:54.587432 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m00:20:54.588019 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:20:54.588169 [debug] [Thread-2 (]: On model.dbt_service.data_overview: BEGIN
[0m00:20:54.588297 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:20:54.593708 [debug] [Thread-2 (]: SQL status: BEGIN in 0.005 seconds
[0m00:20:54.593928 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:20:54.594108 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m00:20:54.595505 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:20:54.596885 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:20:54.597073 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m00:20:54.597532 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.598119 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m00:20:54.598277 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:20:54.598405 [debug] [Thread-2 (]: On model.dbt_service.data_overview: COMMIT
[0m00:20:54.599148 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:20:54.600149 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m00:20:54.601437 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:20:54.601588 [debug] [Thread-2 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m00:20:54.602023 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:20:54.602507 [debug] [Thread-2 (]: On model.dbt_service.data_overview: Close
[0m00:20:54.602792 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086998d0>]}
[0m00:20:54.603138 [info ] [Thread-2 (]: 5 of 5 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m00:20:54.603402 [debug] [Thread-2 (]: Finished running node model.dbt_service.data_overview
[0m00:20:54.691189 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.283 seconds
[0m00:20:54.693656 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.693873 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m00:20:54.694465 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.695575 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.695709 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m00:20:54.696198 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:20:54.696822 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:20:54.697056 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.697224 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:20:54.698711 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:20:54.699646 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m00:20:54.699942 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:20:54.700090 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m00:20:54.701334 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:20:54.701804 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: Close
[0m00:20:54.702135 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d849c67-4fcc-4c5f-b9bb-55e84596ea1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877d110>]}
[0m00:20:54.702391 [info ] [Thread-4 (]: 4 of 5 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.35s]
[0m00:20:54.702620 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_location_profiling
[0m00:20:54.703279 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.703457 [debug] [MainThread]: On master: BEGIN
[0m00:20:54.703582 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:20:54.710821 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m00:20:54.710993 [debug] [MainThread]: On master: COMMIT
[0m00:20:54.711109 [debug] [MainThread]: Using postgres connection "master"
[0m00:20:54.711213 [debug] [MainThread]: On master: COMMIT
[0m00:20:54.711616 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:20:54.711764 [debug] [MainThread]: On master: Close
[0m00:20:54.711950 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:20:54.712072 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m00:20:54.712172 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m00:20:54.712271 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m00:20:54.712369 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m00:20:54.712531 [info ] [MainThread]: 
[0m00:20:54.712679 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 0.51 seconds (0.51s).
[0m00:20:54.713136 [debug] [MainThread]: Command end result
[0m00:20:54.729775 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:20:54.730944 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:20:54.734341 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:20:54.734565 [info ] [MainThread]: 
[0m00:20:54.734765 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m00:20:54.734906 [info ] [MainThread]: 
[0m00:20:54.735098 [error] [MainThread]: [31mFailure in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)[0m
[0m00:20:54.735333 [error] [MainThread]:   Database Error in model staging_company_profiling (models/staging_analysis/staging_company_profiling.sql)
  syntax error at or near "select"
  LINE 81: select * from fxf_company_profile
           ^
  compiled code at target/run/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m00:20:54.735525 [info ] [MainThread]: 
[0m00:20:54.735713 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging_analysis/staging_company_profiling.sql
[0m00:20:54.735869 [info ] [MainThread]: 
[0m00:20:54.736041 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=5
[0m00:20:54.738606 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0293558, "process_in_blocks": "0", "process_kernel_time": 0.219629, "process_mem_max_rss": "142114816", "process_out_blocks": "0", "process_user_time": 1.184551}
[0m00:20:54.738919 [debug] [MainThread]: Command `dbt run` failed at 00:20:54.738872 after 1.03 seconds
[0m00:20:54.739155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10277c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10710fb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10710fa50>]}
[0m00:20:54.739366 [debug] [MainThread]: Flushing usage events
[0m00:20:55.092118 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:21:19.335135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108eb2ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f335d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f33d10>]}


============================== 00:21:19.337234 | 67873925-edbb-45df-a292-878798d29141 ==============================
[0m00:21:19.337234 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:21:19.337552 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'quiet': 'False', 'target_path': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt run --select staging_analysis+', 'partial_parse': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'no_print': 'None', 'indirect_selection': 'eager', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'introspect': 'True', 'empty': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m00:21:19.423787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a1c210>]}
[0m00:21:19.455641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054fa010>]}
[0m00:21:19.456238 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:21:19.501460 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:21:19.568669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:21:19.569097 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging_analysis/staging_company_profiling.sql
[0m00:21:19.699625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109432cd0>]}
[0m00:21:19.765252 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:21:19.766059 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:21:19.777220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10963cf90>]}
[0m00:21:19.777491 [info ] [MainThread]: Found 13 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:21:19.777645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f8cf90>]}
[0m00:21:19.778426 [info ] [MainThread]: 
[0m00:21:19.778572 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:21:19.778684 [info ] [MainThread]: 
[0m00:21:19.778881 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:21:19.780515 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:21:19.780740 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:21:19.802910 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:21:19.803084 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:21:19.803283 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:21:19.803418 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:21:19.803564 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:19.803716 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:19.829976 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.026 seconds
[0m00:21:19.830170 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.026 seconds
[0m00:21:19.830767 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:21:19.831187 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:21:19.832160 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m00:21:19.832365 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m00:21:19.834686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:21:19.834895 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:21:19.835111 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m00:21:19.835911 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:21:19.836086 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:21:19.837252 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:21:19.838002 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:21:19.838131 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:21:19.838245 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:21:19.838365 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:21:19.838482 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:21:19.838586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:21:19.838784 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:19.838921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:19.851558 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.851744 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.851870 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.851986 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:21:19.852092 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.852206 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:21:19.852319 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:21:19.852454 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:21:19.852581 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:21:19.852716 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:21:19.852866 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:21:19.853046 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:21:19.857508 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:21:19.857655 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:21:19.858203 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:21:19.858321 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m00:21:19.858721 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:21:19.859199 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:21:19.859345 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.006 seconds
[0m00:21:19.859495 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:21:19.859975 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:21:19.860106 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:21:19.860329 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_raw)
[0m00:21:19.860435 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:21:19.860967 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public)
[0m00:21:19.861797 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:21:19.862124 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:21:19.863213 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:21:19.863407 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:21:19.863764 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:21:19.863891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:21:19.864026 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:21:19.871564 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:21:19.871752 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:21:19.871903 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:21:19.872031 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:21:19.872172 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:21:19.872314 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:21:19.874750 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m00:21:19.874882 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:21:19.875407 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:21:19.875834 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:21:19.876709 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:21:19.876854 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:21:19.879775 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:19.879921 [debug] [MainThread]: On master: BEGIN
[0m00:21:19.880022 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:21:19.887116 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m00:21:19.887264 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:19.887429 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:21:19.889940 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:21:19.891181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a393490>]}
[0m00:21:19.891374 [debug] [MainThread]: On master: ROLLBACK
[0m00:21:19.891740 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:19.891878 [debug] [MainThread]: On master: BEGIN
[0m00:21:19.892409 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:21:19.892600 [debug] [MainThread]: On master: COMMIT
[0m00:21:19.892725 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:19.892836 [debug] [MainThread]: On master: COMMIT
[0m00:21:19.893192 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:21:19.893429 [debug] [MainThread]: On master: Close
[0m00:21:19.895329 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m00:21:19.895527 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_company_profiling
[0m00:21:19.895910 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_data_profiling
[0m00:21:19.895752 [info ] [Thread-1 (]: 1 of 5 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m00:21:19.896131 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_location_profiling
[0m00:21:19.896323 [info ] [Thread-2 (]: 2 of 5 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m00:21:19.896533 [info ] [Thread-3 (]: 3 of 5 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m00:21:19.896729 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.company_analysis)
[0m00:21:19.896930 [info ] [Thread-4 (]: 4 of 5 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m00:21:19.897104 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.staging_company_profiling)
[0m00:21:19.897346 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.staging_data_profiling)
[0m00:21:19.897526 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m00:21:19.897687 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.staging_location_profiling)
[0m00:21:19.897823 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m00:21:19.897952 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m00:21:19.901333 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m00:21:19.901544 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m00:21:19.903095 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m00:21:19.904454 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m00:21:19.905811 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m00:21:19.906127 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m00:21:19.906290 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_company_profiling
[0m00:21:19.906494 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_data_profiling
[0m00:21:19.922881 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m00:21:19.924452 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m00:21:19.924620 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_location_profiling
[0m00:21:19.925986 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m00:21:19.927548 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m00:21:19.927891 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:19.928080 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:19.928265 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m00:21:19.928420 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:19.928580 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m00:21:19.928737 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:21:19.928896 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:19.929104 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m00:21:19.929253 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:21:19.929496 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m00:21:19.929636 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:21:19.929829 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:21:19.941885 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.942092 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m00:21:19.942287 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.942503 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:19.942732 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m00:21:19.942914 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:19.943069 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:19.943330 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m00:21:19.943602 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:19.943860 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m00:21:19.944167 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m00:21:19.944527 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m00:21:20.080851 [debug] [Thread-3 (]: SQL status: SELECT 2 in 0.136 seconds
[0m00:21:20.085235 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:20.085419 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m00:21:20.086056 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.087411 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:20.087578 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m00:21:20.088151 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.094971 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:21:20.095173 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.150 seconds
[0m00:21:20.095373 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:20.096895 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:20.097110 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:21:20.097272 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m00:21:20.097894 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.099047 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:20.099216 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m00:21:20.099398 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m00:21:20.101936 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m00:21:20.104187 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:21:20.104393 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:21:20.104572 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m00:21:20.105398 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:21:20.105606 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:20.105750 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:21:20.106319 [debug] [Thread-1 (]: SQL status: COMMIT in 0.000 seconds
[0m00:21:20.108258 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m00:21:20.108435 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.003 seconds
[0m00:21:20.108886 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:21:20.110070 [debug] [Thread-3 (]: On model.dbt_service.staging_data_profiling: Close
[0m00:21:20.110261 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m00:21:20.111262 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a421e90>]}
[0m00:21:20.111707 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:21:20.111578 [info ] [Thread-3 (]: 3 of 5 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.21s]
[0m00:21:20.111902 [debug] [Thread-2 (]: SQL status: SELECT 180 in 0.168 seconds
[0m00:21:20.112417 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m00:21:20.112645 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_data_profiling
[0m00:21:20.114179 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:20.114593 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2f6990>]}
[0m00:21:20.114773 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m00:21:20.115080 [info ] [Thread-1 (]: 1 of 5 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.22s]
[0m00:21:20.115443 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m00:21:20.115711 [debug] [Thread-3 (]: Began running node model.dbt_service.data_overview
[0m00:21:20.115878 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:21:20.116095 [info ] [Thread-3 (]: 5 of 5 START sql view model public_marts.data_overview ......................... [RUN]
[0m00:21:20.117563 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:20.117892 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_data_profiling, now model.dbt_service.data_overview)
[0m00:21:20.118121 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m00:21:20.118364 [debug] [Thread-3 (]: Began compiling node model.dbt_service.data_overview
[0m00:21:20.120364 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m00:21:20.120595 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:21:20.121454 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:21:20.121737 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:20.121903 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:21:20.122073 [debug] [Thread-3 (]: Began executing node model.dbt_service.data_overview
[0m00:21:20.128843 [debug] [Thread-2 (]: SQL status: COMMIT in 0.007 seconds
[0m00:21:20.129901 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m00:21:20.131004 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m00:21:20.131400 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:21:20.131592 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m00:21:20.131813 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:21:20.131953 [debug] [Thread-3 (]: On model.dbt_service.data_overview: BEGIN
[0m00:21:20.132084 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:21:20.132758 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:21:20.133296 [debug] [Thread-2 (]: On model.dbt_service.staging_company_profiling: Close
[0m00:21:20.133563 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a30e710>]}
[0m00:21:20.133844 [info ] [Thread-2 (]: 2 of 5 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.24s]
[0m00:21:20.134097 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_company_profiling
[0m00:21:20.137694 [debug] [Thread-3 (]: SQL status: BEGIN in 0.006 seconds
[0m00:21:20.137883 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:21:20.138067 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview across all schemas
-- This model provides a summary of our data pipeline



select
    'raw_fxf_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'FXF contact data' as description
from "finny_db"."public_raw"."raw_fxf_data"

union all

select
    'raw_pdl_data' as table_name,
    'raw' as schema_name,
    count(*) as row_count,
    'PDL contact data' as description
from "finny_db"."public_raw"."raw_pdl_data"

union all

select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'company_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Company-level analysis' as description
from "finny_db"."public_staging_analysis"."company_analysis"

union all

select
    'location_analysis' as table_name,
    'marts' as schema_name,
    count(*) as row_count,
    'Location-based analysis' as description
from "finny_db"."public_marts"."location_analysis"
  );
[0m00:21:20.139358 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m00:21:20.140637 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:21:20.140784 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m00:21:20.141227 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.141871 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m00:21:20.142036 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:21:20.142164 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m00:21:20.142875 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m00:21:20.143933 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m00:21:20.145297 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:21:20.145454 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m00:21:20.145981 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:21:20.146483 [debug] [Thread-3 (]: On model.dbt_service.data_overview: Close
[0m00:21:20.146715 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3c7250>]}
[0m00:21:20.146965 [info ] [Thread-3 (]: 5 of 5 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.03s]
[0m00:21:20.147176 [debug] [Thread-3 (]: Finished running node model.dbt_service.data_overview
[0m00:21:20.227845 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.283 seconds
[0m00:21:20.230208 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:20.230452 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m00:21:20.231097 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.232460 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:20.232685 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m00:21:20.233205 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:21:20.233767 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:21:20.233900 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:20.234023 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:21:20.234720 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:21:20.237229 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m00:21:20.237679 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:21:20.237898 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m00:21:20.239200 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:21:20.239759 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: Close
[0m00:21:20.240131 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67873925-edbb-45df-a292-878798d29141', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4a6950>]}
[0m00:21:20.240450 [info ] [Thread-4 (]: 4 of 5 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.34s]
[0m00:21:20.240688 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_location_profiling
[0m00:21:20.241436 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:20.241673 [debug] [MainThread]: On master: BEGIN
[0m00:21:20.241782 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:21:20.248230 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:21:20.248495 [debug] [MainThread]: On master: COMMIT
[0m00:21:20.248620 [debug] [MainThread]: Using postgres connection "master"
[0m00:21:20.248737 [debug] [MainThread]: On master: COMMIT
[0m00:21:20.249051 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:21:20.249191 [debug] [MainThread]: On master: Close
[0m00:21:20.249397 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:21:20.249526 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m00:21:20.249626 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m00:21:20.249727 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m00:21:20.249823 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m00:21:20.250003 [info ] [MainThread]: 
[0m00:21:20.250155 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 0.47 seconds (0.47s).
[0m00:21:20.250603 [debug] [MainThread]: Command end result
[0m00:21:20.266359 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:21:20.267318 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:21:20.270249 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:21:20.270406 [info ] [MainThread]: 
[0m00:21:20.270588 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:21:20.270716 [info ] [MainThread]: 
[0m00:21:20.270861 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m00:21:20.273159 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9722256, "process_in_blocks": "0", "process_kernel_time": 0.189127, "process_mem_max_rss": "143802368", "process_out_blocks": "0", "process_user_time": 1.186899}
[0m00:21:20.273440 [debug] [MainThread]: Command `dbt run` succeeded at 00:21:20.273395 after 0.97 seconds
[0m00:21:20.273633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f5fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108eb2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102acaa50>]}
[0m00:21:20.273812 [debug] [MainThread]: Flushing usage events
[0m00:21:20.551593 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:24:52.679299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d72a290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7a7a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7a7f90>]}


============================== 00:24:52.682012 | 9f8544cb-2617-4c9d-a24c-170d350bfc4a ==============================
[0m00:24:52.682012 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:24:52.682298 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'introspect': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select staging_country_profiling', 'use_colors': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_format': 'default', 'fail_fast': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'printer_width': '80', 'partial_parse': 'True', 'debug': 'False', 'quiet': 'False', 'static_parser': 'True', 'version_check': 'True', 'empty': 'False'}
[0m00:24:52.812323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd7a410>]}
[0m00:24:52.842558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d72b3d0>]}
[0m00:24:52.843275 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:24:52.891393 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:24:52.954221 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m00:24:52.954600 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/staging_country_profiling.sql
[0m00:24:53.080435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e54a9d0>]}
[0m00:24:53.145387 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:24:53.146357 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:24:53.157820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e58bf90>]}
[0m00:24:53.158083 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:24:53.158247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7d4750>]}
[0m00:24:53.158938 [info ] [MainThread]: 
[0m00:24:53.159087 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:24:53.159201 [info ] [MainThread]: 
[0m00:24:53.159402 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:24:53.159771 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:24:53.185125 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:24:53.185401 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:24:53.185533 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:53.214316 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.029 seconds
[0m00:24:53.214930 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:24:53.217104 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m00:24:53.217338 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m00:24:53.217590 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m00:24:53.220017 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:24:53.220271 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:24:53.221176 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:24:53.221936 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:24:53.222065 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:24:53.223019 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:24:53.223144 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:24:53.223255 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:24:53.223366 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:24:53.223478 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:24:53.223580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:53.223684 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:53.223889 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:53.241498 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m00:24:53.241698 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m00:24:53.241844 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m00:24:53.241954 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m00:24:53.242100 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:24:53.242236 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:24:53.242372 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:24:53.242497 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:24:53.242640 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:24:53.242799 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:24:53.242951 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:24:53.243097 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:24:53.246335 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:24:53.246463 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m00:24:53.246608 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m00:24:53.247187 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:24:53.247324 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m00:24:53.247783 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:24:53.248280 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:24:53.248799 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:24:53.248936 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:24:53.249224 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public)
[0m00:24:53.250216 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:24:53.250323 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:24:53.250443 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:24:53.250548 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:24:53.250655 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:24:53.250856 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_staging)
[0m00:24:53.251629 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:24:53.252762 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:24:53.253085 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:24:53.253268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:24:53.261385 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:24:53.261557 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m00:24:53.261725 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:24:53.261856 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:24:53.261986 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:24:53.262139 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:24:53.264384 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:24:53.264520 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m00:24:53.265104 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:24:53.265541 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:24:53.265927 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:24:53.266049 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:24:53.269163 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.269354 [debug] [MainThread]: On master: BEGIN
[0m00:24:53.269455 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:24:53.277623 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m00:24:53.277803 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.277962 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:24:53.280672 [debug] [MainThread]: SQL status: SELECT 12 in 0.003 seconds
[0m00:24:53.282013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f0e0d90>]}
[0m00:24:53.282235 [debug] [MainThread]: On master: ROLLBACK
[0m00:24:53.282622 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.282741 [debug] [MainThread]: On master: BEGIN
[0m00:24:53.283177 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:24:53.283294 [debug] [MainThread]: On master: COMMIT
[0m00:24:53.283409 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.283517 [debug] [MainThread]: On master: COMMIT
[0m00:24:53.283852 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:24:53.283990 [debug] [MainThread]: On master: Close
[0m00:24:53.285440 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_country_profiling
[0m00:24:53.285661 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging_analysis.staging_country_profiling . [RUN]
[0m00:24:53.285855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.staging_country_profiling)
[0m00:24:53.286119 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_country_profiling
[0m00:24:53.289500 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_country_profiling"
[0m00:24:53.289895 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_country_profiling
[0m00:24:53.304571 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_country_profiling"
[0m00:24:53.305413 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.305627 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: BEGIN
[0m00:24:53.305764 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:24:53.316033 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m00:24:53.316251 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.316460 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Country profiling analysis for staging data
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_country_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location (assuming format like "City, State" or "City, Country")
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by country
),

pdl_country_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        -- Extract country from location
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else location
        end as country,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by country
)

select * from fxf_country_profile
union all
select * from pdl_country_profile
order by contact_count desc
  );
  
[0m00:24:53.455950 [debug] [Thread-1 (]: SQL status: SELECT 32 in 0.139 seconds
[0m00:24:53.463133 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.463460 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_country_profiling" rename to "staging_country_profiling__dbt_backup"
[0m00:24:53.464261 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:24:53.465956 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.466196 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_tmp" rename to "staging_country_profiling"
[0m00:24:53.466798 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:24:53.474777 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m00:24:53.474997 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.475140 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: COMMIT
[0m00:24:53.476348 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:24:53.480304 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup"
[0m00:24:53.482796 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_country_profiling"
[0m00:24:53.483018 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_country_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_country_profiling__dbt_backup" cascade
[0m00:24:53.484366 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:24:53.485474 [debug] [Thread-1 (]: On model.dbt_service.staging_country_profiling: Close
[0m00:24:53.486455 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f8544cb-2617-4c9d-a24c-170d350bfc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1e4210>]}
[0m00:24:53.486764 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging_analysis.staging_country_profiling  [[32mSELECT 32[0m in 0.20s]
[0m00:24:53.487009 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_country_profiling
[0m00:24:53.487642 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.487786 [debug] [MainThread]: On master: BEGIN
[0m00:24:53.487901 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:24:53.493782 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:24:53.493972 [debug] [MainThread]: On master: COMMIT
[0m00:24:53.494102 [debug] [MainThread]: Using postgres connection "master"
[0m00:24:53.494220 [debug] [MainThread]: On master: COMMIT
[0m00:24:53.494553 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:24:53.494753 [debug] [MainThread]: On master: Close
[0m00:24:53.494947 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:24:53.495069 [debug] [MainThread]: Connection 'model.dbt_service.staging_country_profiling' was properly closed.
[0m00:24:53.495178 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m00:24:53.495289 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m00:24:53.495408 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m00:24:53.495550 [info ] [MainThread]: 
[0m00:24:53.495705 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.34 seconds (0.34s).
[0m00:24:53.495976 [debug] [MainThread]: Command end result
[0m00:24:53.509925 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:24:53.511232 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:24:53.514066 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:24:53.514226 [info ] [MainThread]: 
[0m00:24:53.514425 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:24:53.514559 [info ] [MainThread]: 
[0m00:24:53.514697 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m00:24:53.517644 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.8759987, "process_in_blocks": "0", "process_kernel_time": 0.224883, "process_mem_max_rss": "139608064", "process_out_blocks": "0", "process_user_time": 1.103093}
[0m00:24:53.517941 [debug] [MainThread]: Command `dbt run` succeeded at 00:24:53.517894 after 0.88 seconds
[0m00:24:53.518137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7a7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105104090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054769d0>]}
[0m00:24:53.518321 [debug] [MainThread]: Flushing usage events
[0m00:24:53.881170 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:26:51.052104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11372a350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11379a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113799fd0>]}


============================== 00:26:51.054868 | b27e4767-f224-4ccb-b7b8-ff5b0aebd718 ==============================
[0m00:26:51.054868 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:26:51.055172 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select staging_city_state_profiling', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'use_colors': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'log_format': 'default', 'introspect': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'write_json': 'True'}
[0m00:26:51.183928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e88690>]}
[0m00:26:51.213840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f68850>]}
[0m00:26:51.214478 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:26:51.261719 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:26:51.327142 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m00:26:51.327432 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging_analysis/staging_city_state_profiling.sql
[0m00:26:51.327584 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/staging_analysis/staging_country_profiling.sql
[0m00:26:51.454174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114900250>]}
[0m00:26:51.518168 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:26:51.519499 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:26:51.532152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a39910>]}
[0m00:26:51.532447 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:26:51.532607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115229e50>]}
[0m00:26:51.533331 [info ] [MainThread]: 
[0m00:26:51.533481 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:26:51.533600 [info ] [MainThread]: 
[0m00:26:51.533811 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:26:51.534200 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:26:51.560071 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:26:51.560306 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:26:51.560442 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:51.590267 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.030 seconds
[0m00:26:51.590906 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:26:51.593221 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m00:26:51.593428 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m00:26:51.593678 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m00:26:51.593931 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:26:51.596395 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:26:51.597258 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:26:51.598111 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:26:51.599264 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:26:51.599422 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:26:51.599540 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:26:51.599647 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:26:51.599748 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:26:51.599866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:26:51.599983 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:51.600089 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:51.600185 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:51.612313 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:26:51.612514 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:26:51.612687 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:26:51.612809 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:26:51.612910 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:26:51.613041 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:26:51.613163 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:26:51.613283 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:26:51.613413 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:26:51.613531 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:26:51.613667 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:26:51.613868 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:26:51.617185 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m00:26:51.617369 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m00:26:51.618077 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:26:51.618223 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:26:51.618344 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:26:51.618802 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:26:51.619305 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:26:51.619460 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:26:51.619945 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:26:51.620257 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m00:26:51.620645 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:26:51.620778 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:26:51.620900 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:26:51.621757 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:26:51.621960 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging)
[0m00:26:51.622528 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:26:51.623772 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:26:51.623907 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:26:51.624019 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:26:51.624205 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:26:51.630439 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:26:51.630619 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m00:26:51.630776 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:26:51.630890 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:26:51.631015 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:26:51.631157 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:26:51.633205 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m00:26:51.633358 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:26:51.633964 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:26:51.634400 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:26:51.634855 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:26:51.635016 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:26:51.638405 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.638543 [debug] [MainThread]: On master: BEGIN
[0m00:26:51.638648 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:26:51.646844 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m00:26:51.646986 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.647166 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:26:51.650398 [debug] [MainThread]: SQL status: SELECT 12 in 0.003 seconds
[0m00:26:51.651637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158b2210>]}
[0m00:26:51.651880 [debug] [MainThread]: On master: ROLLBACK
[0m00:26:51.652238 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.652351 [debug] [MainThread]: On master: BEGIN
[0m00:26:51.652850 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:26:51.652990 [debug] [MainThread]: On master: COMMIT
[0m00:26:51.653100 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.653200 [debug] [MainThread]: On master: COMMIT
[0m00:26:51.653519 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:26:51.653650 [debug] [MainThread]: On master: Close
[0m00:26:51.655963 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_city_state_profiling
[0m00:26:51.656203 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m00:26:51.656414 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.staging_city_state_profiling)
[0m00:26:51.656557 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m00:26:51.659814 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.660163 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_city_state_profiling
[0m00:26:51.675043 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.675416 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.675556 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m00:26:51.675689 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:26:51.683830 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m00:26:51.684083 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.684297 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m00:26:51.834323 [debug] [Thread-1 (]: SQL status: SELECT 76 in 0.150 seconds
[0m00:26:51.840197 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.840384 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m00:26:51.841143 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:26:51.848357 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m00:26:51.848578 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.848733 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m00:26:51.849723 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:26:51.852856 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m00:26:51.855534 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:26:51.855763 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m00:26:51.856226 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m00:26:51.857463 [debug] [Thread-1 (]: On model.dbt_service.staging_city_state_profiling: Close
[0m00:26:51.858470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27e4767-f224-4ccb-b7b8-ff5b0aebd718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f38390>]}
[0m00:26:51.858762 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.20s]
[0m00:26:51.858999 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_city_state_profiling
[0m00:26:51.859611 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.859766 [debug] [MainThread]: On master: BEGIN
[0m00:26:51.859875 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:26:51.866380 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:26:51.866539 [debug] [MainThread]: On master: COMMIT
[0m00:26:51.866671 [debug] [MainThread]: Using postgres connection "master"
[0m00:26:51.866789 [debug] [MainThread]: On master: COMMIT
[0m00:26:51.867142 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:26:51.867259 [debug] [MainThread]: On master: Close
[0m00:26:51.867422 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:26:51.867546 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m00:26:51.867649 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m00:26:51.867765 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m00:26:51.867882 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m00:26:51.868020 [info ] [MainThread]: 
[0m00:26:51.868166 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m00:26:51.868463 [debug] [MainThread]: Command end result
[0m00:26:51.884917 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:26:51.885926 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:26:51.889439 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:26:51.889695 [info ] [MainThread]: 
[0m00:26:51.889918 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:26:51.890070 [info ] [MainThread]: 
[0m00:26:51.890228 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m00:26:51.892631 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.8780757, "process_in_blocks": "0", "process_kernel_time": 0.222148, "process_mem_max_rss": "137691136", "process_out_blocks": "0", "process_user_time": 1.08497}
[0m00:26:51.892908 [debug] [MainThread]: Command `dbt run` succeeded at 00:26:51.892864 after 0.88 seconds
[0m00:26:51.893108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137aa8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137aa550>]}
[0m00:26:51.893287 [debug] [MainThread]: Flushing usage events
[0m00:26:52.188236 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:27:18.899794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106213cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1025e5dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106937fd0>]}


============================== 00:27:18.902228 | 9dacb863-7e26-495c-bf50-05f9c8c57091 ==============================
[0m00:27:18.902228 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:27:18.902519 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'static_parser': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False', 'printer_width': '80', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'write_json': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'log_format': 'default', 'version_check': 'True', 'invocation_command': 'dbt run --select staging_analysis', 'introspect': 'True', 'use_colors': 'True', 'empty': 'False'}
[0m00:27:18.989822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106937650>]}
[0m00:27:19.021064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102609bd0>]}
[0m00:27:19.021581 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:27:19.067032 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:27:19.130532 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:27:19.130801 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:27:19.154426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10792b510>]}
[0m00:27:19.194339 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:27:19.195289 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:27:19.206659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aab0d0>]}
[0m00:27:19.206909 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:27:19.207077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a9f750>]}
[0m00:27:19.207910 [info ] [MainThread]: 
[0m00:27:19.208073 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:27:19.208186 [info ] [MainThread]: 
[0m00:27:19.208408 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:27:19.210059 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:27:19.254789 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:27:19.255034 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:27:19.255184 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:27:19.276349 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m00:27:19.276977 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:27:19.277789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m00:27:19.278016 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:27:19.280345 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:27:19.280553 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:27:19.280743 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m00:27:19.281579 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:27:19.281707 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:27:19.282410 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:27:19.283126 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:27:19.283271 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:27:19.283387 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:19.283523 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:27:19.283672 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:27:19.283816 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:27:19.284011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:27:19.284133 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:27:19.298412 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:27:19.298618 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:27:19.298810 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:27:19.298975 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:27:19.299098 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:27:19.299225 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m00:27:19.299378 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:27:19.299492 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:27:19.299596 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:27:19.299716 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:27:19.299851 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:27:19.299986 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:27:19.302819 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:27:19.302946 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:27:19.303068 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m00:27:19.303526 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:27:19.303932 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:27:19.304344 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:27:19.304468 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:27:19.305004 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:27:19.305138 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:27:19.305248 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:27:19.305362 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:27:19.305590 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw)
[0m00:27:19.305710 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:27:19.306041 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m00:27:19.307067 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:27:19.308324 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:27:19.308650 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:27:19.308760 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:27:19.308859 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:19.308958 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:19.316176 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:27:19.316334 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:27:19.316463 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:27:19.316585 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:27:19.316711 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:27:19.316860 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:27:19.318368 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m00:27:19.318517 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m00:27:19.319068 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:27:19.319440 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:27:19.319825 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:27:19.319946 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:27:19.322777 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.322901 [debug] [MainThread]: On master: BEGIN
[0m00:27:19.323001 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:27:19.330569 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m00:27:19.330718 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.330870 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:27:19.333428 [debug] [MainThread]: SQL status: SELECT 12 in 0.002 seconds
[0m00:27:19.334590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ad3550>]}
[0m00:27:19.334780 [debug] [MainThread]: On master: ROLLBACK
[0m00:27:19.335176 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.335284 [debug] [MainThread]: On master: BEGIN
[0m00:27:19.335823 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:27:19.335964 [debug] [MainThread]: On master: COMMIT
[0m00:27:19.336075 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.336178 [debug] [MainThread]: On master: COMMIT
[0m00:27:19.336539 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:27:19.336657 [debug] [MainThread]: On master: Close
[0m00:27:19.339706 [debug] [Thread-1 (]: Began running node model.dbt_service.company_analysis
[0m00:27:19.339879 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_city_state_profiling
[0m00:27:19.340049 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_company_profiling
[0m00:27:19.340174 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_data_profiling
[0m00:27:19.340353 [info ] [Thread-1 (]: 1 of 5 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m00:27:19.340577 [info ] [Thread-2 (]: 2 of 5 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m00:27:19.340766 [info ] [Thread-3 (]: 3 of 5 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m00:27:19.340950 [info ] [Thread-4 (]: 4 of 5 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m00:27:19.341140 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.company_analysis)
[0m00:27:19.341290 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.staging_city_state_profiling)
[0m00:27:19.341433 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.staging_company_profiling)
[0m00:27:19.341568 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.staging_data_profiling)
[0m00:27:19.341745 [debug] [Thread-1 (]: Began compiling node model.dbt_service.company_analysis
[0m00:27:19.341893 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m00:27:19.342023 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m00:27:19.342144 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m00:27:19.345428 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m00:27:19.346876 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.348174 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m00:27:19.349405 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m00:27:19.350288 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_city_state_profiling
[0m00:27:19.350472 [debug] [Thread-1 (]: Began executing node model.dbt_service.company_analysis
[0m00:27:19.356989 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_company_profiling
[0m00:27:19.357307 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_data_profiling
[0m00:27:19.367581 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.369330 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m00:27:19.370746 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m00:27:19.372048 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m00:27:19.372606 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.372809 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.372989 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.373143 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.373284 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m00:27:19.373435 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: BEGIN
[0m00:27:19.373561 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m00:27:19.373699 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m00:27:19.373881 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m00:27:19.374161 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:27:19.374317 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:27:19.374465 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:27:19.387780 [debug] [Thread-3 (]: SQL status: BEGIN in 0.014 seconds
[0m00:27:19.388028 [debug] [Thread-2 (]: SQL status: BEGIN in 0.014 seconds
[0m00:27:19.388219 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m00:27:19.388394 [debug] [Thread-4 (]: SQL status: BEGIN in 0.014 seconds
[0m00:27:19.388542 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.388691 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.388848 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.388991 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.389281 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m00:27:19.389715 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m00:27:19.390901 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m00:27:19.391892 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m00:27:19.538719 [debug] [Thread-4 (]: SQL status: SELECT 2 in 0.146 seconds
[0m00:27:19.543772 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.544032 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m00:27:19.544755 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:27:19.546105 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.546276 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m00:27:19.546892 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:27:19.553361 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:27:19.553648 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.161 seconds
[0m00:27:19.553890 [debug] [Thread-1 (]: SQL status: SELECT 91 in 0.161 seconds
[0m00:27:19.554138 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.555668 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.556897 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.557060 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m00:27:19.557211 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m00:27:19.557358 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m00:27:19.557980 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:27:19.558136 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:27:19.559432 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.559587 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m00:27:19.560711 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.560873 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m00:27:19.563484 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m00:27:19.563698 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m00:27:19.566116 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m00:27:19.566340 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m00:27:19.566612 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m00:27:19.567319 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:27:19.567519 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:27:19.567710 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.568457 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m00:27:19.568650 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:27:19.568912 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: COMMIT
[0m00:27:19.569089 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.570098 [debug] [Thread-4 (]: On model.dbt_service.staging_data_profiling: Close
[0m00:27:19.570302 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m00:27:19.570988 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:27:19.571179 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m00:27:19.572369 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m00:27:19.573374 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m00:27:19.573543 [debug] [Thread-3 (]: SQL status: SELECT 180 in 0.181 seconds
[0m00:27:19.573709 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110095010>]}
[0m00:27:19.574052 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m00:27:19.574357 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m00:27:19.576597 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.576922 [info ] [Thread-4 (]: 4 of 5 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.23s]
[0m00:27:19.577123 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m00:27:19.577287 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m00:27:19.577456 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m00:27:19.577771 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_data_profiling
[0m00:27:19.578044 [debug] [Thread-4 (]: Began running node model.dbt_service.staging_location_profiling
[0m00:27:19.578508 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m00:27:19.578350 [info ] [Thread-4 (]: 5 of 5 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m00:27:19.579903 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.580061 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:27:19.580204 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m00:27:19.580425 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_data_profiling, now model.dbt_service.staging_location_profiling)
[0m00:27:19.580604 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m00:27:19.581128 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: Close
[0m00:27:19.581583 [debug] [Thread-1 (]: On model.dbt_service.company_analysis: Close
[0m00:27:19.581751 [debug] [Thread-4 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m00:27:19.582028 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110611490>]}
[0m00:27:19.582275 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:27:19.583932 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m00:27:19.584104 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100e5a50>]}
[0m00:27:19.584535 [info ] [Thread-2 (]: 2 of 5 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.24s]
[0m00:27:19.585214 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:27:19.585765 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_city_state_profiling
[0m00:27:19.585569 [info ] [Thread-1 (]: 1 of 5 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.24s]
[0m00:27:19.586010 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.586253 [debug] [Thread-4 (]: Began executing node model.dbt_service.staging_location_profiling
[0m00:27:19.586471 [debug] [Thread-1 (]: Finished running node model.dbt_service.company_analysis
[0m00:27:19.586633 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m00:27:19.588294 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m00:27:19.588661 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.588818 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m00:27:19.588960 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m00:27:19.589179 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m00:27:19.590254 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m00:27:19.590580 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m00:27:19.590788 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m00:27:19.591735 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:27:19.592346 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: Close
[0m00:27:19.592617 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11008ff50>]}
[0m00:27:19.592887 [info ] [Thread-3 (]: 3 of 5 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.25s]
[0m00:27:19.593121 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_company_profiling
[0m00:27:19.594826 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m00:27:19.595056 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.595295 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m00:27:19.861142 [debug] [Thread-4 (]: SQL status: SELECT 152 in 0.266 seconds
[0m00:27:19.863539 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.863825 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m00:27:19.864516 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:27:19.865591 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.865723 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m00:27:19.866203 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:27:19.866892 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:27:19.867154 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.867389 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m00:27:19.868544 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m00:27:19.869467 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m00:27:19.869909 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m00:27:19.870150 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m00:27:19.871478 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:27:19.871895 [debug] [Thread-4 (]: On model.dbt_service.staging_location_profiling: Close
[0m00:27:19.872180 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9dacb863-7e26-495c-bf50-05f9c8c57091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105237210>]}
[0m00:27:19.872447 [info ] [Thread-4 (]: 5 of 5 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.29s]
[0m00:27:19.872670 [debug] [Thread-4 (]: Finished running node model.dbt_service.staging_location_profiling
[0m00:27:19.873388 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.873569 [debug] [MainThread]: On master: BEGIN
[0m00:27:19.873700 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:27:19.879248 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:27:19.879463 [debug] [MainThread]: On master: COMMIT
[0m00:27:19.879597 [debug] [MainThread]: Using postgres connection "master"
[0m00:27:19.879712 [debug] [MainThread]: On master: COMMIT
[0m00:27:19.880026 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:27:19.880152 [debug] [MainThread]: On master: Close
[0m00:27:19.880329 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:27:19.880441 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m00:27:19.880547 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m00:27:19.880650 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m00:27:19.880748 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m00:27:19.880916 [info ] [MainThread]: 
[0m00:27:19.881059 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 0.67 seconds (0.67s).
[0m00:27:19.881560 [debug] [MainThread]: Command end result
[0m00:27:19.898497 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:27:19.899396 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:27:19.903696 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:27:19.903972 [info ] [MainThread]: 
[0m00:27:19.904176 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:27:19.904387 [info ] [MainThread]: 
[0m00:27:19.904554 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m00:27:19.906974 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0429319, "process_in_blocks": "0", "process_kernel_time": 0.185093, "process_mem_max_rss": "136626176", "process_out_blocks": "0", "process_user_time": 1.069429}
[0m00:27:19.907267 [debug] [MainThread]: Command `dbt run` succeeded at 00:27:19.907218 after 1.04 seconds
[0m00:27:19.907470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100310250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10695c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1003de9d0>]}
[0m00:27:19.907664 [debug] [MainThread]: Flushing usage events
[0m00:27:20.182023 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:33:18.325598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107428c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107437690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107429fd0>]}


============================== 00:33:18.328155 | faff9d7e-d70d-4ec0-9ee7-9e6533fc1870 ==============================
[0m00:33:18.328155 [info ] [MainThread]: Running with dbt=1.10.13
[0m00:33:18.328454 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'write_json': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'printer_width': '80', 'static_parser': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'debug': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'introspect': 'True', 'fail_fast': 'False', 'empty': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select marts', 'log_format': 'default'}
[0m00:33:18.457867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10745ffd0>]}
[0m00:33:18.489199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b33d0>]}
[0m00:33:18.489983 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m00:33:18.538141 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m00:33:18.607101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:33:18.607545 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/data_overview.sql
[0m00:33:18.739606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108504e90>]}
[0m00:33:18.809446 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:33:18.810524 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:33:18.823039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10846c490>]}
[0m00:33:18.823284 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m00:33:18.823440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10851f650>]}
[0m00:33:18.824162 [info ] [MainThread]: 
[0m00:33:18.824307 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m00:33:18.824419 [info ] [MainThread]: 
[0m00:33:18.824619 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:33:18.826189 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m00:33:18.852613 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m00:33:18.852892 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m00:33:18.853015 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:18.884076 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m00:33:18.884725 [debug] [ThreadPool]: On list_finny_db: Close
[0m00:33:18.885600 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m00:33:18.885789 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m00:33:18.886004 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m00:33:18.888338 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:33:18.888512 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m00:33:18.889440 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:33:18.890146 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:33:18.890273 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m00:33:18.891207 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:33:18.891333 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m00:33:18.891453 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m00:33:18.891559 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:33:18.891682 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m00:33:18.891790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:18.891889 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:18.892081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:18.904278 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:33:18.904418 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m00:33:18.904563 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m00:33:18.904686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m00:33:18.904857 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m00:33:18.905019 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m00:33:18.905162 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m00:33:18.905325 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m00:33:18.905448 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m00:33:18.905565 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m00:33:18.905683 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m00:33:18.905833 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m00:33:18.909052 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:33:18.909179 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m00:33:18.909279 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m00:33:18.909814 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m00:33:18.910233 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m00:33:18.910376 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m00:33:18.910834 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m00:33:18.911276 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m00:33:18.911426 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m00:33:18.911532 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m00:33:18.911766 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public)
[0m00:33:18.912043 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m00:33:18.912218 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_raw)
[0m00:33:18.912977 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:33:18.913078 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m00:33:18.913979 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:33:18.914286 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m00:33:18.914770 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m00:33:18.914890 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:33:18.915003 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:33:18.922283 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m00:33:18.922457 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m00:33:18.922594 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m00:33:18.922721 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m00:33:18.922854 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m00:33:18.922999 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m00:33:18.924723 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m00:33:18.925255 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m00:33:18.925730 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m00:33:18.925972 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m00:33:18.926395 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m00:33:18.927216 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m00:33:18.929484 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:18.929631 [debug] [MainThread]: On master: BEGIN
[0m00:33:18.929736 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:33:18.935879 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:33:18.936038 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:18.936192 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m00:33:18.938303 [debug] [MainThread]: SQL status: SELECT 6 in 0.002 seconds
[0m00:33:18.939110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10842ae90>]}
[0m00:33:18.939298 [debug] [MainThread]: On master: ROLLBACK
[0m00:33:18.939660 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:18.939777 [debug] [MainThread]: On master: BEGIN
[0m00:33:18.940362 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m00:33:18.940491 [debug] [MainThread]: On master: COMMIT
[0m00:33:18.940597 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:18.940696 [debug] [MainThread]: On master: COMMIT
[0m00:33:18.941031 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:33:18.941159 [debug] [MainThread]: On master: Close
[0m00:33:18.943186 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m00:33:18.943547 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m00:33:18.943397 [info ] [Thread-1 (]: 1 of 2 START sql view model public_marts.data_overview ......................... [RUN]
[0m00:33:18.943814 [info ] [Thread-2 (]: 2 of 2 START sql table model public_marts.location_analysis .................... [RUN]
[0m00:33:18.944067 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.data_overview)
[0m00:33:18.944333 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.location_analysis)
[0m00:33:18.944485 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m00:33:18.944628 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m00:33:18.947897 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m00:33:18.949155 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m00:33:18.949541 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m00:33:18.949683 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m00:33:18.962976 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m00:33:18.971091 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m00:33:18.971738 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:18.971882 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m00:33:18.972064 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:33:18.972240 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m00:33:18.972416 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m00:33:18.972637 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:33:18.982722 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m00:33:18.982867 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m00:33:18.983012 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:18.983145 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:33:18.983329 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources with structured location data



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location, city, state
),

state_summary as (
    select
        state,
        count(distinct location) as locations_in_state,
        count(distinct city) as cities_in_state,
        sum(contact_count) as total_contacts_in_state,
        sum(company_count) as total_companies_in_state,
        avg(avg_revenue) as avg_state_revenue
    from location_stats
    where state is not null
    group by state
)

select
    ls.location,
    ls.city,
    ls.state,
    ls.contact_count,
    ls.company_count,
    ls.unique_titles,
    ls.data_sources,
    round(ls.avg_revenue::numeric, 2) as avg_revenue,
    -- Add state-level context
    ss.total_contacts_in_state,
    ss.cities_in_state,
    round(100.0 * ls.contact_count / ss.total_contacts_in_state, 2) as pct_of_state_contacts
from location_stats ls
left join state_summary ss on ls.state = ss.state
order by ls.contact_count desc
  );
  
[0m00:33:18.983552 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from staging layer
-- This model provides a summary of our cleaned data pipeline



select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'combined_staging_data' as table_name,
    'staging' as schema_name,
    (select count(*) from "finny_db"."public_staging"."stg_fxf_data") + (select count(*) from "finny_db"."public_staging"."stg_pdl_data") as row_count,
    'Total cleaned contact records' as description
  );
[0m00:33:18.986222 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m00:33:18.989408 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:33:18.989579 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m00:33:18.990168 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:33:18.995720 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m00:33:18.995921 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:33:18.996076 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m00:33:18.997219 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m00:33:19.000293 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m00:33:19.002153 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m00:33:19.002305 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m00:33:19.002763 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m00:33:19.003686 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m00:33:19.004577 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10820d850>]}
[0m00:33:19.004869 [info ] [Thread-1 (]: 1 of 2 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.06s]
[0m00:33:19.005117 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m00:33:19.134063 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.150 seconds
[0m00:33:19.138415 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:19.138704 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m00:33:19.139428 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:33:19.140975 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:19.141229 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m00:33:19.141803 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m00:33:19.143717 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:33:19.143909 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:19.144039 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m00:33:19.144680 [debug] [Thread-2 (]: SQL status: COMMIT in 0.000 seconds
[0m00:33:19.145846 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m00:33:19.147671 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m00:33:19.147914 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m00:33:19.149056 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m00:33:19.149555 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m00:33:19.149800 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'faff9d7e-d70d-4ec0-9ee7-9e6533fc1870', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b75410>]}
[0m00:33:19.150068 [info ] [Thread-2 (]: 2 of 2 OK created sql table model public_marts.location_analysis ............... [[32mSELECT 76[0m in 0.21s]
[0m00:33:19.150296 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m00:33:19.150861 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:19.151009 [debug] [MainThread]: On master: BEGIN
[0m00:33:19.151127 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m00:33:19.157225 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m00:33:19.157396 [debug] [MainThread]: On master: COMMIT
[0m00:33:19.157518 [debug] [MainThread]: Using postgres connection "master"
[0m00:33:19.157626 [debug] [MainThread]: On master: COMMIT
[0m00:33:19.157916 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m00:33:19.158034 [debug] [MainThread]: On master: Close
[0m00:33:19.158202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:33:19.158310 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m00:33:19.158409 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m00:33:19.158508 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m00:33:19.158608 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m00:33:19.158734 [info ] [MainThread]: 
[0m00:33:19.158874 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m00:33:19.159290 [debug] [MainThread]: Command end result
[0m00:33:19.175624 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m00:33:19.176438 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m00:33:19.180133 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m00:33:19.180367 [info ] [MainThread]: 
[0m00:33:19.180583 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:33:19.180733 [info ] [MainThread]: 
[0m00:33:19.180888 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m00:33:19.183329 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.89632064, "process_in_blocks": "0", "process_kernel_time": 0.238252, "process_mem_max_rss": "141590528", "process_out_blocks": "0", "process_user_time": 1.136403}
[0m00:33:19.183609 [debug] [MainThread]: Command `dbt run` succeeded at 00:33:19.183564 after 0.90 seconds
[0m00:33:19.183810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e83050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1002b8350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100386c10>]}
[0m00:33:19.183991 [debug] [MainThread]: Flushing usage events
[0m00:33:19.552444 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:12:08.027698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c2a550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c9e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cabdd0>]}


============================== 05:12:08.030562 | 1cc53f0b-8377-4528-9015-be763280f283 ==============================
[0m05:12:08.030562 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:12:08.030856 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select staging+ marts', 'fail_fast': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'use_colors': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'version_check': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m05:12:08.261693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cf4fd0>]}
[0m05:12:08.291277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137f5a10>]}
[0m05:12:08.292014 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:12:08.337375 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:12:08.400870 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:12:08.401212 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/location_analysis.sql
[0m05:12:08.619185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c37a10>]}
[0m05:12:08.657981 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:12:08.659084 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:12:08.670922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117bc8bd0>]}
[0m05:12:08.671157 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:12:08.671308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160fe710>]}
[0m05:12:08.672279 [info ] [MainThread]: 
[0m05:12:08.672424 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:12:08.672536 [info ] [MainThread]: 
[0m05:12:08.672724 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:12:08.674464 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:12:08.674688 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:12:08.678698 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:12:08.702677 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:12:08.702881 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:12:08.703027 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:12:08.703213 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:12:08.703357 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:12:08.703496 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:12:08.703630 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:08.703749 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:08.703863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:08.739461 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.036 seconds
[0m05:12:08.739657 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.036 seconds
[0m05:12:08.739792 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.036 seconds
[0m05:12:08.740403 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:12:08.740909 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:12:08.741338 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:12:08.742416 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m05:12:08.742593 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m05:12:08.742770 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m05:12:08.742996 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m05:12:08.745252 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:12:08.746024 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:12:08.746678 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:12:08.747315 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:12:08.747427 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:12:08.747535 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:12:08.747638 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:12:08.747736 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:12:08.747835 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:08.747934 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:08.748031 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:08.748136 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:08.762616 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:12:08.762805 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:12:08.763027 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:12:08.764977 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m05:12:08.765101 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m05:12:08.765260 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m05:12:08.765394 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:12:08.765527 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:12:08.765646 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:12:08.765777 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:12:08.765913 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:12:08.766049 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:12:08.767858 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m05:12:08.768472 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:12:08.768682 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m05:12:08.769440 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:12:08.769600 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m05:12:08.769732 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:12:08.769855 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m05:12:08.770407 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:12:08.770571 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:12:08.770840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public)
[0m05:12:08.771603 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:12:08.771888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_staging)
[0m05:12:08.772969 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:12:08.773270 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:12:08.776106 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:12:08.776261 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:12:08.776388 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:12:08.777062 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:12:08.777503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:08.777673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:08.784720 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m05:12:08.784911 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m05:12:08.785165 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:12:08.785304 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:12:08.785451 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:12:08.785600 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:12:08.787106 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m05:12:08.787252 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m05:12:08.787820 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:12:08.788246 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:12:08.788580 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:12:08.788710 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:12:08.792115 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:08.792272 [debug] [MainThread]: On master: BEGIN
[0m05:12:08.792378 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:12:08.801459 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m05:12:08.801667 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:08.801852 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:12:08.805286 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m05:12:08.806369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c41310>]}
[0m05:12:08.806583 [debug] [MainThread]: On master: ROLLBACK
[0m05:12:08.806942 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:08.807053 [debug] [MainThread]: On master: BEGIN
[0m05:12:08.807526 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:12:08.807643 [debug] [MainThread]: On master: COMMIT
[0m05:12:08.807751 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:08.807856 [debug] [MainThread]: On master: COMMIT
[0m05:12:08.808196 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:12:08.808316 [debug] [MainThread]: On master: Close
[0m05:12:08.811142 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_fxf_data
[0m05:12:08.811312 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_pdl_data
[0m05:12:08.811563 [info ] [Thread-1 (]: 1 of 9 START sql view model public_staging.stg_fxf_data ........................ [RUN]
[0m05:12:08.811781 [info ] [Thread-2 (]: 2 of 9 START sql view model public_staging.stg_pdl_data ........................ [RUN]
[0m05:12:08.811967 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m05:12:08.812119 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_pdl_data)
[0m05:12:08.812278 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m05:12:08.812453 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m05:12:08.816577 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m05:12:08.817749 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m05:12:08.818162 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_fxf_data
[0m05:12:08.818356 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_pdl_data
[0m05:12:08.833879 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m05:12:08.835697 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m05:12:08.836232 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.836473 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.836715 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m05:12:08.836917 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m05:12:08.837082 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m05:12:08.837228 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:12:08.849023 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m05:12:08.849272 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.849411 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m05:12:08.849621 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m05:12:08.849835 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.850081 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m05:12:08.853618 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:12:08.853767 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:12:08.856777 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.858124 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.858276 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data" rename to "stg_pdl_data__dbt_backup"
[0m05:12:08.858423 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data" rename to "stg_fxf_data__dbt_backup"
[0m05:12:08.859733 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:08.859897 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:08.860996 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.862041 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.862193 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m05:12:08.862324 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m05:12:08.863398 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:08.863537 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:08.870131 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m05:12:08.870906 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m05:12:08.871086 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.871244 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.871404 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m05:12:08.871542 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m05:12:08.873423 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m05:12:08.873558 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m05:12:08.877007 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m05:12:08.878160 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m05:12:08.880232 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m05:12:08.880518 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m05:12:08.880664 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m05:12:08.880826 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m05:12:08.884531 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.004 seconds
[0m05:12:08.887427 [debug] [Thread-1 (]: On model.dbt_service.stg_fxf_data: Close
[0m05:12:08.887614 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.007 seconds
[0m05:12:08.888369 [debug] [Thread-2 (]: On model.dbt_service.stg_pdl_data: Close
[0m05:12:08.889925 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c05210>]}
[0m05:12:08.891843 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c41ed0>]}
[0m05:12:08.892195 [info ] [Thread-1 (]: 1 of 9 OK created sql view model public_staging.stg_fxf_data ................... [[32mCREATE VIEW[0m in 0.08s]
[0m05:12:08.895090 [info ] [Thread-2 (]: 2 of 9 OK created sql view model public_staging.stg_pdl_data ................... [[32mCREATE VIEW[0m in 0.08s]
[0m05:12:08.895355 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_fxf_data
[0m05:12:08.895604 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_pdl_data
[0m05:12:08.896631 [debug] [Thread-4 (]: Began running node model.dbt_service.company_analysis
[0m05:12:08.896789 [debug] [Thread-3 (]: Began running node model.dbt_service.data_overview
[0m05:12:08.896975 [debug] [Thread-1 (]: Began running node model.dbt_service.location_analysis
[0m05:12:08.897109 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_city_state_profiling
[0m05:12:08.899742 [info ] [Thread-4 (]: 3 of 9 START sql table model public_staging_analysis.company_analysis .......... [RUN]
[0m05:12:08.899962 [info ] [Thread-3 (]: 4 of 9 START sql view model public_marts.data_overview ......................... [RUN]
[0m05:12:08.900143 [info ] [Thread-1 (]: 5 of 9 START sql table model public_marts.location_analysis .................... [RUN]
[0m05:12:08.900333 [info ] [Thread-2 (]: 6 of 9 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m05:12:08.900539 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.company_analysis)
[0m05:12:08.900681 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.data_overview)
[0m05:12:08.900815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_fxf_data, now model.dbt_service.location_analysis)
[0m05:12:08.900945 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.stg_pdl_data, now model.dbt_service.staging_city_state_profiling)
[0m05:12:08.901079 [debug] [Thread-4 (]: Began compiling node model.dbt_service.company_analysis
[0m05:12:08.901203 [debug] [Thread-3 (]: Began compiling node model.dbt_service.data_overview
[0m05:12:08.901321 [debug] [Thread-1 (]: Began compiling node model.dbt_service.location_analysis
[0m05:12:08.901451 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m05:12:08.903066 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m05:12:08.904559 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m05:12:08.906093 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m05:12:08.907354 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m05:12:08.908068 [debug] [Thread-4 (]: Began executing node model.dbt_service.company_analysis
[0m05:12:08.914731 [debug] [Thread-1 (]: Began executing node model.dbt_service.location_analysis
[0m05:12:08.917756 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m05:12:08.917996 [debug] [Thread-3 (]: Began executing node model.dbt_service.data_overview
[0m05:12:08.919657 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m05:12:08.919870 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_city_state_profiling
[0m05:12:08.921332 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m05:12:08.921587 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:08.924023 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m05:12:08.924258 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: BEGIN
[0m05:12:08.924430 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:08.924638 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m05:12:08.924792 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:08.924965 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: BEGIN
[0m05:12:08.925139 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:08.925359 [debug] [Thread-3 (]: On model.dbt_service.data_overview: BEGIN
[0m05:12:08.925497 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:12:08.925706 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m05:12:08.925847 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m05:12:08.926055 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m05:12:08.933712 [debug] [Thread-4 (]: SQL status: BEGIN in 0.009 seconds
[0m05:12:08.933926 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:08.934146 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m05:12:08.937036 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m05:12:08.937191 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:08.937407 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m05:12:08.938665 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m05:12:08.938813 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m05:12:08.938954 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:08.939086 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:08.939246 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from staging layer
-- This model provides a summary of our cleaned data pipeline



select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'combined_staging_data' as table_name,
    'staging' as schema_name,
    (select count(*) from "finny_db"."public_staging"."stg_fxf_data") + (select count(*) from "finny_db"."public_staging"."stg_pdl_data") as row_count,
    'Total cleaned contact records' as description
  );
[0m05:12:08.939447 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources with structured location data



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by city, state
),

state_summary as (
    select
        state,
        count(distinct location) as locations_in_state,
        count(distinct city) as cities_in_state,
        sum(contact_count) as total_contacts_in_state,
        sum(company_count) as total_companies_in_state,
        avg(avg_revenue) as avg_state_revenue
    from location_stats
    where state is not null
    group by state
)

select
    ls.location,
    ls.city,
    ls.state,
    ls.contact_count,
    ls.company_count,
    ls.unique_titles,
    ls.data_sources,
    round(ls.avg_revenue::numeric, 2) as avg_revenue,
    -- Add state-level context
    ss.total_contacts_in_state,
    ss.cities_in_state,
    round(100.0 * ls.contact_count / ss.total_contacts_in_state, 2) as pct_of_state_contacts
from location_stats ls
left join state_summary ss on ls.state = ss.state
order by ls.contact_count desc
  );
  
[0m05:12:08.941536 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "all_contacts.location" must appear in the GROUP BY clause or be used in an aggregate function
LINE 46:         location,
                 ^

[0m05:12:08.941737 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: ROLLBACK
[0m05:12:08.942266 [debug] [Thread-1 (]: On model.dbt_service.location_analysis: Close
[0m05:12:08.942469 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m05:12:08.944572 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:08.944782 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m05:12:08.945427 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:08.946172 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m05:12:08.946354 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:08.946556 [debug] [Thread-3 (]: On model.dbt_service.data_overview: COMMIT
[0m05:12:08.947210 [debug] [Thread-3 (]: SQL status: COMMIT in 0.000 seconds
[0m05:12:08.948291 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m05:12:08.948619 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:08.948766 [debug] [Thread-3 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m05:12:08.949214 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.000 seconds
[0m05:12:08.949883 [debug] [Thread-3 (]: On model.dbt_service.data_overview: Close
[0m05:12:08.950183 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d7af50>]}
[0m05:12:08.950469 [info ] [Thread-3 (]: 4 of 9 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.05s]
[0m05:12:08.950788 [debug] [Thread-3 (]: Finished running node model.dbt_service.data_overview
[0m05:12:08.950979 [debug] [Thread-3 (]: Began running node model.dbt_service.staging_company_profiling
[0m05:12:08.951301 [info ] [Thread-3 (]: 7 of 9 START sql table model public_staging_analysis.staging_company_profiling . [RUN]
[0m05:12:08.951561 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_service.data_overview, now model.dbt_service.staging_company_profiling)
[0m05:12:08.951795 [debug] [Thread-3 (]: Began compiling node model.dbt_service.staging_company_profiling
[0m05:12:08.953557 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m05:12:08.954149 [debug] [Thread-3 (]: Began executing node model.dbt_service.staging_company_profiling
[0m05:12:08.955742 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m05:12:08.956132 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:08.957045 [debug] [Thread-1 (]: Database Error in model location_analysis (models/marts/location_analysis.sql)
  column "all_contacts.location" must appear in the GROUP BY clause or be used in an aggregate function
  LINE 46:         location,
                   ^
  compiled code at target/run/dbt_service/models/marts/location_analysis.sql
[0m05:12:08.957234 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: BEGIN
[0m05:12:08.957489 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d131d0>]}
[0m05:12:08.957660 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m05:12:08.957941 [error] [Thread-1 (]: 5 of 9 ERROR creating sql table model public_marts.location_analysis ........... [[31mERROR[0m in 0.06s]
[0m05:12:08.958244 [debug] [Thread-1 (]: Finished running node model.dbt_service.location_analysis
[0m05:12:08.958411 [debug] [Thread-1 (]: Began running node model.dbt_service.staging_data_profiling
[0m05:12:08.958599 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.location_analysis' to be skipped because of status 'error'.  Reason: Database Error in model location_analysis (models/marts/location_analysis.sql)
  column "all_contacts.location" must appear in the GROUP BY clause or be used in an aggregate function
  LINE 46:         location,
                   ^
  compiled code at target/run/dbt_service/models/marts/location_analysis.sql.
[0m05:12:08.958810 [info ] [Thread-1 (]: 8 of 9 START sql table model public_staging_analysis.staging_data_profiling .... [RUN]
[0m05:12:08.959327 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.location_analysis, now model.dbt_service.staging_data_profiling)
[0m05:12:08.959483 [debug] [Thread-1 (]: Began compiling node model.dbt_service.staging_data_profiling
[0m05:12:08.961190 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m05:12:08.961512 [debug] [Thread-1 (]: Began executing node model.dbt_service.staging_data_profiling
[0m05:12:08.963098 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m05:12:08.963375 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:08.963520 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: BEGIN
[0m05:12:08.963656 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:12:08.964206 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m05:12:08.964354 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:08.964595 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m05:12:08.969548 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m05:12:08.969774 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:08.970018 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m05:12:09.090732 [debug] [Thread-4 (]: SQL status: SELECT 91 in 0.156 seconds
[0m05:12:09.093838 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:09.094021 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.156 seconds
[0m05:12:09.094229 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m05:12:09.095551 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:09.095762 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m05:12:09.096206 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.096426 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.097778 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:09.099802 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:09.099978 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m05:12:09.100134 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m05:12:09.100716 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.100892 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:09.101512 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m05:12:09.102070 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m05:12:09.102224 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:09.102366 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:09.102504 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m05:12:09.102638 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: COMMIT
[0m05:12:09.103831 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:09.104038 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:09.105211 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m05:12:09.106074 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m05:12:09.107447 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.company_analysis"
[0m05:12:09.107612 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.137 seconds
[0m05:12:09.107948 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m05:12:09.108120 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m05:12:09.109398 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:09.109552 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m05:12:09.109737 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m05:12:09.110426 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.111772 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:09.111969 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m05:12:09.112116 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.002 seconds
[0m05:12:09.112252 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m05:12:09.112776 [debug] [Thread-2 (]: On model.dbt_service.staging_city_state_profiling: Close
[0m05:12:09.113229 [debug] [Thread-4 (]: On model.dbt_service.company_analysis: Close
[0m05:12:09.113600 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d11a10>]}
[0m05:12:09.113770 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120827110>]}
[0m05:12:09.113934 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:09.114226 [info ] [Thread-2 (]: 6 of 9 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.21s]
[0m05:12:09.114469 [info ] [Thread-4 (]: 3 of 9 OK created sql table model public_staging_analysis.company_analysis ..... [[32mSELECT 91[0m in 0.21s]
[0m05:12:09.115077 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m05:12:09.115288 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_city_state_profiling
[0m05:12:09.115488 [debug] [Thread-4 (]: Finished running node model.dbt_service.company_analysis
[0m05:12:09.115635 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:09.115795 [debug] [Thread-2 (]: Began running node model.dbt_service.staging_location_profiling
[0m05:12:09.116008 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: COMMIT
[0m05:12:09.116193 [info ] [Thread-2 (]: 9 of 9 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m05:12:09.116391 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.staging_city_state_profiling, now model.dbt_service.staging_location_profiling)
[0m05:12:09.116538 [debug] [Thread-2 (]: Began compiling node model.dbt_service.staging_location_profiling
[0m05:12:09.118211 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m05:12:09.118383 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m05:12:09.119457 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m05:12:09.119776 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m05:12:09.119926 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m05:12:09.120151 [debug] [Thread-2 (]: Began executing node model.dbt_service.staging_location_profiling
[0m05:12:09.121638 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m05:12:09.121826 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m05:12:09.122471 [debug] [Thread-1 (]: On model.dbt_service.staging_data_profiling: Close
[0m05:12:09.122750 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.122899 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12223f590>]}
[0m05:12:09.123059 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: BEGIN
[0m05:12:09.123435 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m05:12:09.123317 [info ] [Thread-1 (]: 8 of 9 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.16s]
[0m05:12:09.123736 [debug] [Thread-1 (]: Finished running node model.dbt_service.staging_data_profiling
[0m05:12:09.128188 [debug] [Thread-3 (]: SQL status: SELECT 180 in 0.163 seconds
[0m05:12:09.129742 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:09.129985 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m05:12:09.130429 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m05:12:09.130617 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.130833 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.132499 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:09.132815 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m05:12:09.133141 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m05:12:09.133809 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.134563 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m05:12:09.134725 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:09.134864 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: COMMIT
[0m05:12:09.135532 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:09.137614 [debug] [Thread-3 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m05:12:09.137989 [debug] [Thread-3 (]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m05:12:09.138151 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m05:12:09.139712 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m05:12:09.140390 [debug] [Thread-3 (]: On model.dbt_service.staging_company_profiling: Close
[0m05:12:09.140686 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222c5810>]}
[0m05:12:09.140975 [info ] [Thread-3 (]: 7 of 9 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.19s]
[0m05:12:09.141201 [debug] [Thread-3 (]: Finished running node model.dbt_service.staging_company_profiling
[0m05:12:09.398833 [debug] [Thread-2 (]: SQL status: SELECT 152 in 0.265 seconds
[0m05:12:09.402024 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.402271 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m05:12:09.402957 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:09.404477 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.404664 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m05:12:09.405218 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:09.405864 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m05:12:09.406123 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.406267 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: COMMIT
[0m05:12:09.407243 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:09.408642 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m05:12:09.409286 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m05:12:09.409624 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m05:12:09.411228 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m05:12:09.411838 [debug] [Thread-2 (]: On model.dbt_service.staging_location_profiling: Close
[0m05:12:09.412146 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cc53f0b-8377-4528-9015-be763280f283', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222c6890>]}
[0m05:12:09.412437 [info ] [Thread-2 (]: 9 of 9 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.30s]
[0m05:12:09.412910 [debug] [Thread-2 (]: Finished running node model.dbt_service.staging_location_profiling
[0m05:12:09.413914 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:09.414323 [debug] [MainThread]: On master: BEGIN
[0m05:12:09.414531 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:12:09.422125 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m05:12:09.422390 [debug] [MainThread]: On master: COMMIT
[0m05:12:09.422537 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:09.422648 [debug] [MainThread]: On master: COMMIT
[0m05:12:09.423015 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:12:09.423167 [debug] [MainThread]: On master: Close
[0m05:12:09.423378 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:12:09.423500 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m05:12:09.423608 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m05:12:09.423793 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m05:12:09.423974 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m05:12:09.424195 [info ] [MainThread]: 
[0m05:12:09.424372 [info ] [MainThread]: Finished running 6 table models, 3 view models in 0 hours 0 minutes and 0.75 seconds (0.75s).
[0m05:12:09.425286 [debug] [MainThread]: Command end result
[0m05:12:09.444683 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:12:09.445908 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:12:09.449651 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:12:09.449828 [info ] [MainThread]: 
[0m05:12:09.450003 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m05:12:09.450145 [info ] [MainThread]: 
[0m05:12:09.450331 [error] [MainThread]: [31mFailure in model location_analysis (models/marts/location_analysis.sql)[0m
[0m05:12:09.450525 [error] [MainThread]:   Database Error in model location_analysis (models/marts/location_analysis.sql)
  column "all_contacts.location" must appear in the GROUP BY clause or be used in an aggregate function
  LINE 46:         location,
                   ^
  compiled code at target/run/dbt_service/models/marts/location_analysis.sql
[0m05:12:09.450671 [info ] [MainThread]: 
[0m05:12:09.450843 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/marts/location_analysis.sql
[0m05:12:09.450993 [info ] [MainThread]: 
[0m05:12:09.451167 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=9
[0m05:12:09.453674 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.4667808, "process_in_blocks": "0", "process_kernel_time": 0.251273, "process_mem_max_rss": "148209664", "process_out_blocks": "0", "process_user_time": 1.345551}
[0m05:12:09.454014 [debug] [MainThread]: Command `dbt run` failed at 05:12:09.453964 after 1.47 seconds
[0m05:12:09.454245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116219350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208ad250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c6fa10>]}
[0m05:12:09.454460 [debug] [MainThread]: Flushing usage events
[0m05:12:09.866175 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:12:45.954787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123ab1cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b37710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b22410>]}


============================== 05:12:45.957763 | 61271b34-d38d-4a52-b945-f1f9c27ecdc2 ==============================
[0m05:12:45.957763 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:12:45.958093 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'cache_selected_only': 'False', 'write_json': 'True', 'log_format': 'default', 'introspect': 'True', 'version_check': 'True', 'target_path': 'None', 'warn_error': 'None', 'quiet': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'printer_width': '80', 'no_print': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select marts'}
[0m05:12:46.057869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123aa7f10>]}
[0m05:12:46.089251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200f4b50>]}
[0m05:12:46.090171 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:12:46.137773 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:12:46.202277 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:12:46.202632 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/location_analysis.sql
[0m05:12:46.427511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124f1cbd0>]}
[0m05:12:46.468126 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:12:46.469598 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:12:46.480952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124d098d0>]}
[0m05:12:46.481218 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:12:46.481371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124cd5ed0>]}
[0m05:12:46.482081 [info ] [MainThread]: 
[0m05:12:46.482227 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:12:46.482340 [info ] [MainThread]: 
[0m05:12:46.482546 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:12:46.484130 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:12:46.509404 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:12:46.509627 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:12:46.509756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:46.540286 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.030 seconds
[0m05:12:46.540956 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:12:46.541822 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m05:12:46.542040 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:12:46.542259 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:12:46.544506 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:12:46.544718 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m05:12:46.545523 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:12:46.546255 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:12:46.546379 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:12:46.547183 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:12:46.547320 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:12:46.547458 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:12:46.547582 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:46.547700 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:12:46.547809 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:46.547908 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:46.548093 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:12:46.561349 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m05:12:46.561523 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:12:46.561658 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:12:46.561762 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:12:46.561913 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:12:46.562041 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:12:46.562150 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:12:46.562259 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:12:46.562384 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:12:46.562523 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:12:46.562666 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:12:46.562805 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:12:46.566015 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m05:12:46.566159 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m05:12:46.566296 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m05:12:46.566395 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m05:12:46.566914 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:12:46.567341 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:12:46.567809 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:12:46.568216 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:12:46.568892 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:12:46.569030 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:12:46.569167 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:12:46.569288 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:12:46.569510 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging)
[0m05:12:46.570146 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_raw)
[0m05:12:46.571867 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:12:46.572589 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:12:46.572710 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:12:46.572820 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:12:46.572926 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:46.573030 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:12:46.582352 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:12:46.582496 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m05:12:46.582625 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:12:46.582742 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:12:46.582881 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:12:46.583022 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:12:46.586007 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:12:46.586138 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:12:46.586593 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:12:46.586978 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:12:46.587516 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:12:46.587641 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:12:46.590543 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.590822 [debug] [MainThread]: On master: BEGIN
[0m05:12:46.590942 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:12:46.598420 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m05:12:46.598624 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.598813 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:12:46.602163 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m05:12:46.603303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124d44110>]}
[0m05:12:46.603542 [debug] [MainThread]: On master: ROLLBACK
[0m05:12:46.604418 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.604534 [debug] [MainThread]: On master: BEGIN
[0m05:12:46.605154 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m05:12:46.605271 [debug] [MainThread]: On master: COMMIT
[0m05:12:46.605378 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.605483 [debug] [MainThread]: On master: COMMIT
[0m05:12:46.605979 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:12:46.606100 [debug] [MainThread]: On master: Close
[0m05:12:46.608186 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m05:12:46.608363 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m05:12:46.608594 [info ] [Thread-1 (]: 1 of 2 START sql view model public_marts.data_overview ......................... [RUN]
[0m05:12:46.608819 [info ] [Thread-2 (]: 2 of 2 START sql table model public_marts.location_analysis .................... [RUN]
[0m05:12:46.609016 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.data_overview)
[0m05:12:46.609221 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.location_analysis)
[0m05:12:46.609400 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m05:12:46.609553 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m05:12:46.613351 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m05:12:46.615812 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m05:12:46.616260 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m05:12:46.622366 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m05:12:46.630344 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m05:12:46.639105 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m05:12:46.639660 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.639870 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.640020 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m05:12:46.640173 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m05:12:46.640339 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m05:12:46.640504 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:12:46.651785 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m05:12:46.651954 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m05:12:46.652129 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.652262 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.652414 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from staging layer
-- This model provides a summary of our cleaned data pipeline



select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'combined_staging_data' as table_name,
    'staging' as schema_name,
    (select count(*) from "finny_db"."public_staging"."stg_fxf_data") + (select count(*) from "finny_db"."public_staging"."stg_pdl_data") as row_count,
    'Total cleaned contact records' as description
  );
[0m05:12:46.652614 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources with structured location data



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

location_stats as (
    select
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where location is not null
    group by location, city, state
),

state_summary as (
    select
        state,
        count(distinct location) as locations_in_state,
        count(distinct city) as cities_in_state,
        sum(contact_count) as total_contacts_in_state,
        sum(company_count) as total_companies_in_state,
        avg(avg_revenue) as avg_state_revenue
    from location_stats
    where state is not null
    group by state
)

select
    ls.location,
    ls.city,
    ls.state,
    ls.contact_count,
    ls.company_count,
    ls.unique_titles,
    ls.data_sources,
    round(ls.avg_revenue::numeric, 2) as avg_revenue,
    -- Add state-level context
    ss.total_contacts_in_state,
    ss.cities_in_state,
    round(100.0 * ls.contact_count / ss.total_contacts_in_state, 2) as pct_of_state_contacts
from location_stats ls
left join state_summary ss on ls.state = ss.state
order by ls.contact_count desc
  );
  
[0m05:12:46.655310 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m05:12:46.658254 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.658400 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview" rename to "data_overview__dbt_backup"
[0m05:12:46.660780 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:12:46.661942 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.662072 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m05:12:46.663310 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:12:46.669440 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m05:12:46.669619 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.669747 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m05:12:46.670579 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:46.673427 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m05:12:46.675193 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:12:46.675344 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m05:12:46.676642 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:12:46.677895 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m05:12:46.678903 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124e11650>]}
[0m05:12:46.679219 [info ] [Thread-1 (]: 1 of 2 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.07s]
[0m05:12:46.679455 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m05:12:46.803987 [debug] [Thread-2 (]: SQL status: SELECT 76 in 0.151 seconds
[0m05:12:46.808630 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.808933 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m05:12:46.809644 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:46.810750 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.810918 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m05:12:46.811470 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:12:46.812013 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m05:12:46.812145 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.812268 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m05:12:46.813008 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m05:12:46.815151 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m05:12:46.817124 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:12:46.817380 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m05:12:46.818565 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m05:12:46.819070 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m05:12:46.819333 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61271b34-d38d-4a52-b945-f1f9c27ecdc2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125f3fe90>]}
[0m05:12:46.819609 [info ] [Thread-2 (]: 2 of 2 OK created sql table model public_marts.location_analysis ............... [[32mSELECT 76[0m in 0.21s]
[0m05:12:46.819833 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m05:12:46.820408 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.820571 [debug] [MainThread]: On master: BEGIN
[0m05:12:46.820691 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:12:46.826780 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m05:12:46.827000 [debug] [MainThread]: On master: COMMIT
[0m05:12:46.827144 [debug] [MainThread]: Using postgres connection "master"
[0m05:12:46.827257 [debug] [MainThread]: On master: COMMIT
[0m05:12:46.827599 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:12:46.827734 [debug] [MainThread]: On master: Close
[0m05:12:46.827917 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:12:46.828030 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m05:12:46.828136 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m05:12:46.828239 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m05:12:46.828337 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m05:12:46.828462 [info ] [MainThread]: 
[0m05:12:46.828603 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.35 seconds (0.35s).
[0m05:12:46.828917 [debug] [MainThread]: Command end result
[0m05:12:46.845644 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:12:46.846527 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:12:46.850046 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:12:46.850312 [info ] [MainThread]: 
[0m05:12:46.850533 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:12:46.850687 [info ] [MainThread]: 
[0m05:12:46.850859 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m05:12:46.853307 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.93671066, "process_in_blocks": "0", "process_kernel_time": 0.231346, "process_mem_max_rss": "143884288", "process_out_blocks": "0", "process_user_time": 1.226457}
[0m05:12:46.853641 [debug] [MainThread]: Command `dbt run` succeeded at 05:12:46.853588 after 0.94 seconds
[0m05:12:46.853851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123ab2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c74350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caf8d0>]}
[0m05:12:46.854062 [debug] [MainThread]: Flushing usage events
[0m05:12:47.083722 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:17:01.071524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110b2210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11112f7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11112fed0>]}


============================== 05:17:01.075083 | 47ab7362-893b-4580-8b31-dada097e7716 ==============================
[0m05:17:01.075083 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:17:01.075463 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'no_print': 'None', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select marts', 'empty': 'False', 'write_json': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'fail_fast': 'False', 'warn_error': 'None', 'printer_width': '80', 'use_colors': 'True', 'version_check': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m05:17:01.181213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a13810>]}
[0m05:17:01.213552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ef1650>]}
[0m05:17:01.214603 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:17:01.261905 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:17:01.328704 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:17:01.329085 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/location_analysis.sql
[0m05:17:01.577872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10750f690>]}
[0m05:17:01.620725 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:17:01.622097 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:17:01.635156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110f7b10>]}
[0m05:17:01.635416 [info ] [MainThread]: Found 14 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:17:01.635569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11122c210>]}
[0m05:17:01.636288 [info ] [MainThread]: 
[0m05:17:01.636425 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:17:01.636539 [info ] [MainThread]: 
[0m05:17:01.636719 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:17:01.638253 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:17:01.665819 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:17:01.666134 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:17:01.666254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:17:01.698726 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.032 seconds
[0m05:17:01.699514 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:17:01.700531 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m05:17:01.700823 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m05:17:01.701090 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:17:01.704038 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:17:01.704272 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:17:01.705238 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:17:01.706080 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:17:01.706229 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:17:01.707090 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:17:01.707256 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:17:01.707392 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:17:01.707549 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:17:01.707701 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:17:01.707817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:17:01.707933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:17:01.708149 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:17:01.722413 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:17:01.722606 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:17:01.722735 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:17:01.722830 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:17:01.722992 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:17:01.723112 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:17:01.723215 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:17:01.723316 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:17:01.723450 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:17:01.723586 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:17:01.723732 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:17:01.723870 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:17:01.726752 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m05:17:01.726884 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m05:17:01.727006 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:17:01.727550 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:17:01.728135 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:17:01.728667 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:17:01.729727 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:17:01.729865 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.006 seconds
[0m05:17:01.729979 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:17:01.730089 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:17:01.730578 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:17:01.730776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m05:17:01.731135 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_raw)
[0m05:17:01.732611 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:17:01.732723 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:17:01.733455 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:17:01.733607 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:17:01.734135 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:17:01.734256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:17:01.734360 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:17:01.742759 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m05:17:01.742948 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:17:01.743158 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:17:01.743365 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:17:01.743522 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:17:01.743675 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:17:01.745568 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m05:17:01.745709 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m05:17:01.746258 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:17:01.746635 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:17:01.747396 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:17:01.747513 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:17:01.750413 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.750577 [debug] [MainThread]: On master: BEGIN
[0m05:17:01.750678 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:17:01.757734 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m05:17:01.757871 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.758024 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:17:01.759922 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m05:17:01.761108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114f29450>]}
[0m05:17:01.761319 [debug] [MainThread]: On master: ROLLBACK
[0m05:17:01.761724 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.761882 [debug] [MainThread]: On master: BEGIN
[0m05:17:01.762362 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:17:01.762493 [debug] [MainThread]: On master: COMMIT
[0m05:17:01.762605 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.762734 [debug] [MainThread]: On master: COMMIT
[0m05:17:01.763115 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:17:01.763264 [debug] [MainThread]: On master: Close
[0m05:17:01.764946 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m05:17:01.765162 [debug] [Thread-2 (]: Began running node model.dbt_service.location_analysis
[0m05:17:01.765439 [info ] [Thread-1 (]: 1 of 2 START sql view model public_marts.data_overview ......................... [RUN]
[0m05:17:01.765731 [info ] [Thread-2 (]: 2 of 2 START sql table model public_marts.location_analysis .................... [RUN]
[0m05:17:01.765952 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.data_overview)
[0m05:17:01.766102 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.location_analysis)
[0m05:17:01.766256 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m05:17:01.766408 [debug] [Thread-2 (]: Began compiling node model.dbt_service.location_analysis
[0m05:17:01.770079 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m05:17:01.772485 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.location_analysis"
[0m05:17:01.773003 [debug] [Thread-2 (]: Began executing node model.dbt_service.location_analysis
[0m05:17:01.779315 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m05:17:01.793553 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.location_analysis"
[0m05:17:01.795650 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m05:17:01.796178 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.796343 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.796481 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m05:17:01.796621 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: BEGIN
[0m05:17:01.796761 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:17:01.796904 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m05:17:01.807503 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m05:17:01.807728 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m05:17:01.807937 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.808094 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.808286 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from staging layer
-- This model provides a summary of our cleaned data pipeline



select
    'stg_fxf_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned FXF contact data' as description
from "finny_db"."public_staging"."stg_fxf_data"

union all

select
    'stg_pdl_data' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Cleaned PDL contact data' as description
from "finny_db"."public_staging"."stg_pdl_data"

union all

select
    'combined_staging_data' as table_name,
    'staging' as schema_name,
    (select count(*) from "finny_db"."public_staging"."stg_fxf_data") + (select count(*) from "finny_db"."public_staging"."stg_pdl_data") as row_count,
    'Total cleaned contact records' as description
  );
[0m05:17:01.808513 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */

  
    

  create  table "finny_db"."public_marts"."location_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Location analysis across both data sources with structured city and state data



with all_contacts as (
    select
        'fxf' as data_source,
        name,
        email,
        company,
        title,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
    
    union all
    
    select
        'pdl' as data_source,
        name,
        email,
        company,
        title,
        city,
        state,
        company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
),

city_state_stats as (
    select
        city,
        state,
        count(*) as contact_count,
        count(distinct company) as company_count,
        count(distinct title) as unique_titles,
        array_agg(distinct data_source) as data_sources,
        avg(company_revenue) as avg_revenue
    from all_contacts
    where city is not null and state is not null
    group by city, state
),

state_summary as (
    select
        state,
        count(distinct city) as cities_in_state,
        sum(contact_count) as total_contacts_in_state,
        sum(company_count) as total_companies_in_state,
        avg(avg_revenue) as avg_state_revenue
    from city_state_stats
    where state is not null
    group by state
)

select
    cs.city,
    cs.state,
    cs.contact_count,
    cs.company_count,
    cs.unique_titles,
    cs.data_sources,
    round(cs.avg_revenue::numeric, 2) as avg_revenue,
    -- Add state-level context
    ss.total_contacts_in_state,
    ss.cities_in_state,
    round(100.0 * cs.contact_count / ss.total_contacts_in_state, 2) as pct_of_state_contacts
from city_state_stats cs
left join state_summary ss on cs.state = ss.state
order by cs.contact_count desc
  );
  
[0m05:17:01.811214 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m05:17:01.814025 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.814202 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview" rename to "data_overview__dbt_backup"
[0m05:17:01.815240 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:17:01.816989 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.817135 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m05:17:01.817875 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:17:01.823850 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m05:17:01.824047 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.824178 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m05:17:01.825517 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m05:17:01.828265 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m05:17:01.830180 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m05:17:01.830327 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m05:17:01.831652 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:17:01.832850 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m05:17:01.833808 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114fefe50>]}
[0m05:17:01.834123 [info ] [Thread-1 (]: 1 of 2 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.07s]
[0m05:17:01.834368 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m05:17:01.955750 [debug] [Thread-2 (]: SQL status: SELECT 22 in 0.147 seconds
[0m05:17:01.959841 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.960113 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis" rename to "location_analysis__dbt_backup"
[0m05:17:01.961013 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:17:01.962500 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.962695 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
alter table "finny_db"."public_marts"."location_analysis__dbt_tmp" rename to "location_analysis"
[0m05:17:01.963195 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:17:01.963738 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m05:17:01.963913 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.964083 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: COMMIT
[0m05:17:01.964784 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m05:17:01.966965 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_marts"."location_analysis__dbt_backup"
[0m05:17:01.968492 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.location_analysis"
[0m05:17:01.968680 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.location_analysis"} */
drop table if exists "finny_db"."public_marts"."location_analysis__dbt_backup" cascade
[0m05:17:01.969986 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m05:17:01.970633 [debug] [Thread-2 (]: On model.dbt_service.location_analysis: Close
[0m05:17:01.970901 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ab7362-893b-4580-8b31-dada097e7716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11583eb10>]}
[0m05:17:01.971191 [info ] [Thread-2 (]: 2 of 2 OK created sql table model public_marts.location_analysis ............... [[32mSELECT 22[0m in 0.20s]
[0m05:17:01.971425 [debug] [Thread-2 (]: Finished running node model.dbt_service.location_analysis
[0m05:17:01.972038 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.972217 [debug] [MainThread]: On master: BEGIN
[0m05:17:01.972346 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:17:01.978550 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m05:17:01.978742 [debug] [MainThread]: On master: COMMIT
[0m05:17:01.978880 [debug] [MainThread]: Using postgres connection "master"
[0m05:17:01.978992 [debug] [MainThread]: On master: COMMIT
[0m05:17:01.979342 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:17:01.979519 [debug] [MainThread]: On master: Close
[0m05:17:01.979728 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:17:01.979853 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m05:17:01.979963 [debug] [MainThread]: Connection 'model.dbt_service.location_analysis' was properly closed.
[0m05:17:01.980063 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m05:17:01.980161 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m05:17:01.980293 [info ] [MainThread]: 
[0m05:17:01.980438 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.34 seconds (0.34s).
[0m05:17:01.980754 [debug] [MainThread]: Command end result
[0m05:17:01.995447 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:17:01.996305 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:17:01.998954 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:17:01.999090 [info ] [MainThread]: 
[0m05:17:01.999269 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:17:01.999396 [info ] [MainThread]: 
[0m05:17:01.999535 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m05:17:02.001898 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9696822, "process_in_blocks": "0", "process_kernel_time": 0.230982, "process_mem_max_rss": "145408000", "process_out_blocks": "0", "process_user_time": 1.25611}
[0m05:17:02.002227 [debug] [MainThread]: Command `dbt run` succeeded at 05:17:02.002178 after 0.97 seconds
[0m05:17:02.002434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11115e050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a90350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102acb8d0>]}
[0m05:17:02.002625 [debug] [MainThread]: Flushing usage events
[0m05:17:02.297181 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:29:14.501552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061eb510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115ab790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11159dfd0>]}


============================== 05:29:14.504266 | 75ed7ebc-a6f3-4f35-bb92-a0b925905922 ==============================
[0m05:29:14.504266 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:29:14.504576 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'static_parser': 'True', 'fail_fast': 'False', 'quiet': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'debug': 'False', 'printer_width': '80', 'empty': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_unified_prospects'}
[0m05:29:14.631588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115fe550>]}
[0m05:29:14.661048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f2a50>]}
[0m05:29:14.661633 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:29:14.708006 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:29:14.771233 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m05:29:14.771532 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging/stg_unified_prospects.sql
[0m05:29:14.896446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11289ea10>]}
[0m05:29:14.959752 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:29:14.960723 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:29:14.972969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072778d0>]}
[0m05:29:14.973203 [info ] [MainThread]: Found 15 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:29:14.973361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b17f10>]}
[0m05:29:14.974030 [info ] [MainThread]: 
[0m05:29:14.974176 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:29:14.974290 [info ] [MainThread]: 
[0m05:29:14.974485 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:29:14.974849 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:29:15.000735 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:29:15.000971 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:29:15.001086 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:29:15.028099 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.027 seconds
[0m05:29:15.028681 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:29:15.030978 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m05:29:15.031217 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m05:29:15.031436 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m05:29:15.033891 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:29:15.034086 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:29:15.034977 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:29:15.035723 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:29:15.035853 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:29:15.036823 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:29:15.036938 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:29:15.037043 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:29:15.037145 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:29:15.037249 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:29:15.037347 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:29:15.037444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:29:15.037630 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:29:15.048066 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:29:15.048221 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:29:15.048384 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:29:15.048623 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:29:15.048828 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:29:15.048961 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:29:15.049091 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:29:15.049228 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:29:15.049363 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:29:15.049481 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:29:15.049627 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:29:15.049849 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:29:15.053085 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m05:29:15.053294 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m05:29:15.053517 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m05:29:15.053731 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m05:29:15.054386 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:29:15.054837 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:29:15.055239 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:29:15.055606 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:29:15.056051 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:29:15.056211 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:29:15.056352 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:29:15.056581 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_staging)
[0m05:29:15.057108 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:29:15.057341 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw)
[0m05:29:15.059017 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:29:15.059926 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:29:15.060333 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:29:15.060472 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:29:15.060606 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:29:15.060736 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:29:15.068364 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m05:29:15.068541 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m05:29:15.068688 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:29:15.068797 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:29:15.068924 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:29:15.069059 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:29:15.070964 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m05:29:15.071126 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m05:29:15.071628 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:29:15.072009 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:29:15.072353 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:29:15.072505 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:29:15.075292 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.075432 [debug] [MainThread]: On master: BEGIN
[0m05:29:15.075530 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:29:15.080885 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m05:29:15.081013 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.081160 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:29:15.083362 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m05:29:15.084349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072761d0>]}
[0m05:29:15.084546 [debug] [MainThread]: On master: ROLLBACK
[0m05:29:15.084867 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.084972 [debug] [MainThread]: On master: BEGIN
[0m05:29:15.085464 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:29:15.085606 [debug] [MainThread]: On master: COMMIT
[0m05:29:15.085705 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.085804 [debug] [MainThread]: On master: COMMIT
[0m05:29:15.086114 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:29:15.086235 [debug] [MainThread]: On master: Close
[0m05:29:15.088410 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_unified_prospects
[0m05:29:15.088675 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_unified_prospects .............. [RUN]
[0m05:29:15.088891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_unified_prospects)
[0m05:29:15.089128 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_unified_prospects
[0m05:29:15.092162 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m05:29:15.092462 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_unified_prospects
[0m05:29:15.108277 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m05:29:15.108890 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:29:15.109071 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m05:29:15.109222 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:29:15.117168 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m05:29:15.117360 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:29:15.117533 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m05:29:15.364480 [debug] [Thread-1 (]: SQL status: SELECT 100010 in 0.246 seconds
[0m05:29:15.377184 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:29:15.377622 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m05:29:15.378457 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:29:15.389091 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m05:29:15.389420 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:29:15.389663 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m05:29:15.394251 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m05:29:15.398008 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m05:29:15.401113 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:29:15.401373 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m05:29:15.401993 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m05:29:15.403322 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: Close
[0m05:29:15.404622 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75ed7ebc-a6f3-4f35-bb92-a0b925905922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129f7cd0>]}
[0m05:29:15.405000 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_unified_prospects ......... [[32mSELECT 100010[0m in 0.31s]
[0m05:29:15.405284 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_unified_prospects
[0m05:29:15.406050 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.406295 [debug] [MainThread]: On master: BEGIN
[0m05:29:15.406477 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:29:15.414044 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m05:29:15.414233 [debug] [MainThread]: On master: COMMIT
[0m05:29:15.414377 [debug] [MainThread]: Using postgres connection "master"
[0m05:29:15.414507 [debug] [MainThread]: On master: COMMIT
[0m05:29:15.414891 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:29:15.415030 [debug] [MainThread]: On master: Close
[0m05:29:15.415225 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:29:15.415357 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m05:29:15.415474 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m05:29:15.415590 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m05:29:15.415701 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m05:29:15.415843 [info ] [MainThread]: 
[0m05:29:15.416005 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m05:29:15.416319 [debug] [MainThread]: Command end result
[0m05:29:15.434994 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:29:15.436272 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:29:15.439714 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:29:15.439891 [info ] [MainThread]: 
[0m05:29:15.440110 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:29:15.440260 [info ] [MainThread]: 
[0m05:29:15.440418 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m05:29:15.442908 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.97924924, "process_in_blocks": "0", "process_kernel_time": 0.219685, "process_mem_max_rss": "140361728", "process_out_blocks": "0", "process_user_time": 1.07861}
[0m05:29:15.443187 [debug] [MainThread]: Command `dbt run` succeeded at 05:29:15.443144 after 0.98 seconds
[0m05:29:15.443378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11159f950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11152a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ce76d0>]}
[0m05:29:15.443566 [debug] [MainThread]: Flushing usage events
[0m05:29:15.868321 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:52:19.325052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1139b1250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a33550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a25fd0>]}


============================== 05:52:19.327538 | c10433b8-8b36-4637-a909-175dcb83aaff ==============================
[0m05:52:19.327538 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:52:19.327868 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'debug': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'printer_width': '80', 'invocation_command': 'dbt run --select stg_unified_prospects', 'static_parser': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'fail_fast': 'False'}
[0m05:52:19.424466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a24cd0>]}
[0m05:52:19.454211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109f1f50>]}
[0m05:52:19.454985 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:52:19.503724 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:52:19.566174 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m05:52:19.566448 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging/stg_prospect_matches.sql
[0m05:52:19.566602 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_unified_prospects.sql
[0m05:52:19.692086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a67150>]}
[0m05:52:19.755069 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:52:19.756101 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:52:19.766889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11413db90>]}
[0m05:52:19.767115 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:52:19.767265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b41250>]}
[0m05:52:19.767938 [info ] [MainThread]: 
[0m05:52:19.768078 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:52:19.768189 [info ] [MainThread]: 
[0m05:52:19.768373 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:52:19.768721 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:52:19.794127 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:52:19.794362 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:52:19.794486 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:19.828339 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.034 seconds
[0m05:52:19.829045 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:52:19.831348 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m05:52:19.831593 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:52:19.831829 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m05:52:19.834244 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:19.834458 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:52:19.835324 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:19.836276 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:19.836406 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:52:19.837094 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:19.837214 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:52:19.837317 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:52:19.837418 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:19.837522 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:52:19.837618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:19.837722 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:19.837925 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:19.848106 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:52:19.848281 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:19.848433 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:52:19.851892 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:52:19.852098 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:52:19.852298 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:19.852538 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:19.852707 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m05:52:19.852846 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:19.852980 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:19.853122 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:52:19.853723 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:52:19.853871 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:52:19.854015 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:52:19.854527 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:52:19.854837 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_marts)
[0m05:52:19.855970 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:19.856114 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:52:19.856277 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m05:52:19.856454 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:19.856579 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m05:52:19.857047 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:52:19.857174 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:52:19.857667 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:52:19.858161 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:52:19.858301 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:52:19.858611 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_raw_analysis)
[0m05:52:19.858753 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:52:19.859940 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:19.860049 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:52:19.860250 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:52:19.860915 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:19.865831 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:52:19.866003 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:19.866124 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:52:19.868072 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m05:52:19.868630 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:52:19.869041 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:52:19.869480 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:52:19.869599 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:19.869721 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:52:19.873024 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m05:52:19.873499 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:52:19.873845 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:52:19.876456 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:19.876606 [debug] [MainThread]: On master: BEGIN
[0m05:52:19.876706 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:52:19.882347 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m05:52:19.882534 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:19.882723 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:52:19.885911 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m05:52:19.887029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a64550>]}
[0m05:52:19.887256 [debug] [MainThread]: On master: ROLLBACK
[0m05:52:19.887635 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:19.887782 [debug] [MainThread]: On master: BEGIN
[0m05:52:19.888346 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:52:19.888480 [debug] [MainThread]: On master: COMMIT
[0m05:52:19.888589 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:19.888694 [debug] [MainThread]: On master: COMMIT
[0m05:52:19.889033 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:52:19.889177 [debug] [MainThread]: On master: Close
[0m05:52:19.891308 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_unified_prospects
[0m05:52:19.891520 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_unified_prospects .............. [RUN]
[0m05:52:19.891810 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_unified_prospects)
[0m05:52:19.891959 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_unified_prospects
[0m05:52:19.895036 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m05:52:19.895380 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_unified_prospects
[0m05:52:19.911363 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m05:52:19.911853 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:19.912006 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m05:52:19.912134 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:19.917923 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m05:52:19.918088 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:19.918263 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m05:52:20.162692 [debug] [Thread-1 (]: SQL status: SELECT 100010 in 0.244 seconds
[0m05:52:20.177006 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:20.177316 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m05:52:20.177999 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:52:20.179589 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:20.179913 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m05:52:20.180514 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m05:52:20.193258 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m05:52:20.193446 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:20.193591 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m05:52:20.199077 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m05:52:20.202893 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m05:52:20.205890 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m05:52:20.206118 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m05:52:20.209690 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m05:52:20.211213 [debug] [Thread-1 (]: On model.dbt_service.stg_unified_prospects: Close
[0m05:52:20.212576 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c10433b8-8b36-4637-a909-175dcb83aaff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d7ce10>]}
[0m05:52:20.213028 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_unified_prospects ......... [[32mSELECT 100010[0m in 0.32s]
[0m05:52:20.213317 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_unified_prospects
[0m05:52:20.214221 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:20.214419 [debug] [MainThread]: On master: BEGIN
[0m05:52:20.214551 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:52:20.221474 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m05:52:20.221684 [debug] [MainThread]: On master: COMMIT
[0m05:52:20.221851 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:20.221990 [debug] [MainThread]: On master: COMMIT
[0m05:52:20.222399 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:52:20.222576 [debug] [MainThread]: On master: Close
[0m05:52:20.222792 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:52:20.222943 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m05:52:20.223066 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m05:52:20.223184 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m05:52:20.223296 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m05:52:20.223461 [info ] [MainThread]: 
[0m05:52:20.223631 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.46 seconds (0.46s).
[0m05:52:20.223964 [debug] [MainThread]: Command end result
[0m05:52:20.242547 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:52:20.244066 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:52:20.247356 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:52:20.247523 [info ] [MainThread]: 
[0m05:52:20.247739 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:52:20.247886 [info ] [MainThread]: 
[0m05:52:20.248052 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m05:52:20.250528 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9609122, "process_in_blocks": "0", "process_kernel_time": 0.208965, "process_mem_max_rss": "140591104", "process_out_blocks": "0", "process_user_time": 1.072573}
[0m05:52:20.250827 [debug] [MainThread]: Command `dbt run` succeeded at 05:52:20.250779 after 0.96 seconds
[0m05:52:20.251048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b34250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a33810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c029d0>]}
[0m05:52:20.251237 [debug] [MainThread]: Flushing usage events
[0m05:52:20.586955 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:52:36.839324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba29150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba9e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10baabcd0>]}


============================== 05:52:36.841832 | 7194774b-956b-4f0d-af13-c310bc3fa85d ==============================
[0m05:52:36.841832 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:52:36.842120 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'target_path': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --select stg_prospect_matches', 'log_cache_events': 'False', 'warn_error': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'printer_width': '80', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'empty': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'partial_parse': 'True', 'introspect': 'True'}
[0m05:52:36.939471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a283210>]}
[0m05:52:36.969017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104df1090>]}
[0m05:52:36.969828 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:52:37.015865 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:52:37.080508 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:52:37.080765 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:52:37.104195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c85c310>]}
[0m05:52:37.143540 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:52:37.144616 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:52:37.155366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb9ffd0>]}
[0m05:52:37.155601 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:52:37.155759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cadd710>]}
[0m05:52:37.156514 [info ] [MainThread]: 
[0m05:52:37.156666 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:52:37.156780 [info ] [MainThread]: 
[0m05:52:37.156996 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:52:37.157367 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:52:37.205808 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:52:37.206063 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:52:37.206190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:37.237751 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.032 seconds
[0m05:52:37.238430 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:52:37.240768 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m05:52:37.241039 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:52:37.243397 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:37.243626 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:52:37.243908 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m05:52:37.244735 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:37.244863 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:52:37.245686 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:37.246430 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:37.246580 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:52:37.246717 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:37.246835 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:52:37.246935 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:52:37.247034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:37.247212 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:37.247334 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:37.261370 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:37.261512 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:52:37.261637 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:37.261748 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m05:52:37.261850 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:37.261965 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:37.262067 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:37.262165 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:37.262290 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:52:37.262446 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:52:37.262577 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:52:37.262711 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:52:37.267236 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m05:52:37.267763 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:52:37.267894 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.005 seconds
[0m05:52:37.268038 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.005 seconds
[0m05:52:37.268138 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m05:52:37.268583 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:52:37.268704 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:52:37.269131 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:52:37.269498 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:52:37.269742 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging)
[0m05:52:37.270810 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:37.270939 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:52:37.271046 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:52:37.271140 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:52:37.271235 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:52:37.271432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_raw)
[0m05:52:37.272502 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:37.273740 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:37.273922 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:52:37.274027 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:37.281685 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:52:37.281803 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:37.281928 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:52:37.282869 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:52:37.282965 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:37.283073 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:52:37.284746 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m05:52:37.285154 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:52:37.285664 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:52:37.285807 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:52:37.286276 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:52:37.286900 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:52:37.289336 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.289479 [debug] [MainThread]: On master: BEGIN
[0m05:52:37.289579 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:52:37.295451 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m05:52:37.295645 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.295841 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:52:37.299813 [debug] [MainThread]: SQL status: SELECT 8 in 0.004 seconds
[0m05:52:37.300859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0bfe90>]}
[0m05:52:37.301060 [debug] [MainThread]: On master: ROLLBACK
[0m05:52:37.301423 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.301533 [debug] [MainThread]: On master: BEGIN
[0m05:52:37.302102 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:52:37.302303 [debug] [MainThread]: On master: COMMIT
[0m05:52:37.302436 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.302565 [debug] [MainThread]: On master: COMMIT
[0m05:52:37.302960 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:52:37.303145 [debug] [MainThread]: On master: Close
[0m05:52:37.304769 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m05:52:37.305051 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m05:52:37.305409 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m05:52:37.305582 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m05:52:37.308864 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m05:52:37.309258 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m05:52:37.330082 [debug] [Thread-1 (]: Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter
[0m05:52:37.331108 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7194774b-956b-4f0d-af13-c310bc3fa85d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb2f1d0>]}
[0m05:52:37.331463 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model public_staging.stg_prospect_matches  [[31mERROR[0m in 0.03s]
[0m05:52:37.331712 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m05:52:37.331938 [debug] [Thread-7 (]: Marking all children of 'model.dbt_service.stg_prospect_matches' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter.
[0m05:52:37.332823 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.332955 [debug] [MainThread]: On master: BEGIN
[0m05:52:37.333057 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:52:37.341035 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m05:52:37.341161 [debug] [MainThread]: On master: COMMIT
[0m05:52:37.341264 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:37.341362 [debug] [MainThread]: On master: COMMIT
[0m05:52:37.341875 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:52:37.341981 [debug] [MainThread]: On master: Close
[0m05:52:37.342119 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:52:37.342215 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m05:52:37.342305 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m05:52:37.342394 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m05:52:37.342479 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m05:52:37.342591 [info ] [MainThread]: 
[0m05:52:37.342730 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m05:52:37.342988 [debug] [MainThread]: Command end result
[0m05:52:37.355366 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:52:37.356132 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:52:37.358312 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m05:52:37.358430 [info ] [MainThread]: 
[0m05:52:37.358578 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m05:52:37.358693 [info ] [MainThread]: 
[0m05:52:37.358832 [error] [MainThread]: [31mFailure in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)[0m
[0m05:52:37.358964 [error] [MainThread]:   Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter
[0m05:52:37.359067 [info ] [MainThread]: 
[0m05:52:37.359187 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_prospect_matches.sql
[0m05:52:37.359290 [info ] [MainThread]: 
[0m05:52:37.359405 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m05:52:37.361639 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.55700845, "process_in_blocks": "0", "process_kernel_time": 0.193028, "process_mem_max_rss": "134627328", "process_out_blocks": "0", "process_user_time": 0.970446}
[0m05:52:37.362017 [debug] [MainThread]: Command `dbt run` failed at 05:52:37.361971 after 0.56 seconds
[0m05:52:37.362211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba2a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1022cc250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba1fc10>]}
[0m05:52:37.362388 [debug] [MainThread]: Flushing usage events
[0m05:52:37.583077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:52:52.572947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11982b590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1198a76d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1198a7e10>]}


============================== 05:52:52.575480 | 52e9df2c-697a-435d-a7ec-36634592231e ==============================
[0m05:52:52.575480 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:52:52.575823 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'static_parser': 'True', 'empty': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'printer_width': '80', 'quiet': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'no_print': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches'}
[0m05:52:52.669536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d61450>]}
[0m05:52:52.698809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105924050>]}
[0m05:52:52.699549 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:52:52.749460 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:52:52.810941 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:52:52.811266 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m05:52:52.935751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a633c90>]}
[0m05:52:52.999723 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:52:53.000770 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:52:53.011441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e24e90>]}
[0m05:52:53.011672 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:52:53.011831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11981f750>]}
[0m05:52:53.012531 [info ] [MainThread]: 
[0m05:52:53.012681 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:52:53.012801 [info ] [MainThread]: 
[0m05:52:53.013040 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:52:53.013449 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:52:53.034601 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:52:53.034798 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:52:53.034917 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:53.064428 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.029 seconds
[0m05:52:53.064945 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:52:53.066999 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m05:52:53.067218 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m05:52:53.067428 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m05:52:53.069975 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:53.070180 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:52:53.071072 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:53.072032 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:53.072156 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:52:53.072882 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:53.073017 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:52:53.073134 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:52:53.073258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:53.073369 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:52:53.073469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:53.073579 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:53.073760 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:53.085881 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m05:52:53.085998 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:52:53.086112 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:52:53.088026 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:53.088137 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:52:53.088279 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:52:53.088425 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:53.088552 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m05:52:53.088682 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:52:53.088793 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:52:53.088931 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:52:53.089069 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:52:53.089833 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m05:52:53.090328 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:52:53.091559 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:52:53.091754 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m05:52:53.091903 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m05:52:53.092016 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m05:52:53.092429 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m05:52:53.093208 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:53.093693 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:52:53.094079 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:52:53.094466 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:52:53.094587 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:52:53.094803 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:53.095068 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:52:53.095211 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:52:53.095332 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:52:53.095589 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw_analysis)
[0m05:52:53.097362 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:53.097572 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:52:53.097707 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:52:53.100611 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m05:52:53.100800 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:52:53.100930 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:52:53.102428 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m05:52:53.103066 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:52:53.103495 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:52:53.103786 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m05:52:53.104183 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:52:53.104406 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:52:53.106001 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m05:52:53.106453 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:52:53.106811 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:52:53.109330 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:53.109479 [debug] [MainThread]: On master: BEGIN
[0m05:52:53.109586 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:52:53.117066 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m05:52:53.117254 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:53.117413 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:52:53.119316 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m05:52:53.120253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52e9df2c-697a-435d-a7ec-36634592231e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af53510>]}
[0m05:52:53.120453 [debug] [MainThread]: On master: ROLLBACK
[0m05:52:53.120841 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:53.120973 [debug] [MainThread]: On master: BEGIN
[0m05:52:53.121571 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:52:53.121704 [debug] [MainThread]: On master: COMMIT
[0m05:52:53.121813 [debug] [MainThread]: Using postgres connection "master"
[0m05:52:53.121917 [debug] [MainThread]: On master: COMMIT
[0m05:52:53.122255 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:52:53.122384 [debug] [MainThread]: On master: Close
[0m05:52:53.124475 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m05:52:53.124694 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m05:52:53.124987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_prospect_matches)
[0m05:52:53.125130 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m05:52:53.128177 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m05:52:53.128488 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m05:52:53.149696 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m05:52:53.150185 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:52:53.150329 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m05:52:53.150451 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:53.159408 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m05:52:53.159569 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:52:53.159725 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.7
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
  
[0m05:55:15.953227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a2b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a6f9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aa7fd0>]}


============================== 05:55:15.955663 | 7e7b465d-b412-4ef9-8a2f-664a4ea4a611 ==============================
[0m05:55:15.955663 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:55:15.955988 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'quiet': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches', 'indirect_selection': 'eager', 'partial_parse': 'True', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'empty': 'False', 'write_json': 'True', 'static_parser': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m05:55:16.049916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ad4150>]}
[0m05:55:16.082328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047f6090>]}
[0m05:55:16.083255 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:55:16.130476 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:55:16.202083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:55:16.202414 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:55:16.226589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bb6b10>]}
[0m05:55:16.267549 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:55:16.268848 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:55:16.280877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109f1e90>]}
[0m05:55:16.281216 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:55:16.281398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11011e6d0>]}
[0m05:55:16.282331 [info ] [MainThread]: 
[0m05:55:16.282494 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:55:16.282611 [info ] [MainThread]: 
[0m05:55:16.282858 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:55:16.283291 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:55:16.336848 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:55:16.337141 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:55:16.337289 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:55:16.354230 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.017 seconds
[0m05:55:16.354951 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:55:16.357234 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m05:55:16.357500 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m05:55:16.359854 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:55:16.360067 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m05:55:16.360339 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m05:55:16.361187 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:55:16.361328 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:55:16.362135 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:55:16.362933 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:55:16.363075 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:55:16.363202 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:55:16.363319 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:55:16.363431 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:55:16.363547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:55:16.363772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:55:16.363939 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:55:16.374846 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m05:55:16.375069 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m05:55:16.375308 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m05:55:16.375449 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m05:55:16.375567 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:55:16.375719 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:55:16.375837 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:55:16.375956 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:55:16.376096 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:55:16.376266 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:55:16.376428 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:55:16.376578 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:55:16.378025 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m05:55:16.378199 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m05:55:16.378316 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m05:55:16.378424 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m05:55:16.378961 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:55:16.379384 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:55:16.379778 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:55:16.380190 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:55:16.380601 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:55:16.380790 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:55:16.380936 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:55:16.381085 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:55:16.381310 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m05:55:16.381768 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public)
[0m05:55:16.383690 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:55:16.385036 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:55:16.385193 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:55:16.385324 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:55:16.385443 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:55:16.385571 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:55:16.394364 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:55:16.394546 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m05:55:16.394685 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:55:16.394799 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:55:16.394928 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:55:16.395091 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:55:16.396474 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m05:55:16.396627 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m05:55:16.397237 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:55:16.397649 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:55:16.398056 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:55:16.398187 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:55:16.401371 [debug] [MainThread]: Using postgres connection "master"
[0m05:55:16.401521 [debug] [MainThread]: On master: BEGIN
[0m05:55:16.401633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:55:16.407066 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m05:55:16.407222 [debug] [MainThread]: Using postgres connection "master"
[0m05:55:16.407386 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:55:16.408931 [debug] [MainThread]: SQL status: SELECT 8 in 0.001 seconds
[0m05:55:16.409910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e7b465d-b412-4ef9-8a2f-664a4ea4a611', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a67190>]}
[0m05:55:16.410125 [debug] [MainThread]: On master: ROLLBACK
[0m05:55:16.410452 [debug] [MainThread]: Using postgres connection "master"
[0m05:55:16.410578 [debug] [MainThread]: On master: BEGIN
[0m05:55:16.411038 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:55:16.411175 [debug] [MainThread]: On master: COMMIT
[0m05:55:16.411295 [debug] [MainThread]: Using postgres connection "master"
[0m05:55:16.411403 [debug] [MainThread]: On master: COMMIT
[0m05:55:16.411733 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:55:16.411853 [debug] [MainThread]: On master: Close
[0m05:55:16.413869 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m05:55:16.414094 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m05:55:16.414275 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_prospect_matches)
[0m05:55:16.414416 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m05:55:16.417876 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m05:55:16.418292 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m05:55:16.440475 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m05:55:16.440993 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:55:16.441162 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m05:55:16.441307 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:55:16.447254 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m05:55:16.447452 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:55:16.447625 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.7
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
  
[0m05:59:29.621079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1b8110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a22ad50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a22a210>]}


============================== 05:59:29.623724 | 61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed ==============================
[0m05:59:29.623724 [info ] [MainThread]: Running with dbt=1.10.13
[0m05:59:29.624100 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'log_cache_events': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'introspect': 'True', 'no_print': 'None', 'target_path': 'None', 'use_colors': 'True', 'printer_width': '80', 'quiet': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'version_check': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'cache_selected_only': 'False'}
[0m05:59:29.719426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4a3fd0>]}
[0m05:59:29.751722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ff9650>]}
[0m05:59:29.752638 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m05:59:29.800418 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m05:59:29.867371 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:59:29.867729 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m05:59:29.999424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4940d0>]}
[0m05:59:30.065335 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m05:59:30.066378 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m05:59:30.077863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8cd250>]}
[0m05:59:30.078120 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m05:59:30.078285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0c0910>]}
[0m05:59:30.078998 [info ] [MainThread]: 
[0m05:59:30.079146 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m05:59:30.079264 [info ] [MainThread]: 
[0m05:59:30.079467 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:59:30.079833 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m05:59:30.105925 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m05:59:30.106149 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m05:59:30.106275 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:59:30.123339 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.017 seconds
[0m05:59:30.123988 [debug] [ThreadPool]: On list_finny_db: Close
[0m05:59:30.126289 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m05:59:30.126516 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m05:59:30.126765 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m05:59:30.129374 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:59:30.129607 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m05:59:30.130514 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:59:30.131622 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:59:30.131787 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m05:59:30.132586 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:59:30.132730 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m05:59:30.132850 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m05:59:30.132964 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:59:30.133089 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m05:59:30.133203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:59:30.133316 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:59:30.133503 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:59:30.145206 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m05:59:30.145447 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m05:59:30.145669 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m05:59:30.145903 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m05:59:30.146075 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m05:59:30.146230 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m05:59:30.146354 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m05:59:30.146494 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m05:59:30.146639 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m05:59:30.146757 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m05:59:30.146982 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:59:30.147222 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m05:59:30.147431 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m05:59:30.148168 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m05:59:30.148375 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m05:59:30.148833 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m05:59:30.148988 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m05:59:30.149116 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m05:59:30.149263 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m05:59:30.149556 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_raw_analysis)
[0m05:59:30.149703 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m05:59:30.150206 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m05:59:30.150848 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m05:59:30.151790 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:59:30.152115 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_staging)
[0m05:59:30.152576 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m05:59:30.152758 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m05:59:30.152910 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m05:59:30.153693 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:59:30.153913 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:59:30.154469 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m05:59:30.155185 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:59:30.161706 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m05:59:30.161906 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m05:59:30.162124 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m05:59:30.162310 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m05:59:30.162479 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m05:59:30.162650 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m05:59:30.164265 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m05:59:30.164448 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m05:59:30.165035 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m05:59:30.165536 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m05:59:30.165971 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m05:59:30.166133 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m05:59:30.169515 [debug] [MainThread]: Using postgres connection "master"
[0m05:59:30.169656 [debug] [MainThread]: On master: BEGIN
[0m05:59:30.169761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:59:30.176658 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m05:59:30.176956 [debug] [MainThread]: Using postgres connection "master"
[0m05:59:30.177212 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m05:59:30.179566 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m05:59:30.180719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61ea1cb6-bc19-4ef7-951b-e2dd19cdd6ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a949610>]}
[0m05:59:30.180959 [debug] [MainThread]: On master: ROLLBACK
[0m05:59:30.181522 [debug] [MainThread]: Using postgres connection "master"
[0m05:59:30.181765 [debug] [MainThread]: On master: BEGIN
[0m05:59:30.182398 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m05:59:30.182537 [debug] [MainThread]: On master: COMMIT
[0m05:59:30.182666 [debug] [MainThread]: Using postgres connection "master"
[0m05:59:30.182782 [debug] [MainThread]: On master: COMMIT
[0m05:59:30.183145 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m05:59:30.183299 [debug] [MainThread]: On master: Close
[0m05:59:30.184928 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m05:59:30.185217 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m05:59:30.185556 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_prospect_matches)
[0m05:59:30.185719 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m05:59:30.190326 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m05:59:30.190883 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m05:59:30.208150 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m05:59:30.208614 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:59:30.208764 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m05:59:30.208897 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:59:30.214802 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m05:59:30.215059 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:59:30.215251 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication (optimized)
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
  LIMIT 1000  -- Limit for testing/performance
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.company = b.company  -- Pre-filter by exact company match
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.7
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
[0m05:59:30.242616 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.027 seconds
[0m05:59:30.247007 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m05:59:30.247195 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:01:14.909832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a62af50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6ab810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6abf50>]}


============================== 06:01:14.913011 | 17ce1816-84b5-4346-a7c8-64c6c2a5ef47 ==============================
[0m06:01:14.913011 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:01:14.913351 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_format': 'default', 'quiet': 'False', 'empty': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches', 'static_parser': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'write_json': 'True', 'use_colors': 'True', 'printer_width': '80', 'introspect': 'True'}
[0m06:01:15.039097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109887490>]}
[0m06:01:15.071731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106761c50>]}
[0m06:01:15.072479 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:01:15.120304 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:01:15.188381 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:01:15.188758 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:01:15.320978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b84aa50>]}
[0m06:01:15.389341 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:01:15.390725 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:01:15.403703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3bdc50>]}
[0m06:01:15.403989 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:01:15.404163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b943350>]}
[0m06:01:15.404903 [info ] [MainThread]: 
[0m06:01:15.405064 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:01:15.405178 [info ] [MainThread]: 
[0m06:01:15.405410 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:01:15.405821 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:01:15.437954 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:01:15.438513 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:01:15.438684 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:01:15.459483 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m06:01:15.460182 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:01:15.462708 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m06:01:15.462972 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:01:15.465399 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:01:15.465575 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:01:15.465794 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:01:15.466700 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:01:15.466841 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:01:15.467791 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:01:15.468726 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:01:15.469027 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:01:15.469179 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:01:15.469319 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:01:15.469437 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:01:15.469550 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:01:15.469775 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:01:15.469918 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:01:15.479193 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:01:15.479521 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:01:15.479753 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:01:15.479995 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:01:15.480193 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:01:15.480309 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:01:15.480448 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:01:15.480570 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:01:15.480685 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:01:15.480818 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:01:15.480964 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:01:15.481115 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:01:15.481477 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:01:15.482044 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:01:15.482440 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:01:15.482740 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_marts)
[0m06:01:15.483865 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:01:15.484072 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:01:15.484305 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m06:01:15.484455 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m06:01:15.484635 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:01:15.485272 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:01:15.485733 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:01:15.486184 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:01:15.486311 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:01:15.486751 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:01:15.486889 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:01:15.487049 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:01:15.487301 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_staging)
[0m06:01:15.489036 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:01:15.489717 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:01:15.489842 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:01:15.492553 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:01:15.492757 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:01:15.492924 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:01:15.494332 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m06:01:15.494867 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:01:15.495211 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:01:15.495812 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:01:15.496006 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:01:15.496162 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:01:15.497724 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m06:01:15.498305 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:01:15.498699 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:01:15.501399 [debug] [MainThread]: Using postgres connection "master"
[0m06:01:15.501563 [debug] [MainThread]: On master: BEGIN
[0m06:01:15.501673 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:01:15.508013 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:01:15.508264 [debug] [MainThread]: Using postgres connection "master"
[0m06:01:15.508516 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:01:15.511779 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m06:01:15.512924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '17ce1816-84b5-4346-a7c8-64c6c2a5ef47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b546b10>]}
[0m06:01:15.513140 [debug] [MainThread]: On master: ROLLBACK
[0m06:01:15.513598 [debug] [MainThread]: Using postgres connection "master"
[0m06:01:15.513796 [debug] [MainThread]: On master: BEGIN
[0m06:01:15.514315 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:01:15.514473 [debug] [MainThread]: On master: COMMIT
[0m06:01:15.514606 [debug] [MainThread]: Using postgres connection "master"
[0m06:01:15.514779 [debug] [MainThread]: On master: COMMIT
[0m06:01:15.515113 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:01:15.515269 [debug] [MainThread]: On master: Close
[0m06:01:15.516851 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:01:15.517154 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:01:15.517583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_prospect_matches)
[0m06:01:15.517762 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:01:15.520895 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:01:15.521551 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:01:15.541178 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:01:15.541614 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:01:15.541769 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:01:15.541910 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:01:15.548573 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m06:01:15.548837 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:01:15.549035 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication (fast exact matches)
-- This model identifies potential duplicate prospects based on exact email matches first



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
  AND email IS NOT NULL
  AND email != ''
),

-- Start with exact email matches (very fast)
exact_email_matches AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    1.0 AS email_sim,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * 1.0 +  -- Exact email match
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.email = b.email  -- Exact email match
)

SELECT 
  source_id,
  target_id,
  total_score
FROM exact_email_matches
WHERE total_score > 0.6  -- Lower threshold since we have exact email match
  );
  
[0m06:02:16.682636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b2c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107233650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107233d90>]}


============================== 06:02:16.685413 | 36ed3d4a-7670-42cb-9a79-614209cfe499 ==============================
[0m06:02:16.685413 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:02:16.685711 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'quiet': 'False', 'cache_selected_only': 'False', 'empty': 'False', 'introspect': 'True', 'write_json': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m06:02:16.789199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076c16d0>]}
[0m06:02:16.821154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f9b3d0>]}
[0m06:02:16.821944 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:02:16.869380 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:02:16.937625 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:02:16.938036 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:02:17.073559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b29d0>]}
[0m06:02:17.142469 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:02:17.143769 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:02:17.155969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107757250>]}
[0m06:02:17.156251 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:02:17.156417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118059cd0>]}
[0m06:02:17.157171 [info ] [MainThread]: 
[0m06:02:17.157335 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:02:17.157450 [info ] [MainThread]: 
[0m06:02:17.157672 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:02:17.158085 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:02:17.185117 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:02:17.185380 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:02:17.185509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:02:17.210651 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.025 seconds
[0m06:02:17.211272 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:02:17.213575 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:02:17.213887 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:02:17.214192 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:02:17.216654 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:02:17.216920 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:02:17.217861 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:02:17.218917 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:02:17.219063 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:02:17.219842 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:02:17.219980 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:02:17.220101 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:02:17.220222 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:02:17.220338 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:02:17.220452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:02:17.220565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:02:17.220805 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:02:17.230377 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:02:17.230718 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:02:17.230928 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:02:17.231163 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:02:17.231377 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:02:17.231542 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:02:17.231659 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:02:17.231803 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:02:17.231949 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:02:17.232089 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:02:17.232228 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:02:17.232443 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:02:17.232645 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:02:17.233293 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:02:17.233635 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:02:17.233885 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_raw)
[0m06:02:17.234388 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:02:17.235289 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:02:17.235422 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m06:02:17.235526 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:02:17.235998 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:02:17.236126 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:02:17.236512 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:02:17.236897 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:02:17.237147 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:02:17.237369 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:02:17.237853 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m06:02:17.238031 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:02:17.238180 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:02:17.239520 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:02:17.240154 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:02:17.240504 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:02:17.244074 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:02:17.244285 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:02:17.244499 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:02:17.246059 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m06:02:17.246655 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:02:17.246974 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:02:17.247180 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:02:17.247393 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:02:17.247804 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:02:17.249337 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m06:02:17.249992 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:02:17.250393 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:02:17.253398 [debug] [MainThread]: Using postgres connection "master"
[0m06:02:17.253588 [debug] [MainThread]: On master: BEGIN
[0m06:02:17.253710 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:02:17.260145 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:02:17.260413 [debug] [MainThread]: Using postgres connection "master"
[0m06:02:17.260665 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:02:17.263182 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:02:17.264337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36ed3d4a-7670-42cb-9a79-614209cfe499', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182692d0>]}
[0m06:02:17.264555 [debug] [MainThread]: On master: ROLLBACK
[0m06:02:17.265026 [debug] [MainThread]: Using postgres connection "master"
[0m06:02:17.265198 [debug] [MainThread]: On master: BEGIN
[0m06:02:17.265867 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m06:02:17.266066 [debug] [MainThread]: On master: COMMIT
[0m06:02:17.266205 [debug] [MainThread]: Using postgres connection "master"
[0m06:02:17.266322 [debug] [MainThread]: On master: COMMIT
[0m06:02:17.266677 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:02:17.266894 [debug] [MainThread]: On master: Close
[0m06:02:17.268679 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:02:17.269034 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:02:17.269378 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:02:17.269542 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:02:17.272686 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:02:17.272999 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:02:17.287436 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:02:17.287816 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:02:17.287966 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:02:17.288100 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:02:17.297680 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m06:02:17.298064 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:02:17.298301 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Fast prospect duplicate detection using simple exact matches
-- This model identifies obvious duplicates without expensive similarity functions



WITH duplicate_emails AS (
  SELECT 
    email,
    count(*) as email_count,
    array_agg(prospect_id order by prospect_id) as prospect_ids
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND email IS NOT NULL 
    AND email != ''
  GROUP BY email
  HAVING count(*) > 1
),

matches AS (
  SELECT 
    prospect_ids[1] as source_id,
    prospect_ids[i] as target_id,
    1.0 as total_score
  FROM duplicate_emails,
       generate_series(2, array_length(prospect_ids, 1)) as i
)

SELECT 
  source_id,
  target_id,
  total_score
FROM matches
  );
  
[0m06:03:58.520861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ea7b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2fa50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2ff90>]}


============================== 06:03:58.523585 | 63b9925a-25dd-43ea-9cf0-54918c67874b ==============================
[0m06:03:58.523585 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:03:58.523917 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select stg_prospect_matches', 'log_format': 'default', 'partial_parse': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'fail_fast': 'False', 'debug': 'False', 'version_check': 'True', 'printer_width': '80', 'introspect': 'True', 'no_print': 'None', 'cache_selected_only': 'False'}
[0m06:03:58.615927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11251f6d0>]}
[0m06:03:58.648304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105becd10>]}
[0m06:03:58.649201 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:03:58.695641 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:03:58.763305 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:03:58.763756 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:03:58.895176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141f4910>]}
[0m06:03:58.963028 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:03:58.964044 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:03:58.976141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c4e510>]}
[0m06:03:58.976413 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:03:58.976581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11422df90>]}
[0m06:03:58.977334 [info ] [MainThread]: 
[0m06:03:58.977490 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:03:58.977608 [info ] [MainThread]: 
[0m06:03:58.977829 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:03:58.978243 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:03:59.005025 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:03:59.005286 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:03:59.005423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:03:59.023824 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.018 seconds
[0m06:03:59.024419 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:03:59.026651 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m06:03:59.026883 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:03:59.027099 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:03:59.029725 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:03:59.029916 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:03:59.030825 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:03:59.031818 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:03:59.031958 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:03:59.032690 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:03:59.032831 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:03:59.032953 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:03:59.033082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:03:59.033196 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:03:59.033321 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:03:59.033444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:03:59.033657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:03:59.045116 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:03:59.045498 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:03:59.045787 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:03:59.046000 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:03:59.046171 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:03:59.046313 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:03:59.046481 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:03:59.046610 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:03:59.046733 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:03:59.046894 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:03:59.047052 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:03:59.047203 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:03:59.048148 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:03:59.048744 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:03:59.048992 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:03:59.049136 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:03:59.049247 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:03:59.049811 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:03:59.049952 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:03:59.050454 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:03:59.050903 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:03:59.051203 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw)
[0m06:03:59.051852 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:03:59.051994 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:03:59.052992 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:03:59.053119 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:03:59.053350 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_staging_analysis)
[0m06:03:59.053785 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:03:59.055247 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:03:59.055712 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:03:59.056005 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:03:59.056277 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:03:59.062696 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:03:59.062883 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:03:59.063106 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:03:59.063244 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:03:59.064219 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:03:59.064566 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:03:59.066947 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:03:59.067778 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m06:03:59.069349 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:03:59.070383 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:03:59.071752 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:03:59.072019 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:03:59.075913 [debug] [MainThread]: Using postgres connection "master"
[0m06:03:59.076111 [debug] [MainThread]: On master: BEGIN
[0m06:03:59.076227 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:03:59.082321 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:03:59.082557 [debug] [MainThread]: Using postgres connection "master"
[0m06:03:59.082873 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:03:59.084971 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:03:59.086041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63b9925a-25dd-43ea-9cf0-54918c67874b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b1d150>]}
[0m06:03:59.086247 [debug] [MainThread]: On master: ROLLBACK
[0m06:03:59.086555 [debug] [MainThread]: Using postgres connection "master"
[0m06:03:59.086705 [debug] [MainThread]: On master: BEGIN
[0m06:03:59.087258 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:03:59.087471 [debug] [MainThread]: On master: COMMIT
[0m06:03:59.087630 [debug] [MainThread]: Using postgres connection "master"
[0m06:03:59.087751 [debug] [MainThread]: On master: COMMIT
[0m06:03:59.088024 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:03:59.088178 [debug] [MainThread]: On master: Close
[0m06:03:59.089673 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:03:59.089929 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:03:59.090122 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_prospect_matches)
[0m06:03:59.090391 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:03:59.093635 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:03:59.093989 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:03:59.109208 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:03:59.109609 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:03:59.110038 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:03:59.110222 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:03:59.116651 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:03:59.116966 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:03:59.117194 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Simple prospect duplicate detection for testing
-- This model finds a small sample of duplicate emails quickly



WITH small_sample AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE prospect_id <= 100  -- Only check first 100 records for speed
),

duplicate_emails AS (
  SELECT 
    email,
    min(prospect_id) as source_id,
    max(prospect_id) as target_id
  FROM small_sample
  WHERE email IS NOT NULL 
    AND email != ''
  GROUP BY email
  HAVING count(*) > 1
)

SELECT 
  source_id,
  target_id,
  1.0 as total_score
FROM duplicate_emails
  );
  
[0m06:04:41.341662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c9b0150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca33a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca33f90>]}


============================== 06:04:41.343831 | a2dad6df-3968-4e39-ad9c-90f2490d2e80 ==============================
[0m06:04:41.343831 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:04:41.344139 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'quiet': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'static_parser': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'no_print': 'None', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select stg_prospect_matches', 'warn_error': 'None', 'write_json': 'True', 'use_colors': 'True', 'version_check': 'True', 'partial_parse': 'True'}
[0m06:04:41.434538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca26590>]}
[0m06:04:41.467147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109af9610>]}
[0m06:04:41.467588 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:04:41.510618 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:04:41.570547 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:04:41.570890 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:04:41.702595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dec4b10>]}
[0m06:04:41.772603 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:04:41.773498 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:04:41.781466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dc8950>]}
[0m06:04:41.781729 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:04:41.781910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10daaf790>]}
[0m06:04:41.782618 [info ] [MainThread]: 
[0m06:04:41.782774 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:04:41.782894 [info ] [MainThread]: 
[0m06:04:41.783122 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:04:41.783560 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:04:41.807756 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:04:41.807986 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:04:41.808113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:04:41.824676 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.017 seconds
[0m06:04:41.825334 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:04:41.827776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:04:41.828151 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:04:41.828475 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:04:41.830956 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:04:41.831207 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:04:41.832139 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:04:41.833229 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:04:41.833428 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:04:41.834441 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:04:41.834667 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:04:41.834858 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:04:41.835003 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:04:41.835133 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:04:41.835257 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:04:41.835395 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:04:41.835610 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:04:41.844097 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:04:41.844322 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:04:41.844455 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:04:41.844618 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:04:41.844789 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:04:41.844940 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:04:41.845055 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:04:41.845170 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:04:41.845320 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:04:41.845468 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:04:41.845620 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:04:41.845768 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:04:41.847467 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m06:04:41.847676 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:04:41.847821 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:04:41.848371 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:04:41.848493 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m06:04:41.848947 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:04:41.849357 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:04:41.850041 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:04:41.850225 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:04:41.850584 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_marts)
[0m06:04:41.850748 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:04:41.850857 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:04:41.850961 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:04:41.851852 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:04:41.852286 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_raw_analysis)
[0m06:04:41.853101 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:04:41.854485 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:04:41.854631 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:04:41.854759 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:04:41.854998 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:04:41.860839 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:04:41.861130 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:04:41.861302 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:04:41.861476 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:04:41.861605 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:04:41.861747 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:04:41.862991 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m06:04:41.863206 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m06:04:41.863812 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:04:41.864402 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:04:41.864815 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:04:41.865001 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:04:41.868486 [debug] [MainThread]: Using postgres connection "master"
[0m06:04:41.869756 [debug] [MainThread]: On master: BEGIN
[0m06:04:41.870308 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:04:41.881409 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m06:04:41.881643 [debug] [MainThread]: Using postgres connection "master"
[0m06:04:41.881825 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:04:41.883806 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:04:41.884927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2dad6df-3968-4e39-ad9c-90f2490d2e80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4f3610>]}
[0m06:04:41.885142 [debug] [MainThread]: On master: ROLLBACK
[0m06:04:41.885583 [debug] [MainThread]: Using postgres connection "master"
[0m06:04:41.885824 [debug] [MainThread]: On master: BEGIN
[0m06:04:41.886358 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:04:41.886545 [debug] [MainThread]: On master: COMMIT
[0m06:04:41.886753 [debug] [MainThread]: Using postgres connection "master"
[0m06:04:41.886895 [debug] [MainThread]: On master: COMMIT
[0m06:04:41.887245 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:04:41.887449 [debug] [MainThread]: On master: Close
[0m06:04:41.888759 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:04:41.889208 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:04:41.889996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:04:41.890551 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:04:41.893842 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:04:41.894250 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:04:41.909358 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:04:41.909952 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:04:41.910188 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:04:41.910337 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:04:41.916971 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:04:41.917191 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:04:41.917355 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Minimal prospect matches for testing
-- This model returns a simple test result



SELECT 
  1 as source_id,
  2 as target_id,
  1.0 as total_score

UNION ALL

SELECT 
  3 as source_id,
  4 as target_id,
  0.9 as total_score
  );
  
[0m06:08:08.274777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f2ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f9a9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f9a210>]}


============================== 06:08:08.277131 | efaa1b6e-eb6e-4b72-94ab-4e476d512f53 ==============================
[0m06:08:08.277131 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:08:08.277416 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'debug': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'cache_selected_only': 'False', 'fail_fast': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_format': 'default', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'write_json': 'True', 'quiet': 'False'}
[0m06:08:08.403683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d0f750>]}
[0m06:08:08.435652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f29450>]}
[0m06:08:08.436412 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:08:08.484079 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:08:08.552210 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:08:08.552435 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:08:08.575483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123131390>]}
[0m06:08:08.616009 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:08:08.617099 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:08:08.628991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12341fed0>]}
[0m06:08:08.629249 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:08:08.629418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12336d750>]}
[0m06:08:08.630147 [info ] [MainThread]: 
[0m06:08:08.630306 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:08:08.630423 [info ] [MainThread]: 
[0m06:08:08.630638 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:08:08.630995 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:08:08.682051 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:08:08.682287 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:08:08.682418 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:08:08.703345 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m06:08:08.703955 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:08:08.706289 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:08:08.706526 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:08:08.708942 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:08:08.709130 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:08:08.709428 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:08:08.710250 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:08:08.710393 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:08:08.711168 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:08:08.711910 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:08:08.712063 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:08:08.712189 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:08:08.712308 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:08:08.712426 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:08:08.712596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:08:08.712854 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:08:08.712981 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:08:08.728792 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m06:08:08.728967 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m06:08:08.729107 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m06:08:08.729234 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m06:08:08.729394 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:08:08.729519 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:08:08.729632 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:08:08.729742 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:08:08.729867 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:08:08.730009 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:08:08.730148 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:08:08.730300 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:08:08.734371 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m06:08:08.734524 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m06:08:08.734691 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m06:08:08.734827 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m06:08:08.735391 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:08:08.735822 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:08:08.736291 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:08:08.736699 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:08:08.737857 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:08:08.737992 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:08:08.738118 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:08:08.738354 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m06:08:08.738503 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:08:08.739072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_raw_analysis)
[0m06:08:08.740180 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:08:08.742966 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:08:08.743197 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:08:08.743346 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:08:08.743483 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:08:08.743609 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:08:08.753262 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:08:08.753535 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:08:08.753760 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:08:08.753912 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:08:08.754059 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:08:08.754209 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:08:08.756097 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:08:08.756645 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:08:08.756881 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:08:08.757719 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:08:08.757901 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:08:08.758449 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:08:08.761305 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.761539 [debug] [MainThread]: On master: BEGIN
[0m06:08:08.761665 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:08:08.767745 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:08:08.767953 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.768138 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:08:08.770661 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:08:08.771661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bfb310>]}
[0m06:08:08.771867 [debug] [MainThread]: On master: ROLLBACK
[0m06:08:08.772250 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.772518 [debug] [MainThread]: On master: BEGIN
[0m06:08:08.773032 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:08:08.773209 [debug] [MainThread]: On master: COMMIT
[0m06:08:08.773348 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.773472 [debug] [MainThread]: On master: COMMIT
[0m06:08:08.773818 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:08:08.773961 [debug] [MainThread]: On master: Close
[0m06:08:08.775793 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:08:08.776184 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:08:08.776417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_prospect_matches)
[0m06:08:08.776601 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:08:08.779768 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:08:08.780154 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:08:08.799743 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:08:08.800560 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:08:08.800879 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:08:08.801072 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:08:08.807412 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:08:08.807678 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:08:08.807861 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Minimal prospect matches for testing
-- This model returns a simple test result



SELECT 
  1 as source_id,
  2 as target_id,
  1.0 as total_score

UNION ALL

SELECT 
  3 as source_id,
  4 as target_id,
  0.9 as total_score
  );
  
[0m06:08:08.810777 [debug] [Thread-1 (]: SQL status: SELECT 2 in 0.003 seconds
[0m06:08:08.815111 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:08:08.815298 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:08:08.815992 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:08:08.821531 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:08:08.821728 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:08:08.821873 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:08:08.822795 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:08:08.825122 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:08:08.826942 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:08:08.827100 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:08:08.827801 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m06:08:08.829127 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:08:08.830162 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efaa1b6e-eb6e-4b72-94ab-4e476d512f53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b9bd50>]}
[0m06:08:08.830502 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 2[0m in 0.05s]
[0m06:08:08.830740 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:08:08.831499 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.831794 [debug] [MainThread]: On master: BEGIN
[0m06:08:08.831943 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:08:08.837935 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:08:08.838149 [debug] [MainThread]: On master: COMMIT
[0m06:08:08.838264 [debug] [MainThread]: Using postgres connection "master"
[0m06:08:08.838376 [debug] [MainThread]: On master: COMMIT
[0m06:08:08.838730 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:08:08.839059 [debug] [MainThread]: On master: Close
[0m06:08:08.839305 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:08:08.839435 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:08:08.839555 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:08:08.839663 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:08:08.839768 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:08:08.839912 [info ] [MainThread]: 
[0m06:08:08.840069 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m06:08:08.840356 [debug] [MainThread]: Command end result
[0m06:08:08.854034 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:08:08.854863 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:08:08.857574 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:08:08.857765 [info ] [MainThread]: 
[0m06:08:08.858004 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:08:08.858135 [info ] [MainThread]: 
[0m06:08:08.858280 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:08:08.860579 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.621901, "process_in_blocks": "0", "process_kernel_time": 0.199111, "process_mem_max_rss": "135184384", "process_out_blocks": "0", "process_user_time": 0.998328}
[0m06:08:08.860925 [debug] [MainThread]: Command `dbt run` succeeded at 06:08:08.860862 after 0.62 seconds
[0m06:08:08.861168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102820390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fd58d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028eec10>]}
[0m06:08:08.861399 [debug] [MainThread]: Flushing usage events
[0m06:08:09.151971 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:09:00.088163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110b2350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11112fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11112ff90>]}


============================== 06:09:00.090708 | 09ec96a3-0efe-43dc-bab6-ceba7798fbaf ==============================
[0m06:09:00.090708 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:09:00.091015 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'debug': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'introspect': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'cache_selected_only': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80', 'target_path': 'None', 'log_format': 'default', 'empty': 'False'}
[0m06:09:00.188995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e6a4d0>]}
[0m06:09:00.218605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f933d0>]}
[0m06:09:00.219432 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:09:00.266285 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:09:00.329618 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:09:00.329942 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:09:00.457682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123b6b10>]}
[0m06:09:00.522693 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:09:00.523709 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:09:00.534572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ea0c50>]}
[0m06:09:00.534816 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:09:00.535020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113028310>]}
[0m06:09:00.535723 [info ] [MainThread]: 
[0m06:09:00.535874 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:09:00.535990 [info ] [MainThread]: 
[0m06:09:00.536182 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:09:00.536535 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:09:00.564059 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:09:00.564321 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:09:00.564447 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:00.595444 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.031 seconds
[0m06:09:00.596099 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:09:00.598236 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m06:09:00.598459 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:09:00.598704 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:09:00.601122 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:09:00.601307 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:09:00.602448 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:09:00.603232 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:09:00.603362 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:09:00.604084 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:09:00.604215 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:09:00.604329 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:09:00.604439 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:00.604551 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:09:00.604662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:00.604785 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:00.605000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:00.617975 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:09:00.618121 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:09:00.618255 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:09:00.618360 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:09:00.618459 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:09:00.618573 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:09:00.618677 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:09:00.618807 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:09:00.618930 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:09:00.619046 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:09:00.619179 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:09:00.619355 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:09:00.622792 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m06:09:00.622983 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m06:09:00.623514 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:09:00.623659 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m06:09:00.623767 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:09:00.624187 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:09:00.624635 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:09:00.624762 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:09:00.625126 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:09:00.625493 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging)
[0m06:09:00.626239 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:09:00.626394 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:09:00.627412 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:09:00.627529 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:09:00.627842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_marts)
[0m06:09:00.628594 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:09:00.629601 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:09:00.629938 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:00.630056 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:09:00.630271 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:00.637369 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:09:00.637577 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:09:00.637772 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:09:00.637981 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m06:09:00.638170 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:09:00.638314 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:09:00.640699 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:09:00.640995 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:09:00.641964 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:09:00.642554 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:09:00.643317 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:09:00.643499 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:09:00.652799 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:00.653032 [debug] [MainThread]: On master: BEGIN
[0m06:09:00.653147 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:09:00.660361 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:09:00.660678 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:00.660848 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:09:00.663201 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:09:00.665005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09ec96a3-0efe-43dc-bab6-ceba7798fbaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11302af10>]}
[0m06:09:00.665440 [debug] [MainThread]: On master: ROLLBACK
[0m06:09:00.665934 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:00.666125 [debug] [MainThread]: On master: BEGIN
[0m06:09:00.666675 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:09:00.666846 [debug] [MainThread]: On master: COMMIT
[0m06:09:00.666977 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:00.667098 [debug] [MainThread]: On master: COMMIT
[0m06:09:00.667457 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:09:00.667724 [debug] [MainThread]: On master: Close
[0m06:09:00.669792 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:09:00.670222 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m06:09:00.670679 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:09:00.670919 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:09:00.675766 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:09:00.676157 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:09:00.698432 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:09:00.698853 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:09:00.698992 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:09:00.699132 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:09:00.705105 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:09:00.705299 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:09:00.705477 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.7
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
  
[0m06:09:49.999036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba28090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10baaf8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10baaff90>]}


============================== 06:09:50.001209 | 4df1bdc0-6db1-428e-b3d9-a6c469a70a5d ==============================
[0m06:09:50.001209 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:09:50.001502 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'quiet': 'False', 'write_json': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'empty': 'False', 'debug': 'False', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_format': 'default', 'invocation_command': 'dbt run --select stg_prospect_matches', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:09:50.092629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8172d0>]}
[0m06:09:50.124588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8173d0>]}
[0m06:09:50.125119 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:09:50.168357 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:09:50.228266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:09:50.228638 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:09:50.363313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cca8c50>]}
[0m06:09:50.430791 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:09:50.431595 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:09:50.438232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c000890>]}
[0m06:09:50.438525 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:09:50.438711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8cb390>]}
[0m06:09:50.439597 [info ] [MainThread]: 
[0m06:09:50.439755 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:09:50.439873 [info ] [MainThread]: 
[0m06:09:50.440101 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:09:50.440507 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:09:50.463317 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:09:50.463543 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:09:50.463829 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:50.480268 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.016 seconds
[0m06:09:50.480857 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:09:50.483095 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m06:09:50.483382 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:09:50.485812 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:09:50.486062 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:09:50.486300 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:09:50.487189 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:09:50.487346 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:09:50.488341 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:09:50.489084 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:09:50.489222 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:09:50.489350 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:50.489477 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:09:50.489596 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:09:50.489714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:50.489936 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:50.490069 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:09:50.498337 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:09:50.498592 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:09:50.498755 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:09:50.498881 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:09:50.499049 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:09:50.499187 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:09:50.499328 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:09:50.499448 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:09:50.499597 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:09:50.499762 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:09:50.499924 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:09:50.500072 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:09:50.501748 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m06:09:50.501946 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:09:50.502101 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:09:50.502288 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:09:50.502927 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:09:50.503404 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:09:50.503828 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:09:50.504222 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:09:50.504665 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:09:50.504817 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:09:50.504938 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:09:50.505042 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:09:50.505292 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_staging_analysis)
[0m06:09:50.505818 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_marts)
[0m06:09:50.507515 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:09:50.508771 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:09:50.508913 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:09:50.509101 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:09:50.509263 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:50.509383 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:09:50.515789 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:09:50.515995 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:09:50.516198 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:09:50.516326 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:09:50.516472 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:09:50.516635 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:09:50.518161 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m06:09:50.518324 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:09:50.518906 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:09:50.519467 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:09:50.519957 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:09:50.520106 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:09:50.523123 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:50.523298 [debug] [MainThread]: On master: BEGIN
[0m06:09:50.523415 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:09:50.529495 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:09:50.529709 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:50.529899 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:09:50.531644 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:09:50.532747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac5dfd0>]}
[0m06:09:50.532983 [debug] [MainThread]: On master: ROLLBACK
[0m06:09:50.533663 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:50.533866 [debug] [MainThread]: On master: BEGIN
[0m06:09:50.534329 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:09:50.534498 [debug] [MainThread]: On master: COMMIT
[0m06:09:50.534631 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:50.534749 [debug] [MainThread]: On master: COMMIT
[0m06:09:50.535053 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:09:50.535193 [debug] [MainThread]: On master: Close
[0m06:09:50.536289 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:09:50.536514 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:09:50.536825 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:09:50.537158 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:09:50.540309 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:09:50.540685 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:09:50.561419 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:09:50.561999 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:09:50.562174 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:09:50.562490 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:09:50.568592 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:09:50.568862 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:09:50.569056 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication (batch processing)
-- This model identifies potential duplicate prospects using small batch processing



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id BETWEEN 1 AND 500  -- Process only first 500 records as a batch
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.email = b.email  -- Pre-filter: only exact email matches for efficiency
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.9  -- Higher threshold for email since we pre-filter
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
[0m06:09:52.166508 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:09:52.168897 [debug] [MainThread]: Postgres adapter: Cancelling query 'model.dbt_service.stg_prospect_matches' (2023)
[0m06:09:52.169313 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:52.169577 [debug] [MainThread]: On master: BEGIN
[0m06:09:52.169807 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:09:52.179004 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m06:09:52.179634 [debug] [MainThread]: Using postgres connection "master"
[0m06:09:52.179872 [debug] [MainThread]: On master: select pg_terminate_backend(2023)
[0m06:09:52.180628 [debug] [MainThread]: SQL status: SELECT 1 in 0.001 seconds
[0m06:09:52.180798 [debug] [MainThread]: Postgres adapter: Cancel query 'model.dbt_service.stg_prospect_matches': (True,)
[0m06:09:52.180955 [error] [MainThread]: CANCEL query model.dbt_service.stg_prospect_matches ............................ [[31mCANCEL[0m]
[0m06:09:52.181379 [error] [MainThread]: CANCEL query list_finny_db_public_marts ........................................ [[31mCANCEL[0m]
[0m06:09:52.181839 [error] [MainThread]: CANCEL query list_finny_db_public_staging_analysis ............................. [[31mCANCEL[0m]
[0m06:09:52.182146 [error] [MainThread]: CANCEL query list_finny_db_public .............................................. [[31mCANCEL[0m]
[0m06:09:52.182340 [debug] [MainThread]: On master: ROLLBACK
[0m06:09:52.183070 [debug] [Thread-1 (]: Postgres adapter: Postgres error: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[0m06:09:52.183255 [debug] [MainThread]: On master: Close
[0m06:09:52.183459 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: ROLLBACK
[0m06:09:52.185124 [debug] [Thread-1 (]: Failed to rollback 'model.dbt_service.stg_prospect_matches'
[0m06:09:52.185391 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:09:52.193575 [debug] [Thread-1 (]: Database Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  server closed the connection unexpectedly
  	This probably means the server terminated abnormally
  	before or while processing the request.
  compiled code at target/run/dbt_service/models/staging/stg_prospect_matches.sql
[0m06:09:52.196924 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4df1bdc0-6db1-428e-b3d9-a6c469a70a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098d1050>]}
[0m06:09:52.197542 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model public_staging.stg_prospect_matches ...... [[31mERROR[0m in 1.66s]
[0m06:09:52.198263 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:09:52.198533 [info ] [MainThread]: 
[0m06:09:52.199133 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m06:09:52.199476 [info ] [MainThread]: 
[0m06:09:52.199714 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=0
[0m06:09:52.199935 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:09:52.200345 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:09:52.200654 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:09:52.201009 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:09:52.201277 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:09:52.201466 [info ] [MainThread]: 
[0m06:09:52.201612 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 1.76 seconds (1.76s).
[0m06:09:52.201811 [error] [MainThread]: Encountered an error:

[0m06:09:52.204473 [error] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 373, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 390, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/main.py", line 587, in run
    results = task.run()
              ^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 599, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 536, in execute_with_hooks
    res = self.execute_nodes()
          ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 416, in execute_nodes
    self.run_queue(pool)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 343, in run_queue
    self.job_queue.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/graph/queue.py", line 206, in join
    self.inner.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", line 327, in wait
    waiter.acquire()
KeyboardInterrupt

[0m06:09:52.206963 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.2444298, "process_in_blocks": "0", "process_kernel_time": 0.195764, "process_mem_max_rss": "143818752", "process_out_blocks": "0", "process_user_time": 1.115257}
[0m06:09:52.207391 [debug] [MainThread]: Command `dbt run` failed at 06:09:52.207320 after 2.24 seconds
[0m06:09:52.207599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bad6090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103048250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098dbdd0>]}
[0m06:09:52.207782 [debug] [MainThread]: Flushing usage events
[0m06:09:52.562207 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:10:18.071073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b1410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b3410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112133fd0>]}


============================== 06:10:18.073157 | d1f2153d-85cb-4ec3-872f-ba956c050143 ==============================
[0m06:10:18.073157 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:10:18.073468 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_cache_events': 'False', 'quiet': 'False', 'fail_fast': 'False', 'empty': 'False', 'version_check': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'no_print': 'None', 'target_path': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'static_parser': 'True', 'printer_width': '80', 'introspect': 'True'}
[0m06:10:18.158965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111650610>]}
[0m06:10:18.188611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e9b3d0>]}
[0m06:10:18.189069 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:10:18.232033 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:10:18.287249 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:10:18.287628 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:10:18.413834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112efc250>]}
[0m06:10:18.477924 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:10:18.478721 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:10:18.485478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128198d0>]}
[0m06:10:18.485720 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:10:18.485884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131d0450>]}
[0m06:10:18.486612 [info ] [MainThread]: 
[0m06:10:18.486771 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:10:18.486891 [info ] [MainThread]: 
[0m06:10:18.487089 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:10:18.487447 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:10:18.505575 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:10:18.505817 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:10:18.505948 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:10:18.526066 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.020 seconds
[0m06:10:18.526852 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:10:18.529100 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:10:18.529377 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:10:18.531938 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:10:18.532203 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:10:18.532444 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:10:18.533372 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:10:18.533524 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:10:18.534532 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:10:18.535260 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:10:18.535386 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:10:18.535509 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:10:18.535639 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:10:18.535765 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:10:18.535883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:10:18.536116 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:10:18.536264 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:10:18.549539 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:10:18.549702 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:10:18.549897 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:10:18.550010 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:10:18.550115 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:10:18.550259 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:10:18.550394 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:10:18.550507 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:10:18.550637 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:10:18.550784 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:10:18.550936 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:10:18.551078 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:10:18.556065 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m06:10:18.556188 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m06:10:18.556714 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:10:18.556844 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.006 seconds
[0m06:10:18.557291 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:10:18.557433 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.006 seconds
[0m06:10:18.557867 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:10:18.557990 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:10:18.558561 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:10:18.558730 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:10:18.559002 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m06:10:18.559393 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:10:18.559670 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m06:10:18.560572 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:10:18.560686 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:10:18.561915 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:10:18.562480 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:10:18.562652 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:10:18.562992 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:10:18.563120 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:10:18.570272 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:10:18.570498 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:10:18.570806 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:10:18.570955 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:10:18.571111 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:10:18.571288 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:10:18.572936 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m06:10:18.573147 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:10:18.573794 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:10:18.574247 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:10:18.574791 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:10:18.574938 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:10:18.578140 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:18.578335 [debug] [MainThread]: On master: BEGIN
[0m06:10:18.578451 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:10:18.585349 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:10:18.585609 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:18.585859 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:10:18.588062 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:10:18.589320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fc310>]}
[0m06:10:18.589574 [debug] [MainThread]: On master: ROLLBACK
[0m06:10:18.590037 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:18.590210 [debug] [MainThread]: On master: BEGIN
[0m06:10:18.590686 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:10:18.590850 [debug] [MainThread]: On master: COMMIT
[0m06:10:18.591879 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:18.592050 [debug] [MainThread]: On master: COMMIT
[0m06:10:18.592561 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:10:18.592713 [debug] [MainThread]: On master: Close
[0m06:10:18.594133 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:10:18.594511 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:10:18.594891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:10:18.595066 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:10:18.598630 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:10:18.599395 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:10:18.615775 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:10:18.616406 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:18.616579 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:10:18.616720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:10:18.623389 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m06:10:18.623717 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:18.623931 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Simple prospect exact matches for deduplication
-- This model identifies exact duplicates without expensive similarity functions



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
),

-- Find exact email matches
exact_email_pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    'exact_email' as match_type,
    1.0 AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.email = b.email
    AND a.email IS NOT NULL
    AND a.email != ''
),

-- Find exact name + company matches  
exact_name_company_pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    'exact_name_company' as match_type,
    0.95 AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.name = b.name
    AND a.company = b.company
    AND a.name IS NOT NULL
    AND a.name != ''
    AND a.company IS NOT NULL
    AND a.company != ''
)

SELECT source_id, target_id, match_type, total_score FROM exact_email_pairs
UNION ALL
SELECT source_id, target_id, match_type, total_score FROM exact_name_company_pairs
  );
  
[0m06:10:19.399838 [debug] [Thread-1 (]: SQL status: SELECT 13821 in 0.776 seconds
[0m06:10:19.406024 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:19.406209 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:10:19.406863 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:10:19.407956 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:19.408167 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:10:19.408762 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:10:19.418661 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:10:19.419008 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:19.419257 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:10:19.421583 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m06:10:19.425292 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:10:19.427855 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:10:19.428105 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:10:19.429473 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:10:19.430917 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:10:19.431804 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd1f2153d-85cb-4ec3-872f-ba956c050143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138e1fd0>]}
[0m06:10:19.432115 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 13821[0m in 0.84s]
[0m06:10:19.432364 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:10:19.432994 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:19.433184 [debug] [MainThread]: On master: BEGIN
[0m06:10:19.433320 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:10:19.439498 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:10:19.439739 [debug] [MainThread]: On master: COMMIT
[0m06:10:19.439925 [debug] [MainThread]: Using postgres connection "master"
[0m06:10:19.440062 [debug] [MainThread]: On master: COMMIT
[0m06:10:19.440418 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:10:19.440571 [debug] [MainThread]: On master: Close
[0m06:10:19.440775 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:10:19.440909 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:10:19.441019 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:10:19.441133 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:10:19.441241 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:10:19.441380 [info ] [MainThread]: 
[0m06:10:19.441528 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.95 seconds (0.95s).
[0m06:10:19.441811 [debug] [MainThread]: Command end result
[0m06:10:19.457406 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:10:19.458263 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:10:19.461311 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:10:19.461513 [info ] [MainThread]: 
[0m06:10:19.461705 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:10:19.461844 [info ] [MainThread]: 
[0m06:10:19.462018 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:10:19.463552 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.4265476, "process_in_blocks": "0", "process_kernel_time": 0.173411, "process_mem_max_rss": "142475264", "process_out_blocks": "0", "process_user_time": 1.107572}
[0m06:10:19.463782 [debug] [MainThread]: Command `dbt run` succeeded at 06:10:19.463746 after 1.43 seconds
[0m06:10:19.463948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1004180d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11215dd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100616750>]}
[0m06:10:19.464114 [debug] [MainThread]: Flushing usage events
[0m06:10:19.686157 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:11:08.057170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200138d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1202ab750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1202abe90>]}


============================== 06:11:08.058808 | 64a21998-cb2c-415b-bf70-70859af720b7 ==============================
[0m06:11:08.058808 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:11:08.059083 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_format': 'default', 'debug': 'False', 'partial_parse': 'True', 'target_path': 'None', 'version_check': 'True', 'empty': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_colors': 'True', 'fail_fast': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'printer_width': '80', 'log_cache_events': 'False', 'warn_error': 'None', 'write_json': 'True', 'no_print': 'None', 'introspect': 'True', 'cache_selected_only': 'False', 'static_parser': 'True'}
[0m06:11:08.142456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12022a450>]}
[0m06:11:08.171970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050f61d0>]}
[0m06:11:08.172399 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:11:08.213997 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:11:08.268317 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:11:08.268621 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:11:08.391355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e8c910>]}
[0m06:11:08.455091 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:11:08.455929 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:11:08.463121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b39510>]}
[0m06:11:08.463412 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:11:08.463581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a4d9d0>]}
[0m06:11:08.464302 [info ] [MainThread]: 
[0m06:11:08.464453 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:11:08.464571 [info ] [MainThread]: 
[0m06:11:08.464774 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:11:08.465123 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:11:08.484433 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:11:08.484689 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:11:08.484825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:11:08.506060 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m06:11:08.506664 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:11:08.508838 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m06:11:08.509070 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:11:08.511483 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:11:08.511680 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:11:08.511881 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:11:08.512726 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:11:08.512879 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:11:08.513807 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:11:08.514507 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:11:08.514630 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:11:08.514746 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:11:08.514859 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:11:08.514967 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:11:08.515074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:11:08.515254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:11:08.515385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:11:08.524529 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:11:08.524696 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:11:08.524844 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:11:08.527497 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:11:08.527633 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:11:08.527743 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:11:08.527859 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:11:08.527965 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:11:08.528097 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:11:08.528220 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m06:11:08.528331 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:11:08.528471 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:11:08.528992 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:11:08.529149 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:11:08.529733 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:11:08.529933 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public_raw)
[0m06:11:08.530841 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:11:08.530948 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:11:08.531054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:11:08.531904 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m06:11:08.532437 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:11:08.532605 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:11:08.532986 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:11:08.533109 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:11:08.533225 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:11:08.533661 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:11:08.533810 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:11:08.534143 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging_analysis)
[0m06:11:08.534849 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:11:08.536272 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:11:08.536798 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:11:08.536924 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:11:08.537824 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:11:08.537995 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:11:08.538202 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:11:08.539707 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m06:11:08.540295 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:11:08.540727 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:11:08.543488 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:11:08.543712 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:11:08.543884 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:11:08.545338 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m06:11:08.545818 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:11:08.546162 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:11:08.548910 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.549080 [debug] [MainThread]: On master: BEGIN
[0m06:11:08.549192 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:11:08.555598 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:11:08.555752 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.555920 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:11:08.557504 [debug] [MainThread]: SQL status: SELECT 8 in 0.001 seconds
[0m06:11:08.558810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f2efd0>]}
[0m06:11:08.559032 [debug] [MainThread]: On master: ROLLBACK
[0m06:11:08.559424 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.559603 [debug] [MainThread]: On master: BEGIN
[0m06:11:08.560109 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:11:08.560245 [debug] [MainThread]: On master: COMMIT
[0m06:11:08.560369 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.560482 [debug] [MainThread]: On master: COMMIT
[0m06:11:08.560953 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:11:08.561613 [debug] [MainThread]: On master: Close
[0m06:11:08.563850 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:11:08.564134 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:11:08.564497 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:11:08.564662 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:11:08.568517 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:11:08.569110 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:11:08.583944 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:11:08.584681 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.584910 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:11:08.585049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:11:08.591362 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:11:08.591631 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.591836 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication (batch processing)
-- This model identifies potential duplicate prospects using small batch processing



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id BETWEEN 1 AND 500  -- Process only first 500 records as a batch
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
    AND a.email = b.email  -- Pre-filter: only exact email matches for efficiency
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.9  -- Higher threshold for email since we pre-filter
  )
)

SELECT 
  source_id,
  target_id,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
[0m06:11:08.602091 [debug] [Thread-1 (]: SQL status: SELECT 0 in 0.010 seconds
[0m06:11:08.606350 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.606576 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:11:08.607249 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:11:08.608605 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.608775 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:11:08.609280 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:11:08.616809 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:11:08.617038 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.617178 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:11:08.618503 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:11:08.621932 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:11:08.623820 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:11:08.623979 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:11:08.625494 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:11:08.626852 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:11:08.627715 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '64a21998-cb2c-415b-bf70-70859af720b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122eca690>]}
[0m06:11:08.628079 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 0[0m in 0.06s]
[0m06:11:08.628327 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:11:08.629001 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.629159 [debug] [MainThread]: On master: BEGIN
[0m06:11:08.629277 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:11:08.634496 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m06:11:08.634645 [debug] [MainThread]: On master: COMMIT
[0m06:11:08.634762 [debug] [MainThread]: Using postgres connection "master"
[0m06:11:08.634867 [debug] [MainThread]: On master: COMMIT
[0m06:11:08.635177 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:11:08.635404 [debug] [MainThread]: On master: Close
[0m06:11:08.635628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:11:08.635747 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:11:08.635855 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:11:08.635965 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:11:08.636073 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:11:08.636209 [info ] [MainThread]: 
[0m06:11:08.636453 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m06:11:08.636705 [debug] [MainThread]: Command end result
[0m06:11:08.649209 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:11:08.650318 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:11:08.652610 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:11:08.653081 [info ] [MainThread]: 
[0m06:11:08.653425 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:11:08.653624 [info ] [MainThread]: 
[0m06:11:08.653816 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:11:08.655207 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.6304324, "process_in_blocks": "0", "process_kernel_time": 0.15139, "process_mem_max_rss": "143196160", "process_out_blocks": "0", "process_user_time": 1.04841}
[0m06:11:08.655424 [debug] [MainThread]: Command `dbt run` succeeded at 06:11:08.655391 after 0.63 seconds
[0m06:11:08.655586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102538390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1202d5f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102573910>]}
[0m06:11:08.655752 [debug] [MainThread]: Flushing usage events
[0m06:11:08.872103 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:13:45.510611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094b23d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095376d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109537e50>]}


============================== 06:13:45.512701 | d790e8c8-8382-491a-b06e-1016c8c9c459 ==============================
[0m06:13:45.512701 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:13:45.513026 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'cache_selected_only': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'write_json': 'True', 'empty': 'False', 'quiet': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'debug': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'log_format': 'default', 'printer_width': '80', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select stg_prospect_matches', 'warn_error': 'None', 'no_print': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'introspect': 'True'}
[0m06:13:45.600765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10976abd0>]}
[0m06:13:45.631585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051f9dd0>]}
[0m06:13:45.632157 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:13:45.674496 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:13:45.730809 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:13:45.731146 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:13:45.856586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4f6190>]}
[0m06:13:45.921640 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:13:45.922531 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:13:45.929418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051e8610>]}
[0m06:13:45.929667 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:13:45.929829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4aa950>]}
[0m06:13:45.930572 [info ] [MainThread]: 
[0m06:13:45.930735 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:13:45.930858 [info ] [MainThread]: 
[0m06:13:45.931067 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:13:45.931441 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:13:45.953646 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:13:45.953906 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:13:45.954048 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:13:45.971675 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.018 seconds
[0m06:13:45.972490 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:13:45.974766 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m06:13:45.975016 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:13:45.975251 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:13:45.977603 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:13:45.977812 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:13:45.978709 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:13:45.979916 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:13:45.980097 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:13:45.980819 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:13:45.980952 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:13:45.981066 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:13:45.981210 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:13:45.981323 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:13:45.981424 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:13:45.981526 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:13:45.981736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:13:45.994049 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:13:45.994287 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:13:45.994462 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:13:45.994601 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:13:45.994756 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:13:45.994886 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:13:45.995002 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:13:45.995111 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:13:45.995236 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:13:45.995379 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:13:45.995518 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:13:45.995662 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:13:45.997234 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m06:13:45.997414 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:13:45.997599 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:13:45.997716 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:13:45.998227 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:13:45.998654 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:13:45.999039 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:13:45.999426 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:13:45.999867 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:13:46.000134 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:13:46.000366 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:13:46.000525 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:13:46.000803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public)
[0m06:13:46.001198 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_staging_analysis)
[0m06:13:46.002813 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:13:46.003730 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:13:46.003852 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:13:46.003960 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:13:46.004063 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:13:46.004167 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:13:46.011084 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:13:46.011278 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:13:46.011582 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:13:46.011744 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:13:46.011928 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:13:46.012113 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:13:46.013991 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:13:46.014156 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:13:46.014904 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:13:46.015350 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:13:46.015822 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:13:46.015992 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:13:46.019330 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.019542 [debug] [MainThread]: On master: BEGIN
[0m06:13:46.019656 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:13:46.026371 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:13:46.026676 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.026968 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:13:46.028933 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:13:46.030057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3e78d0>]}
[0m06:13:46.030258 [debug] [MainThread]: On master: ROLLBACK
[0m06:13:46.030679 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.030845 [debug] [MainThread]: On master: BEGIN
[0m06:13:46.031445 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:13:46.031649 [debug] [MainThread]: On master: COMMIT
[0m06:13:46.031786 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.031908 [debug] [MainThread]: On master: COMMIT
[0m06:13:46.032188 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:13:46.032353 [debug] [MainThread]: On master: Close
[0m06:13:46.033890 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:13:46.034536 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:13:46.035344 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:13:46.035546 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:13:46.039763 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:13:46.040250 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:13:46.056967 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:13:46.057419 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.057562 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:13:46.057689 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:13:46.063783 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:13:46.064021 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.064223 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 50  -- Small batch for testing similarity functions
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.7 OR
    similarity(a.email, b.email) > 0.7
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.8
  );
  
[0m06:13:46.078462 [debug] [Thread-1 (]: SQL status: SELECT 0 in 0.014 seconds
[0m06:13:46.082996 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.083181 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:13:46.083965 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:13:46.085706 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.085885 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:13:46.086402 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:13:46.094331 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:13:46.094553 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.094690 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:13:46.095669 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:13:46.098459 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:13:46.100366 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:13:46.100538 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:13:46.101621 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:13:46.103009 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:13:46.104177 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd790e8c8-8382-491a-b06e-1016c8c9c459', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4967d0>]}
[0m06:13:46.104600 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 0[0m in 0.07s]
[0m06:13:46.104874 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:13:46.105542 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.105705 [debug] [MainThread]: On master: BEGIN
[0m06:13:46.105821 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:13:46.111249 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m06:13:46.111449 [debug] [MainThread]: On master: COMMIT
[0m06:13:46.111579 [debug] [MainThread]: Using postgres connection "master"
[0m06:13:46.111698 [debug] [MainThread]: On master: COMMIT
[0m06:13:46.112006 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:13:46.112151 [debug] [MainThread]: On master: Close
[0m06:13:46.112349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:13:46.112465 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:13:46.112573 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:13:46.112676 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:13:46.112774 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:13:46.112901 [info ] [MainThread]: 
[0m06:13:46.113044 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m06:13:46.113318 [debug] [MainThread]: Command end result
[0m06:13:46.125611 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:13:46.126396 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:13:46.128706 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:13:46.128828 [info ] [MainThread]: 
[0m06:13:46.128983 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:13:46.129093 [info ] [MainThread]: 
[0m06:13:46.129213 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:13:46.131617 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.65688837, "process_in_blocks": "0", "process_kernel_time": 0.194825, "process_mem_max_rss": "141230080", "process_out_blocks": "0", "process_user_time": 1.097093}
[0m06:13:46.131979 [debug] [MainThread]: Command `dbt run` succeeded at 06:13:46.131921 after 0.66 seconds
[0m06:13:46.132198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10955e1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1027b8390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029a6c90>]}
[0m06:13:46.132430 [debug] [MainThread]: Flushing usage events
[0m06:13:46.401308 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:14:08.181264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d32a310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d3ab6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d39dfd0>]}


============================== 06:14:08.183148 | 0a3091b0-e9df-443a-b814-d2b5e241fde7 ==============================
[0m06:14:08.183148 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:14:08.183453 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'write_json': 'True', 'warn_error': 'None', 'empty': 'False', 'no_print': 'None', 'use_colors': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'printer_width': '80', 'partial_parse': 'True', 'quiet': 'False', 'fail_fast': 'False', 'introspect': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select stg_prospect_matches', 'target_path': 'None'}
[0m06:14:08.267027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109047610>]}
[0m06:14:08.296553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cf6110>]}
[0m06:14:08.296993 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:14:08.339373 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:14:08.396335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:14:08.396713 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:14:08.523206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2f6890>]}
[0m06:14:08.587662 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:14:08.588520 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:14:08.595350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d93c5d0>]}
[0m06:14:08.595594 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:14:08.595749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6f2190>]}
[0m06:14:08.596448 [info ] [MainThread]: 
[0m06:14:08.596606 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:14:08.596730 [info ] [MainThread]: 
[0m06:14:08.596936 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:14:08.597286 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:14:08.619432 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:14:08.619656 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:14:08.619787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:08.641806 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.022 seconds
[0m06:14:08.642563 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:14:08.644887 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m06:14:08.645134 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:14:08.647780 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:14:08.647959 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:14:08.648182 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:14:08.649147 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:14:08.649311 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:14:08.650322 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:14:08.651039 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:14:08.651164 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:14:08.651295 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:08.651402 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:14:08.651509 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:14:08.651616 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:08.651821 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:08.651967 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:08.664139 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:14:08.664296 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:14:08.664471 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:14:08.664589 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:14:08.664691 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:14:08.664820 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:14:08.664941 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:14:08.665051 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:14:08.665194 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:14:08.665354 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:14:08.665510 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:14:08.665656 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:14:08.669299 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m06:14:08.669426 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:14:08.669990 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:14:08.670130 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:14:08.670239 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m06:14:08.670651 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:14:08.671163 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:14:08.671304 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:14:08.671709 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:14:08.671964 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now list_finny_db_public_marts)
[0m06:14:08.672918 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:14:08.673040 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:14:08.673143 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:14:08.673252 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:14:08.673340 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:14:08.673523 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public, now list_finny_db_public_staging_analysis)
[0m06:14:08.674136 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:08.675471 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:14:08.675740 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:14:08.675874 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:08.684418 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:14:08.684690 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:14:08.684867 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:14:08.685072 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:14:08.685311 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:14:08.685557 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:14:08.688260 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m06:14:08.688470 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m06:14:08.689105 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:14:08.689579 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:14:08.690301 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:14:08.690451 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:14:08.694050 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:08.694198 [debug] [MainThread]: On master: BEGIN
[0m06:14:08.694303 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:14:08.700752 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:14:08.701004 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:08.701219 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:14:08.703200 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:14:08.704264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eaf7650>]}
[0m06:14:08.704525 [debug] [MainThread]: On master: ROLLBACK
[0m06:14:08.704962 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:08.705151 [debug] [MainThread]: On master: BEGIN
[0m06:14:08.705796 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:14:08.705962 [debug] [MainThread]: On master: COMMIT
[0m06:14:08.706106 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:08.706215 [debug] [MainThread]: On master: COMMIT
[0m06:14:08.706603 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:14:08.706793 [debug] [MainThread]: On master: Close
[0m06:14:08.709250 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:14:08.709575 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:14:08.709991 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_prospect_matches)
[0m06:14:08.710208 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:14:08.713420 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:14:08.713749 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:14:08.728912 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:14:08.729462 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:08.729633 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:14:08.729781 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:14:08.735949 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:14:08.736168 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:08.736357 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:14:10.629454 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.893 seconds
[0m06:14:10.641762 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:10.642098 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:14:10.642935 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:14:10.644300 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:10.644454 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:14:10.645055 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:14:10.657233 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:14:10.657497 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:10.657650 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:14:10.659449 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m06:14:10.663077 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:14:10.665988 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:10.666171 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:14:10.667222 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:14:10.668364 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:14:10.669458 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a3091b0-e9df-443a-b814-d2b5e241fde7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb9aad0>]}
[0m06:14:10.669807 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 32[0m in 1.96s]
[0m06:14:10.670095 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:14:10.670871 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:10.671091 [debug] [MainThread]: On master: BEGIN
[0m06:14:10.671238 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:14:10.678529 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:14:10.678757 [debug] [MainThread]: On master: COMMIT
[0m06:14:10.678919 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:10.679067 [debug] [MainThread]: On master: COMMIT
[0m06:14:10.679459 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:14:10.679597 [debug] [MainThread]: On master: Close
[0m06:14:10.679796 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:14:10.679942 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:14:10.680060 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:14:10.680173 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:14:10.680282 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:14:10.680431 [info ] [MainThread]: 
[0m06:14:10.680764 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.08 seconds (2.08s).
[0m06:14:10.681158 [debug] [MainThread]: Command end result
[0m06:14:10.696657 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:14:10.697584 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:14:10.700526 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:14:10.700701 [info ] [MainThread]: 
[0m06:14:10.700917 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:14:10.701073 [info ] [MainThread]: 
[0m06:14:10.701234 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:14:10.703777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.5564265, "process_in_blocks": "0", "process_kernel_time": 0.187685, "process_mem_max_rss": "141099008", "process_out_blocks": "0", "process_user_time": 1.091538}
[0m06:14:10.704006 [debug] [MainThread]: Command `dbt run` succeeded at 06:14:10.703969 after 2.56 seconds
[0m06:14:10.704178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051b8110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105282a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051ef750>]}
[0m06:14:10.704338 [debug] [MainThread]: Flushing usage events
[0m06:14:10.927598 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:14:58.208532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7b9090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b833890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b833f90>]}


============================== 06:14:58.210318 | e52417bb-1046-4648-8dbd-086138a2fa3d ==============================
[0m06:14:58.210318 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:14:58.210615 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'version_check': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'introspect': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'partial_parse': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'fail_fast': 'False', 'log_cache_events': 'False', 'quiet': 'False', 'printer_width': '80', 'debug': 'False', 'write_json': 'True', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'no_print': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs'}
[0m06:14:58.293119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b7b2ad0>]}
[0m06:14:58.322128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107525750>]}
[0m06:14:58.322526 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:14:58.363732 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:14:58.419119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:14:58.419425 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:14:58.541358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c707750>]}
[0m06:14:58.603691 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:14:58.604549 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:14:58.611278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdad190>]}
[0m06:14:58.611507 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:14:58.611667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9ef650>]}
[0m06:14:58.612366 [info ] [MainThread]: 
[0m06:14:58.612530 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:14:58.612655 [info ] [MainThread]: 
[0m06:14:58.612852 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:14:58.613224 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:14:58.635732 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:14:58.636010 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:14:58.636142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:58.656348 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.020 seconds
[0m06:14:58.656967 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:14:58.659109 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m06:14:58.659363 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:14:58.659610 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:14:58.661982 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:14:58.662224 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:14:58.663072 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:14:58.664023 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:14:58.664161 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:14:58.664855 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:14:58.664984 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:14:58.665097 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:14:58.665210 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:58.665325 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:14:58.665446 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:58.665551 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:58.665748 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:14:58.675861 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:14:58.676007 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:14:58.676118 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:14:58.676242 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:14:58.676358 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:14:58.676501 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:14:58.676650 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:14:58.676779 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:14:58.676998 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:14:58.678037 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:14:58.678144 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:14:58.678262 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:14:58.679873 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m06:14:58.680025 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:14:58.680504 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:14:58.680619 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m06:14:58.681019 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:14:58.681473 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:14:58.681609 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:14:58.681839 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m06:14:58.682615 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:14:58.682736 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:14:58.682852 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:14:58.682959 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:14:58.683220 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:14:58.683449 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw)
[0m06:14:58.684193 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:14:58.684372 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:58.685399 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:14:58.686100 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:14:58.686299 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:14:58.686447 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:14:58.692152 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m06:14:58.692348 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:14:58.692496 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:14:58.692639 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:14:58.692788 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:14:58.692951 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:14:58.695021 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:14:58.695163 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:14:58.695748 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:14:58.696160 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:14:58.696628 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:14:58.696789 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:14:58.699600 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:58.699758 [debug] [MainThread]: On master: BEGIN
[0m06:14:58.699891 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:14:58.706465 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:14:58.706623 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:58.706831 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:14:58.709080 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:14:58.710455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf54610>]}
[0m06:14:58.710791 [debug] [MainThread]: On master: ROLLBACK
[0m06:14:58.711283 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:58.711468 [debug] [MainThread]: On master: BEGIN
[0m06:14:58.712028 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:14:58.712209 [debug] [MainThread]: On master: COMMIT
[0m06:14:58.712345 [debug] [MainThread]: Using postgres connection "master"
[0m06:14:58.712463 [debug] [MainThread]: On master: COMMIT
[0m06:14:58.712781 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:14:58.712918 [debug] [MainThread]: On master: Close
[0m06:14:58.714099 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:14:58.714404 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:14:58.714827 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:14:58.715055 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:14:58.718038 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:14:58.718380 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:14:58.736747 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:14:58.737175 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:58.737321 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:14:58.737574 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:14:58.743689 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:14:58.743944 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:14:58.744146 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:19:44.059581 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:19:44.061732 [debug] [MainThread]: Postgres adapter: Cancelling query 'model.dbt_service.stg_prospect_matches' (2130)
[0m06:19:44.062049 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:44.062340 [debug] [MainThread]: On master: BEGIN
[0m06:19:44.062467 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:19:44.069380 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:19:44.069730 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:44.069935 [debug] [MainThread]: On master: select pg_terminate_backend(2130)
[0m06:19:44.070715 [debug] [MainThread]: SQL status: SELECT 1 in 0.001 seconds
[0m06:19:44.070921 [debug] [MainThread]: Postgres adapter: Cancel query 'model.dbt_service.stg_prospect_matches': (True,)
[0m06:19:44.071135 [error] [MainThread]: CANCEL query model.dbt_service.stg_prospect_matches ............................ [[31mCANCEL[0m]
[0m06:19:44.071335 [error] [MainThread]: CANCEL query list_finny_db_public_staging_analysis ............................. [[31mCANCEL[0m]
[0m06:19:44.071477 [error] [MainThread]: CANCEL query list_finny_db_public_raw_analysis ................................. [[31mCANCEL[0m]
[0m06:19:44.071606 [error] [MainThread]: CANCEL query list_finny_db_public .............................................. [[31mCANCEL[0m]
[0m06:19:44.071759 [debug] [MainThread]: On master: ROLLBACK
[0m06:19:44.072132 [debug] [MainThread]: On master: Close
[0m06:19:44.073206 [debug] [Thread-1 (]: Postgres adapter: Postgres error: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[0m06:19:44.073401 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: ROLLBACK
[0m06:19:44.074969 [debug] [Thread-1 (]: Failed to rollback 'model.dbt_service.stg_prospect_matches'
[0m06:19:44.075361 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:19:44.081178 [debug] [Thread-1 (]: Database Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  server closed the connection unexpectedly
  	This probably means the server terminated abnormally
  	before or while processing the request.
  compiled code at target/run/dbt_service/models/staging/stg_prospect_matches.sql
[0m06:19:44.082637 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e52417bb-1046-4648-8dbd-086138a2fa3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd090d0>]}
[0m06:19:44.083060 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model public_staging.stg_prospect_matches ...... [[31mERROR[0m in 285.37s]
[0m06:19:44.083575 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:19:44.083811 [info ] [MainThread]: 
[0m06:19:44.084063 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m06:19:44.084240 [info ] [MainThread]: 
[0m06:19:44.084475 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=0
[0m06:19:44.084688 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:19:44.084870 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:19:44.085050 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:19:44.085220 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:19:44.085385 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:19:44.085584 [info ] [MainThread]: 
[0m06:19:44.085793 [info ] [MainThread]: Finished running  in 0 hours 4 minutes and 45.47 seconds (285.47s).
[0m06:19:44.086130 [error] [MainThread]: Encountered an error:

[0m06:19:44.088944 [error] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 373, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 390, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/main.py", line 587, in run
    results = task.run()
              ^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 599, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 536, in execute_with_hooks
    res = self.execute_nodes()
          ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 416, in execute_nodes
    self.run_queue(pool)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 343, in run_queue
    self.job_queue.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/graph/queue.py", line 206, in join
    self.inner.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", line 327, in wait
    waiter.acquire()
KeyboardInterrupt

[0m06:19:44.093319 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 285.90945, "process_in_blocks": "0", "process_kernel_time": 0.180571, "process_mem_max_rss": "142655488", "process_out_blocks": "0", "process_user_time": 1.023284}
[0m06:19:44.094410 [debug] [MainThread]: Command `dbt run` failed at 06:19:44.094347 after 285.91 seconds
[0m06:19:44.094895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b825e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050680d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105132810>]}
[0m06:19:44.095133 [debug] [MainThread]: Flushing usage events
[0m06:19:44.386244 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:19:53.080106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10682aed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068abad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068abf90>]}


============================== 06:19:53.082350 | 93a34c2f-66a2-4810-9c3f-75e86ba8ce05 ==============================
[0m06:19:53.082350 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:19:53.082636 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'invocation_command': 'dbt run --select stg_prospect_matches', 'use_experimental_parser': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'quiet': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'partial_parse': 'True', 'log_format': 'default', 'fail_fast': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m06:19:53.170316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069ae410>]}
[0m06:19:53.200745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102bf1690>]}
[0m06:19:53.201601 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:19:53.248135 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:19:53.310546 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:19:53.310856 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:19:53.432499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107686a50>]}
[0m06:19:53.494560 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:19:53.495617 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:19:53.507269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f33a50>]}
[0m06:19:53.507495 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:19:53.507654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bbff10>]}
[0m06:19:53.508345 [info ] [MainThread]: 
[0m06:19:53.508496 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:19:53.508610 [info ] [MainThread]: 
[0m06:19:53.508810 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:19:53.509190 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:19:53.535728 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:19:53.535973 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:19:53.536113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:53.556919 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m06:19:53.557472 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:19:53.559607 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m06:19:53.559867 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:19:53.562224 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:19:53.562409 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:19:53.562629 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:19:53.563529 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:19:53.563687 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:19:53.564623 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:19:53.565379 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:19:53.565514 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:19:53.565636 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:19:53.565751 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:19:53.565863 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:19:53.565976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:53.566172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:53.566305 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:19:53.576028 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m06:19:53.576224 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:19:53.576354 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:19:53.577036 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:19:53.577186 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:19:53.577292 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:19:53.577389 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m06:19:53.577499 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:19:53.577629 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:19:53.577756 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:19:53.577895 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:19:53.578067 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:19:53.581054 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:19:53.581568 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:19:53.581694 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m06:19:53.581831 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.005 seconds
[0m06:19:53.582242 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:19:53.582368 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:19:53.582475 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:19:53.582869 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:19:53.583361 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:19:53.583498 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:19:53.583726 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public)
[0m06:19:53.584160 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:19:53.584436 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m06:19:53.585393 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:19:53.585868 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:19:53.586818 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:19:53.587124 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:19:53.587358 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:19:53.588004 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:19:53.588277 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:19:53.594480 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:19:53.594668 [debug] [ThreadPool]: SQL status: BEGIN in 0.006 seconds
[0m06:19:53.594881 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:19:53.595041 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:19:53.595197 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:19:53.595361 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:19:53.597350 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:19:53.597545 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:19:53.598153 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:19:53.598766 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:19:53.599234 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:19:53.599433 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:19:53.602760 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:53.602962 [debug] [MainThread]: On master: BEGIN
[0m06:19:53.603082 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:19:53.614290 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m06:19:53.614529 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:53.614715 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:19:53.616679 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:19:53.617988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e755d0>]}
[0m06:19:53.618249 [debug] [MainThread]: On master: ROLLBACK
[0m06:19:53.618661 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:53.618784 [debug] [MainThread]: On master: BEGIN
[0m06:19:53.619321 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:19:53.619476 [debug] [MainThread]: On master: COMMIT
[0m06:19:53.619604 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:53.619719 [debug] [MainThread]: On master: COMMIT
[0m06:19:53.620064 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:19:53.620265 [debug] [MainThread]: On master: Close
[0m06:19:53.622065 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:19:53.622355 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:19:53.622722 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:19:53.622904 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:19:53.626470 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:19:53.626979 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:19:53.642500 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:19:53.643018 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:53.643178 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:19:53.643316 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:19:53.649658 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:19:53.649964 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:53.650193 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:19:55.557600 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.906 seconds
[0m06:19:55.570633 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:55.570902 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:19:55.571904 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:19:55.573316 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:55.573494 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:19:55.574074 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:19:55.581572 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:19:55.581775 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:55.581918 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:19:55.582706 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:19:55.585038 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:19:55.587105 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:19:55.587263 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:19:55.588516 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:19:55.589558 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:19:55.590518 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93a34c2f-66a2-4810-9c3f-75e86ba8ce05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e967d0>]}
[0m06:19:55.590838 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 32[0m in 1.97s]
[0m06:19:55.591076 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:19:55.591960 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:55.592179 [debug] [MainThread]: On master: BEGIN
[0m06:19:55.592308 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:19:55.598394 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:19:55.598630 [debug] [MainThread]: On master: COMMIT
[0m06:19:55.598778 [debug] [MainThread]: Using postgres connection "master"
[0m06:19:55.598900 [debug] [MainThread]: On master: COMMIT
[0m06:19:55.599200 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:19:55.599326 [debug] [MainThread]: On master: Close
[0m06:19:55.599514 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:19:55.599631 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:19:55.599739 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:19:55.599967 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:19:55.600110 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:19:55.600254 [info ] [MainThread]: 
[0m06:19:55.600411 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.09 seconds (2.09s).
[0m06:19:55.600687 [debug] [MainThread]: Command end result
[0m06:19:55.617035 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:19:55.618093 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:19:55.621542 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:19:55.621771 [info ] [MainThread]: 
[0m06:19:55.621958 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:19:55.622097 [info ] [MainThread]: 
[0m06:19:55.622273 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:19:55.624574 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.5781186, "process_in_blocks": "0", "process_kernel_time": 0.194686, "process_mem_max_rss": "139771904", "process_out_blocks": "0", "process_user_time": 1.074873}
[0m06:19:55.624904 [debug] [MainThread]: Command `dbt run` succeeded at 06:19:55.624848 after 2.58 seconds
[0m06:19:55.625144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100914390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068abdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1009e2c10>]}
[0m06:19:55.625314 [debug] [MainThread]: Flushing usage events
[0m06:19:55.849544 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:20:05.891936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083b2e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108426210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108437e90>]}


============================== 06:20:05.893692 | 58f84ca8-351b-4ddd-8a5f-dd1844c5d359 ==============================
[0m06:20:05.893692 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:20:05.894048 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select stg_prospect_matches', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'partial_parse': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'no_print': 'None', 'empty': 'False', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'cache_selected_only': 'False', 'printer_width': '80', 'fail_fast': 'False', 'quiet': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'version_check': 'True', 'write_json': 'True', 'introspect': 'True'}
[0m06:20:05.976171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108acae50>]}
[0m06:20:06.005667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b773d0>]}
[0m06:20:06.006038 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:20:06.048015 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:20:06.102401 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:20:06.102705 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:20:06.224205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a09f10>]}
[0m06:20:06.288244 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:20:06.289041 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:20:06.295444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b41950>]}
[0m06:20:06.295678 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:20:06.295844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e065d0>]}
[0m06:20:06.296536 [info ] [MainThread]: 
[0m06:20:06.296696 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:20:06.296813 [info ] [MainThread]: 
[0m06:20:06.297010 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:20:06.297363 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:20:06.315947 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:20:06.316197 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:20:06.316335 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:20:06.336202 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.020 seconds
[0m06:20:06.336922 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:20:06.339235 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m06:20:06.339506 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:20:06.342128 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:20:06.342322 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:20:06.342558 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:20:06.343527 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:20:06.343694 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:20:06.344704 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:20:06.345423 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:20:06.345557 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:20:06.345685 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:20:06.345807 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:20:06.345919 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:20:06.346035 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:20:06.346242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:20:06.346377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:20:06.358491 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:20:06.358643 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:20:06.358790 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:20:06.358909 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:20:06.359015 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:20:06.359137 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:20:06.359253 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:20:06.359372 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:20:06.359494 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:20:06.359649 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:20:06.359801 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:20:06.359946 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:20:06.363026 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m06:20:06.363174 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m06:20:06.363296 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:20:06.363399 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:20:06.363933 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:20:06.364368 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:20:06.364780 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:20:06.365179 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:20:06.366035 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:20:06.366149 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:20:06.366252 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:20:06.366356 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:20:06.366569 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now list_finny_db_public_raw)
[0m06:20:06.366988 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now list_finny_db_public_marts)
[0m06:20:06.368805 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:20:06.369537 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:20:06.369658 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:20:06.369768 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:20:06.369875 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:20:06.369982 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:20:06.377073 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:20:06.377323 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m06:20:06.377595 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:20:06.377734 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:20:06.377880 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:20:06.378036 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:20:06.380126 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:20:06.380265 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:20:06.380849 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:20:06.381356 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:20:06.381891 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:20:06.382138 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:20:06.385076 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:06.385247 [debug] [MainThread]: On master: BEGIN
[0m06:20:06.385366 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:20:06.394108 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m06:20:06.394510 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:06.395085 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:20:06.398132 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m06:20:06.399335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a0a050>]}
[0m06:20:06.399555 [debug] [MainThread]: On master: ROLLBACK
[0m06:20:06.399986 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:06.400102 [debug] [MainThread]: On master: BEGIN
[0m06:20:06.400611 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:20:06.400774 [debug] [MainThread]: On master: COMMIT
[0m06:20:06.401023 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:06.401187 [debug] [MainThread]: On master: COMMIT
[0m06:20:06.401627 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:20:06.401788 [debug] [MainThread]: On master: Close
[0m06:20:06.403333 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:20:06.403670 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:20:06.404048 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:20:06.404219 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:20:06.407518 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:20:06.407869 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:20:06.422545 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:20:06.422978 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:20:06.423136 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:20:06.423361 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:20:06.429285 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:20:06.429519 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:20:06.429711 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:20:57.913925 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:20:57.914769 [debug] [MainThread]: Postgres adapter: Cancelling query 'model.dbt_service.stg_prospect_matches' (2165)
[0m06:20:57.914947 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:57.915076 [debug] [MainThread]: On master: BEGIN
[0m06:20:57.915189 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:20:57.922682 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:20:57.923184 [debug] [MainThread]: Using postgres connection "master"
[0m06:20:57.923484 [debug] [MainThread]: On master: select pg_terminate_backend(2165)
[0m06:20:57.924159 [debug] [MainThread]: SQL status: SELECT 1 in 0.000 seconds
[0m06:20:57.924620 [debug] [MainThread]: Postgres adapter: Cancel query 'model.dbt_service.stg_prospect_matches': (True,)
[0m06:20:57.925022 [error] [MainThread]: CANCEL query model.dbt_service.stg_prospect_matches ............................ [[31mCANCEL[0m]
[0m06:20:57.925416 [error] [MainThread]: CANCEL query list_finny_db_public .............................................. [[31mCANCEL[0m]
[0m06:20:57.925803 [error] [MainThread]: CANCEL query list_finny_db_public_marts ........................................ [[31mCANCEL[0m]
[0m06:20:57.926356 [error] [MainThread]: CANCEL query list_finny_db_public_staging ...................................... [[31mCANCEL[0m]
[0m06:20:57.926891 [debug] [MainThread]: On master: ROLLBACK
[0m06:20:57.927292 [debug] [Thread-1 (]: Postgres adapter: Postgres error: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[0m06:20:57.927921 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: ROLLBACK
[0m06:20:57.928106 [debug] [MainThread]: On master: Close
[0m06:20:57.929891 [debug] [Thread-1 (]: Failed to rollback 'model.dbt_service.stg_prospect_matches'
[0m06:20:57.930281 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:20:57.934919 [debug] [Thread-1 (]: Database Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  server closed the connection unexpectedly
  	This probably means the server terminated abnormally
  	before or while processing the request.
  compiled code at target/run/dbt_service/models/staging/stg_prospect_matches.sql
[0m06:20:57.936245 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58f84ca8-351b-4ddd-8a5f-dd1844c5d359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4e0390>]}
[0m06:20:57.936730 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model public_staging.stg_prospect_matches ...... [[31mERROR[0m in 51.53s]
[0m06:20:57.937042 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:20:57.937232 [info ] [MainThread]: 
[0m06:20:57.937484 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m06:20:57.937624 [info ] [MainThread]: 
[0m06:20:57.937903 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=0
[0m06:20:57.938217 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:20:57.938328 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:20:57.938426 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:20:57.938523 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:20:57.938612 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:20:57.938739 [info ] [MainThread]: 
[0m06:20:57.938880 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 51.64 seconds (51.64s).
[0m06:20:57.939060 [error] [MainThread]: Encountered an error:

[0m06:20:57.941436 [error] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 373, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 390, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/main.py", line 587, in run
    results = task.run()
              ^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 599, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 536, in execute_with_hooks
    res = self.execute_nodes()
          ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 416, in execute_nodes
    self.run_queue(pool)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 343, in run_queue
    self.job_queue.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/graph/queue.py", line 206, in join
    self.inner.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", line 327, in wait
    waiter.acquire()
KeyboardInterrupt

[0m06:20:57.943985 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 52.08487, "process_in_blocks": "0", "process_kernel_time": 0.158341, "process_mem_max_rss": "142000128", "process_out_blocks": "0", "process_user_time": 1.033284}
[0m06:20:57.944414 [debug] [MainThread]: Command `dbt run` failed at 06:20:57.944346 after 52.09 seconds
[0m06:20:57.944704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10845fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083b2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a587190>]}
[0m06:20:57.944974 [debug] [MainThread]: Flushing usage events
[0m06:20:58.175143 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:21:00.006492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108228390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10829e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082abfd0>]}


============================== 06:21:00.008606 | 71b29d92-2874-416a-8c06-826a5e801f76 ==============================
[0m06:21:00.008606 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:21:00.008906 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'no_print': 'None', 'empty': 'False', 'quiet': 'False', 'printer_width': '80', 'debug': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'fail_fast': 'False', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'invocation_command': 'dbt run --select stg_prospect_matches', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'partial_parse': 'True', 'write_json': 'True', 'use_colors': 'True', 'log_cache_events': 'False'}
[0m06:21:00.097670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087a8c10>]}
[0m06:21:00.127038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10822b3d0>]}
[0m06:21:00.127465 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:21:00.170554 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:21:00.226930 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:21:00.227282 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:21:00.352349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083a3f10>]}
[0m06:21:00.425100 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:00.426437 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:00.434733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109119350>]}
[0m06:21:00.435032 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:21:00.435212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10929f850>]}
[0m06:21:00.436126 [info ] [MainThread]: 
[0m06:21:00.436355 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m06:21:00.436589 [info ] [MainThread]: 
[0m06:21:00.436846 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:21:00.437357 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:21:00.461893 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:21:00.462176 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:21:00.462309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:00.481646 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.019 seconds
[0m06:21:00.482263 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:21:00.484612 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:21:00.484884 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:21:00.485138 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:21:00.487547 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:00.487764 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:21:00.488746 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:00.489748 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:00.489897 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:21:00.490663 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:00.490803 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:21:00.490923 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:21:00.491042 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:21:00.491171 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:21:00.491290 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:00.491410 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:00.491623 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:00.506354 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m06:21:00.506538 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m06:21:00.506681 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m06:21:00.506825 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m06:21:00.507010 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:00.507143 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:00.507275 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:00.507405 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:00.507554 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:21:00.507709 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:21:00.507889 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:21:00.508046 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:21:00.511049 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:21:00.511198 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m06:21:00.511810 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:21:00.511966 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m06:21:00.512075 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m06:21:00.512494 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:21:00.512909 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:21:00.513037 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:21:00.513429 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:21:00.513721 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now list_finny_db_public_raw_analysis)
[0m06:21:00.514878 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:00.515005 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:21:00.515133 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:21:00.515241 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:21:00.515344 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:21:00.515548 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now list_finny_db_public)
[0m06:21:00.516328 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:21:00.517097 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:00.517298 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:21:00.517405 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:21:00.525479 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:21:00.525660 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m06:21:00.525891 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:00.526042 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:00.526175 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:21:00.526318 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:21:00.528641 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:21:00.528829 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:21:00.529554 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:21:00.530021 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:21:00.530545 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:21:00.530707 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:21:00.534292 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:00.534424 [debug] [MainThread]: On master: BEGIN
[0m06:21:00.534524 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:21:00.541007 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:21:00.541314 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:00.541504 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:21:00.543401 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:21:00.544519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9c78d0>]}
[0m06:21:00.544734 [debug] [MainThread]: On master: ROLLBACK
[0m06:21:00.545148 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:00.545267 [debug] [MainThread]: On master: BEGIN
[0m06:21:00.545767 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:21:00.545972 [debug] [MainThread]: On master: COMMIT
[0m06:21:00.546106 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:00.546220 [debug] [MainThread]: On master: COMMIT
[0m06:21:00.546577 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:00.546752 [debug] [MainThread]: On master: Close
[0m06:21:00.548135 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:21:00.548394 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:21:00.548739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:21:00.548908 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:21:00.552434 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:21:00.552843 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:21:00.568512 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:21:00.568882 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:00.569033 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:21:00.569169 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:21:00.575399 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:21:00.575636 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:00.575829 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:21:02.491404 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.915 seconds
[0m06:21:02.504730 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:02.505244 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:21:02.505881 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:21:02.507684 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:02.508037 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:21:02.508705 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:21:02.519946 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:02.520211 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:02.520446 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:02.521632 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:21:02.525765 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:21:02.528471 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:02.528733 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:21:02.530027 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:21:02.531452 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:21:02.532366 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '71b29d92-2874-416a-8c06-826a5e801f76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaabfd0>]}
[0m06:21:02.532704 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 32[0m in 1.98s]
[0m06:21:02.533010 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:21:02.533745 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:02.533934 [debug] [MainThread]: On master: BEGIN
[0m06:21:02.534074 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:21:02.541229 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:21:02.541502 [debug] [MainThread]: On master: COMMIT
[0m06:21:02.541683 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:02.541825 [debug] [MainThread]: On master: COMMIT
[0m06:21:02.542335 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:02.542576 [debug] [MainThread]: On master: Close
[0m06:21:02.542809 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:21:02.542958 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:21:02.543081 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:21:02.543203 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:21:02.543324 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:21:02.543482 [info ] [MainThread]: 
[0m06:21:02.543668 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.11 seconds (2.11s).
[0m06:21:02.543997 [debug] [MainThread]: Command end result
[0m06:21:02.560445 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:02.562278 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:02.565312 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:21:02.565493 [info ] [MainThread]: 
[0m06:21:02.565690 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:21:02.565827 [info ] [MainThread]: 
[0m06:21:02.565975 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:21:02.567871 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.5987265, "process_in_blocks": "0", "process_kernel_time": 0.190281, "process_mem_max_rss": "141230080", "process_out_blocks": "0", "process_user_time": 1.124535}
[0m06:21:02.568209 [debug] [MainThread]: Command `dbt run` succeeded at 06:21:02.568166 after 2.60 seconds
[0m06:21:02.568400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102448390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082d60d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102516cd0>]}
[0m06:21:02.568581 [debug] [MainThread]: Flushing usage events
[0m06:21:03.110332 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:21:11.749252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7b9510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a826190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a82ffd0>]}


============================== 06:21:11.751225 | 292c9206-cd22-4593-a645-702ac80ed5b5 ==============================
[0m06:21:11.751225 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:21:11.751518 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'use_colors': 'True', 'debug': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select stg_prospect_matches', 'target_path': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80', 'static_parser': 'True', 'quiet': 'False', 'empty': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'version_check': 'True'}
[0m06:21:11.836914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109253b10>]}
[0m06:21:11.866303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0133d0>]}
[0m06:21:11.866693 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:21:11.909836 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:21:11.965499 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:21:11.965695 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:21:11.988165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b736510>]}
[0m06:21:12.027112 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:12.027960 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:12.034778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8cc710>]}
[0m06:21:12.035012 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:21:12.035175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad50d10>]}
[0m06:21:12.035911 [info ] [MainThread]: 
[0m06:21:12.036063 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:21:12.036180 [info ] [MainThread]: 
[0m06:21:12.036384 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:21:12.036756 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:21:12.085423 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:21:12.085686 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:21:12.085839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.106323 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.020 seconds
[0m06:21:12.106903 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:21:12.109042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m06:21:12.111639 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:12.111893 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:21:12.112023 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:21:12.112271 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:21:12.112565 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:21:12.113337 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:12.113528 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:21:12.113734 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:21:12.113879 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:21:12.114580 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:12.115302 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:12.115441 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:21:12.116135 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:12.117136 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:12.117344 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:21:12.117493 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:21:12.117623 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.117777 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:21:12.117938 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:21:12.118073 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.118195 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.118427 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.118625 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:12.125415 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:21:12.125540 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:12.125676 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:21:12.127057 [debug] [ThreadPool]: SQL status: BEGIN in 0.009 seconds
[0m06:21:12.127192 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:12.127334 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:21:12.128312 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:21:12.128830 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:21:12.129215 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:21:12.129652 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:21:12.130134 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:21:12.130834 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:21:12.131104 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:21:12.131377 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:21:12.132020 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:21:12.132165 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:12.132300 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:21:12.132494 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:12.132672 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:12.132846 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:21:12.132995 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:12.133153 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:21:12.133309 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:21:12.133506 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:21:12.134894 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m06:21:12.135089 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m06:21:12.135669 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:21:12.135868 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:21:12.135985 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:21:12.136455 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:21:12.137229 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:21:12.137439 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:21:12.137877 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:21:12.138558 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:21:12.138703 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:21:12.138864 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:21:12.142597 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:12.142747 [debug] [MainThread]: On master: BEGIN
[0m06:21:12.142856 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:21:12.148992 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:21:12.149123 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:12.149312 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:21:12.151195 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:21:12.152762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad54950>]}
[0m06:21:12.153014 [debug] [MainThread]: On master: ROLLBACK
[0m06:21:12.153510 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:12.153747 [debug] [MainThread]: On master: BEGIN
[0m06:21:12.154283 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:21:12.154452 [debug] [MainThread]: On master: COMMIT
[0m06:21:12.154584 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:12.154706 [debug] [MainThread]: On master: COMMIT
[0m06:21:12.155054 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:12.155210 [debug] [MainThread]: On master: Close
[0m06:21:12.157161 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:21:12.157488 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:21:12.157707 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.stg_prospect_matches)
[0m06:21:12.157871 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:21:12.161510 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:21:12.161953 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:21:12.177101 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:21:12.177816 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:12.178073 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:21:12.178250 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:21:12.184069 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:21:12.184297 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:12.184495 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:21:14.090604 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.905 seconds
[0m06:21:14.104255 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:14.104611 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:21:14.105354 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:21:14.106819 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:14.107124 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:21:14.107738 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:21:14.118202 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:14.118407 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:14.118563 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:14.120253 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m06:21:14.124228 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:21:14.126772 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:14.127012 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:21:14.128009 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:21:14.129497 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:21:14.130492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '292c9206-cd22-4593-a645-702ac80ed5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0056d0>]}
[0m06:21:14.130885 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 32[0m in 1.97s]
[0m06:21:14.131162 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:21:14.132245 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:14.132398 [debug] [MainThread]: On master: BEGIN
[0m06:21:14.132563 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:21:14.139226 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:21:14.139482 [debug] [MainThread]: On master: COMMIT
[0m06:21:14.139655 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:14.139796 [debug] [MainThread]: On master: COMMIT
[0m06:21:14.140253 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:14.140422 [debug] [MainThread]: On master: Close
[0m06:21:14.140628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:21:14.140853 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:21:14.141104 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:21:14.141321 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:21:14.141465 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:21:14.141598 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:21:14.141719 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:21:14.141892 [info ] [MainThread]: 
[0m06:21:14.142087 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.11 seconds (2.11s).
[0m06:21:14.142466 [debug] [MainThread]: Command end result
[0m06:21:14.157549 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:14.158676 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:14.161851 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:21:14.162037 [info ] [MainThread]: 
[0m06:21:14.162248 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:21:14.162403 [info ] [MainThread]: 
[0m06:21:14.162574 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:21:14.164220 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.448245, "process_in_blocks": "0", "process_kernel_time": 0.176751, "process_mem_max_rss": "134397952", "process_out_blocks": "0", "process_user_time": 1.008947}
[0m06:21:14.164495 [debug] [MainThread]: Command `dbt run` succeeded at 06:21:14.164449 after 2.45 seconds
[0m06:21:14.164691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a85c750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100ca4110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100e76a50>]}
[0m06:21:14.164870 [debug] [MainThread]: Flushing usage events
[0m06:21:14.406205 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:21:25.691894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102312d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11022a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102abc90>]}


============================== 06:21:25.693909 | e67bf877-b2d6-457a-9e93-784c165c10b3 ==============================
[0m06:21:25.693909 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:21:25.694214 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'empty': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'indirect_selection': 'eager', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'quiet': 'False', 'fail_fast': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'log_format': 'default', 'version_check': 'True'}
[0m06:21:25.784799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110308d0>]}
[0m06:21:25.814948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102df84d0>]}
[0m06:21:25.815397 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:21:25.857279 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:21:25.912517 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:21:25.912717 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:21:25.935267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1112e61d0>]}
[0m06:21:25.974140 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:25.974935 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:25.981656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1112f0c50>]}
[0m06:21:25.981877 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:21:25.982040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110b0c50>]}
[0m06:21:25.982748 [info ] [MainThread]: 
[0m06:21:25.982909 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:21:25.983034 [info ] [MainThread]: 
[0m06:21:25.983250 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:21:25.983624 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:21:26.031269 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:21:26.031526 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:21:26.031654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.053725 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.022 seconds
[0m06:21:26.054316 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:21:26.056547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m06:21:26.056815 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:21:26.059278 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:26.059520 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:21:26.059807 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:21:26.060053 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:21:26.061014 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:26.061253 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:21:26.061413 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:21:26.062156 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:26.062845 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:26.063557 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:26.063699 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:21:26.064730 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:26.064857 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:21:26.064980 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:21:26.065096 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:21:26.065206 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:21:26.065315 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.065423 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:21:26.065622 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.065803 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.065936 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.066126 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:21:26.081840 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m06:21:26.082069 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:21:26.082246 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:21:26.083554 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m06:21:26.083709 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m06:21:26.083842 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:21:26.083956 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:21:26.084094 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:21:26.084233 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m06:21:26.084389 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:21:26.084559 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:21:26.084740 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:21:26.085575 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m06:21:26.085691 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:21:26.085799 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m06:21:26.085938 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:21:26.086065 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:21:26.086209 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:21:26.087727 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:21:26.087862 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.005 seconds
[0m06:21:26.088547 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:21:26.088685 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m06:21:26.089342 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:21:26.089996 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:21:26.090260 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m06:21:26.090430 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.006 seconds
[0m06:21:26.090573 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:21:26.090750 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:21:26.090930 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m06:21:26.091611 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:21:26.091736 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:21:26.092217 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:21:26.093244 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:21:26.094252 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:21:26.094466 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:21:26.094684 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:21:26.098327 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:26.098497 [debug] [MainThread]: On master: BEGIN
[0m06:21:26.098617 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:21:26.105277 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:21:26.105434 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:26.105616 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:21:26.107720 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:21:26.109066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11115c950>]}
[0m06:21:26.109314 [debug] [MainThread]: On master: ROLLBACK
[0m06:21:26.109763 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:26.109887 [debug] [MainThread]: On master: BEGIN
[0m06:21:26.110385 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:21:26.110546 [debug] [MainThread]: On master: COMMIT
[0m06:21:26.110677 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:26.110795 [debug] [MainThread]: On master: COMMIT
[0m06:21:26.111127 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:26.111254 [debug] [MainThread]: On master: Close
[0m06:21:26.113564 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:21:26.113872 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_prospect_matches ............... [RUN]
[0m06:21:26.114174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_prospect_matches)
[0m06:21:26.114372 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:21:26.117455 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:21:26.117827 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:21:26.133251 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:21:26.133753 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:26.134036 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:21:26.134192 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:21:26.140500 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:21:26.140863 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:26.141066 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
[0m06:21:28.076770 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.935 seconds
[0m06:21:28.089872 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:28.090370 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:21:28.091088 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:21:28.092383 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:28.092572 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:21:28.093053 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:21:28.101505 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:28.101782 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:28.101927 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:21:28.103531 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:21:28.108467 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:21:28.111707 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:21:28.111982 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:21:28.113016 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:21:28.114313 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:21:28.115119 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e67bf877-b2d6-457a-9e93-784c165c10b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110a2d90>]}
[0m06:21:28.115438 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_prospect_matches .......... [[32mSELECT 32[0m in 2.00s]
[0m06:21:28.115700 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:21:28.116881 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:28.117032 [debug] [MainThread]: On master: BEGIN
[0m06:21:28.117159 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:21:28.123815 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:21:28.124062 [debug] [MainThread]: On master: COMMIT
[0m06:21:28.124230 [debug] [MainThread]: Using postgres connection "master"
[0m06:21:28.124374 [debug] [MainThread]: On master: COMMIT
[0m06:21:28.124814 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:21:28.124956 [debug] [MainThread]: On master: Close
[0m06:21:28.125149 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:21:28.125289 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:21:28.125410 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:21:28.125607 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:21:28.125975 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:21:28.126142 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:21:28.126269 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:21:28.126436 [info ] [MainThread]: 
[0m06:21:28.126594 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.14 seconds (2.14s).
[0m06:21:28.126998 [debug] [MainThread]: Command end result
[0m06:21:28.141970 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:21:28.143147 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:21:28.146833 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:21:28.147090 [info ] [MainThread]: 
[0m06:21:28.147305 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:21:28.147450 [info ] [MainThread]: 
[0m06:21:28.147607 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:21:28.149323 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.492066, "process_in_blocks": "0", "process_kernel_time": 0.18224, "process_mem_max_rss": "136380416", "process_out_blocks": "0", "process_user_time": 1.030915}
[0m06:21:28.149614 [debug] [MainThread]: Command `dbt run` succeeded at 06:21:28.149569 after 2.49 seconds
[0m06:21:28.149849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10759b810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a8c110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100ac3750>]}
[0m06:21:28.150039 [debug] [MainThread]: Flushing usage events
[0m06:21:28.377658 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:23:26.181817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059334d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b3f90>]}


============================== 06:23:26.184535 | 163fdc44-ee93-4c28-b8ef-7ede0c600558 ==============================
[0m06:23:26.184535 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:23:26.184824 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'write_json': 'True', 'invocation_command': 'dbt run --select stg_prospect_matches', 'indirect_selection': 'eager', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'static_parser': 'True', 'debug': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'no_print': 'None', 'cache_selected_only': 'False', 'printer_width': '80', 'introspect': 'True', 'use_colors': 'True', 'empty': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True'}
[0m06:23:26.286964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106051e90>]}
[0m06:23:26.316801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102fca0d0>]}
[0m06:23:26.317517 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:23:26.364532 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:23:26.426410 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:23:26.426699 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:23:26.550120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c08710>]}
[0m06:23:26.610857 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:23:26.613517 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:23:26.624935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066c5690>]}
[0m06:23:26.625158 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:23:26.625313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068bcd10>]}
[0m06:23:26.625986 [info ] [MainThread]: 
[0m06:23:26.626140 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:23:26.626254 [info ] [MainThread]: 
[0m06:23:26.626447 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:23:26.626789 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:23:26.652795 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:23:26.653019 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:23:26.653139 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.682194 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.029 seconds
[0m06:23:26.682743 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:23:26.684839 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m06:23:26.685035 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:23:26.685287 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:23:26.687593 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:23:26.687810 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:23:26.688068 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:23:26.688811 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:23:26.688996 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:23:26.689939 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:23:26.690086 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:23:26.690751 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:23:26.691396 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:23:26.691509 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:23:26.692162 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:23:26.692282 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:23:26.692395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:23:26.692511 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:23:26.692640 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:23:26.692752 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.692873 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:23:26.692989 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.693190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.693343 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.693557 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:23:26.711581 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m06:23:26.711717 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:23:26.711837 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:23:26.713451 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m06:23:26.713660 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:23:26.713862 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:23:26.720949 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.009 seconds
[0m06:23:26.721181 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m06:23:26.721367 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m06:23:26.721495 [debug] [ThreadPool]: SQL status: BEGIN in 0.028 seconds
[0m06:23:26.721628 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.008 seconds
[0m06:23:26.721803 [debug] [ThreadPool]: SQL status: BEGIN in 0.029 seconds
[0m06:23:26.722336 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:23:26.722495 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:23:26.722622 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:23:26.722734 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:23:26.723113 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:23:26.723230 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:23:26.723480 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:23:26.723740 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:23:26.723918 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:23:26.724084 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:23:26.724291 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:23:26.724581 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:23:26.726219 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:23:26.726449 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:23:26.726996 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:23:26.727189 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:23:26.727463 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m06:23:26.728166 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:23:26.728710 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:23:26.728844 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:23:26.729317 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:23:26.730137 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:23:26.730299 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:23:26.730517 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:23:26.733908 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.734039 [debug] [MainThread]: On master: BEGIN
[0m06:23:26.734144 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:23:26.740522 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:23:26.740727 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.740924 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:23:26.743203 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:23:26.744435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dddf90>]}
[0m06:23:26.744682 [debug] [MainThread]: On master: ROLLBACK
[0m06:23:26.745149 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.745307 [debug] [MainThread]: On master: BEGIN
[0m06:23:26.745808 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:23:26.745978 [debug] [MainThread]: On master: COMMIT
[0m06:23:26.746109 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.746231 [debug] [MainThread]: On master: COMMIT
[0m06:23:26.746545 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:23:26.746682 [debug] [MainThread]: On master: Close
[0m06:23:26.748913 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:23:26.749194 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m06:23:26.749430 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:23:26.749597 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:23:26.753325 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:23:26.753730 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:23:26.778166 [debug] [Thread-1 (]: Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter
[0m06:23:26.779307 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '163fdc44-ee93-4c28-b8ef-7ede0c600558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f79010>]}
[0m06:23:26.779646 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model public_staging.stg_prospect_matches  [[31mERROR[0m in 0.03s]
[0m06:23:26.779900 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:23:26.780131 [debug] [Thread-35 ]: Marking all children of 'model.dbt_service.stg_prospect_matches' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter.
[0m06:23:26.781360 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.781520 [debug] [MainThread]: On master: BEGIN
[0m06:23:26.781739 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:23:26.787564 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:23:26.787789 [debug] [MainThread]: On master: COMMIT
[0m06:23:26.787926 [debug] [MainThread]: Using postgres connection "master"
[0m06:23:26.788043 [debug] [MainThread]: On master: COMMIT
[0m06:23:26.788469 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:23:26.788673 [debug] [MainThread]: On master: Close
[0m06:23:26.788868 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:23:26.788989 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:23:26.789099 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:23:26.789206 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:23:26.789308 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:23:26.789400 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:23:26.789490 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:23:26.789623 [info ] [MainThread]: 
[0m06:23:26.789765 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m06:23:26.790026 [debug] [MainThread]: Command end result
[0m06:23:26.802187 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:23:26.802978 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:23:26.805295 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:23:26.805449 [info ] [MainThread]: 
[0m06:23:26.805602 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:23:26.805725 [info ] [MainThread]: 
[0m06:23:26.805867 [error] [MainThread]: [31mFailure in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)[0m
[0m06:23:26.806003 [error] [MainThread]:   Runtime Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  The incremental strategy 'insert_overwrite' is not valid for this adapter
[0m06:23:26.806109 [info ] [MainThread]: 
[0m06:23:26.806245 [info ] [MainThread]:   compiled code at target/compiled/dbt_service/models/staging/stg_prospect_matches.sql
[0m06:23:26.806352 [info ] [MainThread]: 
[0m06:23:26.806474 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m06:23:26.808774 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.66240156, "process_in_blocks": "0", "process_kernel_time": 0.206341, "process_mem_max_rss": "140476416", "process_out_blocks": "0", "process_user_time": 1.070816}
[0m06:23:26.809233 [debug] [MainThread]: Command `dbt run` failed at 06:23:26.809178 after 0.66 seconds
[0m06:23:26.809456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100fcc110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070ba3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927990>]}
[0m06:23:26.809662 [debug] [MainThread]: Flushing usage events
[0m06:23:27.081407 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:24:19.077837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a2a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a9e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a9dfd0>]}


============================== 06:24:19.079858 | fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3 ==============================
[0m06:24:19.079858 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:24:19.080194 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'log_format': 'default', 'version_check': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'use_colors': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False', 'partial_parse': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh'}
[0m06:24:19.165464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128ae290>]}
[0m06:24:19.195754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104df5910>]}
[0m06:24:19.196233 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:24:19.238716 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:24:19.295365 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:24:19.295686 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:24:19.425906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11333f510>]}
[0m06:24:19.489538 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:24:19.490385 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:24:19.497172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106359410>]}
[0m06:24:19.497407 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:24:19.497566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132d8250>]}
[0m06:24:19.498258 [info ] [MainThread]: 
[0m06:24:19.498412 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:24:19.498538 [info ] [MainThread]: 
[0m06:24:19.498756 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:24:19.499128 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:24:19.521435 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:24:19.521713 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:24:19.521897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.543203 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.021 seconds
[0m06:24:19.543824 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:24:19.546005 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m06:24:19.546279 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:24:19.548722 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:24:19.548948 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:24:19.549188 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:24:19.550372 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:24:19.550588 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:24:19.550794 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:24:19.550966 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:24:19.551721 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:24:19.552439 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:24:19.552554 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:24:19.553203 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:24:19.553911 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:24:19.554033 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:24:19.554164 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:24:19.554290 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:24:19.554412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.554534 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:24:19.554648 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:24:19.554863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.555003 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.555242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.555468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:19.571567 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m06:24:19.571712 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:24:19.571830 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:24:19.573879 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m06:24:19.573983 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:24:19.574099 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:24:19.576415 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m06:24:19.576548 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m06:24:19.577130 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:24:19.577366 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m06:24:19.577573 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m06:24:19.577784 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m06:24:19.577977 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m06:24:19.578108 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:24:19.578327 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:24:19.578460 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:24:19.578621 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:24:19.578732 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:24:19.579207 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:24:19.579366 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:24:19.579531 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:24:19.579687 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:24:19.579882 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:24:19.580653 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:24:19.581820 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m06:24:19.582013 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:24:19.582156 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m06:24:19.582278 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:24:19.582873 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:24:19.583321 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:24:19.583700 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:24:19.584114 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:24:19.584626 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:24:19.584886 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:24:19.585053 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:24:19.585181 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:24:19.589452 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:19.589684 [debug] [MainThread]: On master: BEGIN
[0m06:24:19.589806 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:24:19.596826 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:24:19.597071 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:19.597260 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:24:19.599518 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:24:19.600701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1139f0410>]}
[0m06:24:19.600939 [debug] [MainThread]: On master: ROLLBACK
[0m06:24:19.601367 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:19.601495 [debug] [MainThread]: On master: BEGIN
[0m06:24:19.602101 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m06:24:19.602277 [debug] [MainThread]: On master: COMMIT
[0m06:24:19.602405 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:19.602509 [debug] [MainThread]: On master: COMMIT
[0m06:24:19.603040 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:24:19.603280 [debug] [MainThread]: On master: Close
[0m06:24:19.605842 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:24:19.606166 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m06:24:19.606408 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_prospect_matches)
[0m06:24:19.606571 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:24:19.610831 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:24:19.611182 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:24:19.634868 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:24:19.635346 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:19.635490 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:24:19.635623 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:24:19.641644 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:24:19.641978 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:19.642224 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m06:24:21.527895 [debug] [Thread-1 (]: SQL status: SELECT 32 in 1.885 seconds
[0m06:24:21.536150 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:21.536602 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:24:21.537489 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:24:21.539290 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:21.539506 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:24:21.540054 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m06:24:21.550509 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:24:21.550809 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:21.551046 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:24:21.552833 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m06:24:21.556522 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:24:21.559235 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:21.559520 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:24:21.560625 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:24:21.561878 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:24:21.562938 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb93b5a1-9ee3-4e07-a0ee-cbfd84efeed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11286b3d0>]}
[0m06:24:21.563343 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mSELECT 32[0m in 1.96s]
[0m06:24:21.563686 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:24:21.565015 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:21.565356 [debug] [MainThread]: On master: BEGIN
[0m06:24:21.565535 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:24:21.571597 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:24:21.571802 [debug] [MainThread]: On master: COMMIT
[0m06:24:21.571959 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:21.572099 [debug] [MainThread]: On master: COMMIT
[0m06:24:21.572486 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:24:21.572663 [debug] [MainThread]: On master: Close
[0m06:24:21.572880 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:24:21.573016 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:24:21.573135 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:24:21.573252 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:24:21.573366 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:24:21.573475 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:24:21.573600 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:24:21.573756 [info ] [MainThread]: 
[0m06:24:21.573919 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.08 seconds (2.08s).
[0m06:24:21.574237 [debug] [MainThread]: Command end result
[0m06:24:21.589130 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:24:21.590100 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:24:21.593059 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:24:21.593228 [info ] [MainThread]: 
[0m06:24:21.593419 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:24:21.593557 [info ] [MainThread]: 
[0m06:24:21.593703 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:24:21.596047 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.5529127, "process_in_blocks": "0", "process_kernel_time": 0.189691, "process_mem_max_rss": "142114816", "process_out_blocks": "0", "process_user_time": 1.139196}
[0m06:24:21.596474 [debug] [MainThread]: Command `dbt run` succeeded at 06:24:21.596420 after 2.55 seconds
[0m06:24:21.596686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102348110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111aab9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10237f750>]}
[0m06:24:21.596890 [debug] [MainThread]: Flushing usage events
[0m06:24:21.879221 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:24:34.863118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112331090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112378510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123a7f90>]}


============================== 06:24:34.866740 | b946b51b-0c65-4d52-942e-61958f2fadbe ==============================
[0m06:24:34.866740 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:24:34.867051 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'write_json': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'partial_parse': 'True', 'version_check': 'True', 'static_parser': 'True', 'no_print': 'None', 'target_path': 'None', 'printer_width': '80', 'invocation_command': 'dbt run --select stg_prospect_matches', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False'}
[0m06:24:34.965766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11155c050>]}
[0m06:24:34.996184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a69450>]}
[0m06:24:34.997024 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:24:35.043334 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:24:35.108362 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:24:35.108590 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:24:35.131523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113413590>]}
[0m06:24:35.171273 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:24:35.172352 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:24:35.183064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11361bf10>]}
[0m06:24:35.183304 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:24:35.183469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11356bb10>]}
[0m06:24:35.184219 [info ] [MainThread]: 
[0m06:24:35.184372 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:24:35.184490 [info ] [MainThread]: 
[0m06:24:35.184704 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:24:35.185067 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:24:35.232549 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:24:35.232786 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:24:35.232929 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.262876 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.030 seconds
[0m06:24:35.263439 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:24:35.265489 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m06:24:35.265707 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:24:35.265949 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:24:35.268329 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:24:35.268631 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m06:24:35.268866 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:24:35.269086 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:24:35.269855 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:24:35.270613 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:24:35.270739 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:24:35.271404 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:24:35.272087 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:24:35.273061 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:24:35.273188 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:24:35.273301 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:24:35.273417 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:24:35.273536 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:24:35.273652 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:24:35.273763 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:24:35.273874 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.274030 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.274257 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.274398 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.274525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:24:35.286204 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:24:35.286350 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:24:35.286493 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:24:35.289629 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m06:24:35.290177 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:24:35.292007 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:24:35.292279 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m06:24:35.292471 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m06:24:35.292734 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m06:24:35.292905 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m06:24:35.293179 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m06:24:35.293334 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:24:35.293489 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:24:35.293637 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:24:35.293766 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:24:35.293890 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:24:35.294038 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:24:35.294193 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:24:35.294347 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:24:35.294498 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:24:35.294658 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:24:35.296363 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m06:24:35.296635 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:24:35.296870 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:24:35.297031 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:24:35.297154 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m06:24:35.297750 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:24:35.298235 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:24:35.298771 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:24:35.299352 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:24:35.299814 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:24:35.300445 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:24:35.300650 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:24:35.300791 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:24:35.300940 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:24:35.301052 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:24:35.305880 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.306077 [debug] [MainThread]: On master: BEGIN
[0m06:24:35.306207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:24:35.313021 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:24:35.313242 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.313433 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:24:35.315781 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:24:35.317727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114113490>]}
[0m06:24:35.317989 [debug] [MainThread]: On master: ROLLBACK
[0m06:24:35.318475 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.318624 [debug] [MainThread]: On master: BEGIN
[0m06:24:35.319340 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m06:24:35.319500 [debug] [MainThread]: On master: COMMIT
[0m06:24:35.319629 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.319745 [debug] [MainThread]: On master: COMMIT
[0m06:24:35.320256 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:24:35.320400 [debug] [MainThread]: On master: Close
[0m06:24:35.323115 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:24:35.323488 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m06:24:35.323700 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m06:24:35.323852 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:24:35.329163 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:24:35.329503 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:24:35.350665 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.350949 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp062435341246"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 1000  -- Increase to 1000 records for better chance of matches
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m06:24:35.351136 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:24:35.424882 [debug] [Thread-1 (]: SQL status: SELECT 0 in 0.074 seconds
[0m06:24:35.432472 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.432717 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:24:35.433326 [debug] [Thread-1 (]: SQL status: BEGIN in 0.000 seconds
[0m06:24:35.433560 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.433848 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp062435341246'
        
      order by ordinal_position

  
[0m06:24:35.437669 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.004 seconds
[0m06:24:35.440129 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.440370 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m06:24:35.442190 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.002 seconds
[0m06:24:35.448472 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.448769 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp062435341246'
        
      order by ordinal_position

  
[0m06:24:35.450511 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.002 seconds
[0m06:24:35.451827 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.452026 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m06:24:35.453302 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.001 seconds
[0m06:24:35.457221 [debug] [Thread-1 (]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m06:24:35.463486 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:24:35.463945 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.464116 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp062435341246" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp062435341246"
    )
  
[0m06:24:35.464893 [debug] [Thread-1 (]: SQL status: INSERT 0 0 in 0.001 seconds
[0m06:24:35.471472 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:24:35.471741 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:24:35.471915 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:24:35.472836 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:24:35.473954 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:24:35.474994 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b946b51b-0c65-4d52-942e-61958f2fadbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112808050>]}
[0m06:24:35.475350 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mINSERT 0 0[0m in 0.15s]
[0m06:24:35.475682 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:24:35.476687 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.476863 [debug] [MainThread]: On master: BEGIN
[0m06:24:35.477000 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:24:35.482953 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m06:24:35.483177 [debug] [MainThread]: On master: COMMIT
[0m06:24:35.483338 [debug] [MainThread]: Using postgres connection "master"
[0m06:24:35.483473 [debug] [MainThread]: On master: COMMIT
[0m06:24:35.483859 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:24:35.484018 [debug] [MainThread]: On master: Close
[0m06:24:35.484221 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:24:35.484350 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:24:35.484465 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:24:35.484577 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:24:35.484684 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m06:24:35.484797 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:24:35.484908 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:24:35.485047 [info ] [MainThread]: 
[0m06:24:35.485204 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m06:24:35.485510 [debug] [MainThread]: Command end result
[0m06:24:35.498798 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:24:35.499641 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:24:35.502069 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:24:35.502203 [info ] [MainThread]: 
[0m06:24:35.502375 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:24:35.502497 [info ] [MainThread]: 
[0m06:24:35.502633 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:24:35.504845 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.67651975, "process_in_blocks": "0", "process_kernel_time": 0.196589, "process_mem_max_rss": "138231808", "process_out_blocks": "0", "process_user_time": 1.016454}
[0m06:24:35.505098 [debug] [MainThread]: Command `dbt run` succeeded at 06:24:35.505060 after 0.68 seconds
[0m06:24:35.505266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f30390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123d6310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ffecd0>]}
[0m06:24:35.505425 [debug] [MainThread]: Flushing usage events
[0m06:24:35.732527 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:25:23.114647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111b1290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111224c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111b13d0>]}


============================== 06:25:23.116781 | 53fff151-6431-4dec-aa6d-525d93e7b3d9 ==============================
[0m06:25:23.116781 [info ] [MainThread]: Running with dbt=1.10.13
[0m06:25:23.117081 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh', 'quiet': 'False', 'printer_width': '80', 'partial_parse': 'True', 'no_print': 'None', 'fail_fast': 'False', 'static_parser': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'log_format': 'default', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs'}
[0m06:25:23.201431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111283450>]}
[0m06:25:23.230848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e398d0>]}
[0m06:25:23.231275 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m06:25:23.273151 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m06:25:23.329696 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:25:23.330018 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m06:25:23.459024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112caf750>]}
[0m06:25:23.523910 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:25:23.524770 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:25:23.531313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b65d0>]}
[0m06:25:23.531550 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m06:25:23.531741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c34a10>]}
[0m06:25:23.532408 [info ] [MainThread]: 
[0m06:25:23.532562 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m06:25:23.532682 [info ] [MainThread]: 
[0m06:25:23.532871 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m06:25:23.533231 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m06:25:23.555607 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m06:25:23.555809 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m06:25:23.555931 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.578289 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.022 seconds
[0m06:25:23.578925 [debug] [ThreadPool]: On list_finny_db: Close
[0m06:25:23.581043 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m06:25:23.581311 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m06:25:23.583493 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:25:23.583691 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m06:25:23.583955 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m06:25:23.584154 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m06:25:23.584348 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m06:25:23.585308 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:25:23.585463 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m06:25:23.586179 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:25:23.586860 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:25:23.587620 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:25:23.588290 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:25:23.588421 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m06:25:23.588554 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:25:23.588672 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m06:25:23.588785 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m06:25:23.588906 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m06:25:23.589020 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m06:25:23.589132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.589333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.589470 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.589685 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.589854 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:25:23.597057 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m06:25:23.597199 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m06:25:23.597336 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m06:25:23.599984 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:25:23.600540 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m06:25:23.601169 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m06:25:23.601908 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m06:25:23.602102 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m06:25:23.602257 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m06:25:23.603100 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:25:23.603312 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m06:25:23.603501 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:25:23.603623 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m06:25:23.603789 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m06:25:23.603927 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m06:25:23.604050 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m06:25:23.604168 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m06:25:23.604288 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m06:25:23.604423 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m06:25:23.604919 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m06:25:23.605055 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m06:25:23.605196 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m06:25:23.605334 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m06:25:23.605789 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m06:25:23.607039 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m06:25:23.607679 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m06:25:23.607864 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.002 seconds
[0m06:25:23.607994 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m06:25:23.608114 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m06:25:23.608691 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m06:25:23.608822 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m06:25:23.609306 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m06:25:23.609696 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m06:25:23.610423 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m06:25:23.610548 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m06:25:23.611043 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m06:25:23.614051 [debug] [MainThread]: Using postgres connection "master"
[0m06:25:23.614251 [debug] [MainThread]: On master: BEGIN
[0m06:25:23.614389 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:25:23.623005 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m06:25:23.623343 [debug] [MainThread]: Using postgres connection "master"
[0m06:25:23.623526 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m06:25:23.626072 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m06:25:23.627356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11201e550>]}
[0m06:25:23.627592 [debug] [MainThread]: On master: ROLLBACK
[0m06:25:23.628062 [debug] [MainThread]: Using postgres connection "master"
[0m06:25:23.628224 [debug] [MainThread]: On master: BEGIN
[0m06:25:23.628908 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m06:25:23.629162 [debug] [MainThread]: On master: COMMIT
[0m06:25:23.629327 [debug] [MainThread]: Using postgres connection "master"
[0m06:25:23.629448 [debug] [MainThread]: On master: COMMIT
[0m06:25:23.629775 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:25:23.629977 [debug] [MainThread]: On master: Close
[0m06:25:23.631923 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m06:25:23.632256 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m06:25:23.632488 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.stg_prospect_matches)
[0m06:25:23.632653 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m06:25:23.637455 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m06:25:23.637850 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m06:25:23.659849 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m06:25:23.660265 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:25:23.660415 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m06:25:23.660553 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:25:23.666536 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m06:25:23.666848 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:25:23.667073 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m06:28:38.213599 [debug] [Thread-1 (]: SQL status: SELECT 1908 in 194.545 seconds
[0m06:28:38.230473 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:28:38.231124 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m06:28:38.232337 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:28:38.234709 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:28:38.235020 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m06:28:38.235815 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m06:28:38.246815 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:28:38.247127 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:28:38.247362 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m06:28:38.248319 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m06:28:38.251291 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m06:28:38.253786 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m06:28:38.254029 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m06:28:38.255157 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m06:28:38.256482 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m06:28:38.258048 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53fff151-6431-4dec-aa6d-525d93e7b3d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bb1950>]}
[0m06:28:38.258478 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mSELECT 1908[0m in 194.62s]
[0m06:28:38.258787 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m06:28:38.260186 [debug] [MainThread]: Using postgres connection "master"
[0m06:28:38.260387 [debug] [MainThread]: On master: BEGIN
[0m06:28:38.260557 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m06:28:38.267481 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m06:28:38.267719 [debug] [MainThread]: On master: COMMIT
[0m06:28:38.267875 [debug] [MainThread]: Using postgres connection "master"
[0m06:28:38.268017 [debug] [MainThread]: On master: COMMIT
[0m06:28:38.268498 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m06:28:38.268805 [debug] [MainThread]: On master: Close
[0m06:28:38.269084 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:28:38.269232 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m06:28:38.269376 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m06:28:38.269497 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m06:28:38.269625 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m06:28:38.269748 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m06:28:38.269879 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m06:28:38.270046 [info ] [MainThread]: 
[0m06:28:38.270361 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 3 minutes and 14.74 seconds (194.74s).
[0m06:28:38.270818 [debug] [MainThread]: Command end result
[0m06:28:38.286018 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m06:28:38.287700 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m06:28:38.292828 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m06:28:38.293030 [info ] [MainThread]: 
[0m06:28:38.293356 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:28:38.293552 [info ] [MainThread]: 
[0m06:28:38.293815 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m06:28:38.296870 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 195.21635, "process_in_blocks": "0", "process_kernel_time": 0.193186, "process_mem_max_rss": "141819904", "process_out_blocks": "0", "process_user_time": 1.114599}
[0m06:28:38.297238 [debug] [MainThread]: Command `dbt run` succeeded at 06:28:38.297193 after 195.22 seconds
[0m06:28:38.297442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111225990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1011ff490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1012d6bd0>]}
[0m06:28:38.297617 [debug] [MainThread]: Flushing usage events
[0m06:28:38.591611 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:32:12.635233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066b3290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106733b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106733f90>]}


============================== 07:32:12.637865 | 8cef4e4b-1c1a-48ff-a4c1-c1605539afdc ==============================
[0m07:32:12.637865 [info ] [MainThread]: Running with dbt=1.10.13
[0m07:32:12.638170 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'partial_parse': 'True', 'quiet': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'use_colors': 'True', 'write_json': 'True', 'printer_width': '80', 'target_path': 'None', 'fail_fast': 'False', 'static_parser': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'introspect': 'True', 'no_print': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'log_format': 'default'}
[0m07:32:12.841149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106724f90>]}
[0m07:32:12.868733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10649b3d0>]}
[0m07:32:12.869319 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m07:32:12.914728 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m07:32:12.980429 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:32:12.980617 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:32:13.002100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10763e010>]}
[0m07:32:13.040762 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:32:13.041745 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:32:13.053601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a1ff10>]}
[0m07:32:13.053827 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m07:32:13.053992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10777fad0>]}
[0m07:32:13.054740 [info ] [MainThread]: 
[0m07:32:13.054901 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m07:32:13.055016 [info ] [MainThread]: 
[0m07:32:13.055228 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m07:32:13.055603 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m07:32:13.102850 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m07:32:13.103060 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m07:32:13.103188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.129174 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.026 seconds
[0m07:32:13.129706 [debug] [ThreadPool]: On list_finny_db: Close
[0m07:32:13.131818 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m07:32:13.134207 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:32:13.134419 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m07:32:13.134537 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m07:32:13.134722 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m07:32:13.134986 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m07:32:13.135198 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m07:32:13.135953 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:32:13.136144 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m07:32:13.136294 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m07:32:13.137064 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:32:13.137750 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:32:13.138422 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:32:13.138544 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m07:32:13.139466 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:32:13.139656 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m07:32:13.139785 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m07:32:13.139900 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m07:32:13.140017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.140155 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m07:32:13.140299 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.140440 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.140575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.140795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:32:13.152294 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m07:32:13.152447 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:32:13.152591 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m07:32:13.152752 [debug] [ThreadPool]: SQL status: BEGIN in 0.013 seconds
[0m07:32:13.152877 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:32:13.153009 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m07:32:13.157388 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m07:32:13.157802 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:32:13.158027 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.005 seconds
[0m07:32:13.158261 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m07:32:13.158436 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m07:32:13.158570 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m07:32:13.158691 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m07:32:13.158840 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m07:32:13.159571 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m07:32:13.159728 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:32:13.160173 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m07:32:13.160302 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:32:13.160431 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:32:13.160652 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m07:32:13.160877 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m07:32:13.161082 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m07:32:13.161253 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m07:32:13.161386 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m07:32:13.163194 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m07:32:13.163836 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m07:32:13.164299 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m07:32:13.164430 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m07:32:13.164611 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.003 seconds
[0m07:32:13.164789 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.003 seconds
[0m07:32:13.165444 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m07:32:13.165992 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m07:32:13.166867 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m07:32:13.167430 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m07:32:13.167566 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m07:32:13.167696 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m07:32:13.171230 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:13.171396 [debug] [MainThread]: On master: BEGIN
[0m07:32:13.171510 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:32:13.178405 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m07:32:13.178591 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:13.178760 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m07:32:13.182213 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m07:32:13.183264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ce3950>]}
[0m07:32:13.183476 [debug] [MainThread]: On master: ROLLBACK
[0m07:32:13.183946 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:13.184175 [debug] [MainThread]: On master: BEGIN
[0m07:32:13.184768 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m07:32:13.184956 [debug] [MainThread]: On master: COMMIT
[0m07:32:13.185091 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:13.185207 [debug] [MainThread]: On master: COMMIT
[0m07:32:13.185571 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:32:13.185818 [debug] [MainThread]: On master: Close
[0m07:32:13.188220 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m07:32:13.188551 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m07:32:13.188802 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_prospect_matches)
[0m07:32:13.188967 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m07:32:13.194497 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m07:32:13.195177 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m07:32:13.214832 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m07:32:13.215308 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:32:13.216129 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m07:32:13.216657 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:32:13.223207 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m07:32:13.223673 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:32:13.223893 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m07:32:44.812273 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m07:32:44.813311 [debug] [MainThread]: Postgres adapter: Cancelling query 'model.dbt_service.stg_prospect_matches' (2460)
[0m07:32:44.813598 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:44.813786 [debug] [MainThread]: On master: BEGIN
[0m07:32:44.814030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m07:32:44.821299 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m07:32:44.821536 [debug] [MainThread]: Using postgres connection "master"
[0m07:32:44.821671 [debug] [MainThread]: On master: select pg_terminate_backend(2460)
[0m07:32:44.822309 [debug] [MainThread]: SQL status: SELECT 1 in 0.001 seconds
[0m07:32:44.822524 [debug] [MainThread]: Postgres adapter: Cancel query 'model.dbt_service.stg_prospect_matches': (True,)
[0m07:32:44.822726 [error] [MainThread]: CANCEL query model.dbt_service.stg_prospect_matches ............................ [[31mCANCEL[0m]
[0m07:32:44.822892 [error] [MainThread]: CANCEL query list_finny_db_public_raw_analysis ................................. [[31mCANCEL[0m]
[0m07:32:44.823016 [error] [MainThread]: CANCEL query list_finny_db_public .............................................. [[31mCANCEL[0m]
[0m07:32:44.823141 [error] [MainThread]: CANCEL query list_finny_db_public_marts ........................................ [[31mCANCEL[0m]
[0m07:32:44.823294 [error] [MainThread]: CANCEL query list_finny_db_public_raw .......................................... [[31mCANCEL[0m]
[0m07:32:44.823408 [error] [MainThread]: CANCEL query list_finny_db_public_staging ...................................... [[31mCANCEL[0m]
[0m07:32:44.823529 [debug] [MainThread]: On master: ROLLBACK
[0m07:32:44.823953 [debug] [MainThread]: On master: Close
[0m07:32:44.825441 [debug] [Thread-1 (]: Postgres adapter: Postgres error: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[0m07:32:44.825605 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: ROLLBACK
[0m07:32:44.827352 [debug] [Thread-1 (]: Failed to rollback 'model.dbt_service.stg_prospect_matches'
[0m07:32:44.827758 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m07:32:44.833421 [debug] [Thread-1 (]: Database Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  server closed the connection unexpectedly
  	This probably means the server terminated abnormally
  	before or while processing the request.
  compiled code at target/run/dbt_service/models/staging/stg_prospect_matches.sql
[0m07:32:44.834477 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cef4e4b-1c1a-48ff-a4c1-c1605539afdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de6c90>]}
[0m07:32:44.834980 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model public_staging.stg_prospect_matches  [[31mERROR[0m in 31.64s]
[0m07:32:44.835316 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m07:32:44.835506 [info ] [MainThread]: 
[0m07:32:44.835744 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m07:32:44.835923 [info ] [MainThread]: 
[0m07:32:44.836100 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=0
[0m07:32:44.836231 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:32:44.836342 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m07:32:44.836446 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m07:32:44.836549 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m07:32:44.836651 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m07:32:44.836751 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m07:32:44.836852 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m07:32:44.836970 [info ] [MainThread]: 
[0m07:32:44.837096 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 31.78 seconds (31.78s).
[0m07:32:44.837318 [error] [MainThread]: Encountered an error:

[0m07:32:44.839996 [error] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 373, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 390, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/main.py", line 587, in run
    results = task.run()
              ^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 599, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 536, in execute_with_hooks
    res = self.execute_nodes()
          ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 416, in execute_nodes
    self.run_queue(pool)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 343, in run_queue
    self.job_queue.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/graph/queue.py", line 206, in join
    self.inner.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", line 327, in wait
    waiter.acquire()
KeyboardInterrupt

[0m07:32:44.842813 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 32.245255, "process_in_blocks": "0", "process_kernel_time": 0.231338, "process_mem_max_rss": "136429568", "process_out_blocks": "0", "process_user_time": 0.964951}
[0m07:32:44.843180 [debug] [MainThread]: Command `dbt run` failed at 07:32:44.843117 after 32.25 seconds
[0m07:32:44.843369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010b00d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de4e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106733c10>]}
[0m07:32:44.843551 [debug] [MainThread]: Flushing usage events
[0m07:32:45.145540 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:34:10.204838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11322afd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132a2210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132afc90>]}


============================== 07:34:10.207437 | 68dada52-1e3b-4945-b5f3-2f1e2acb73b8 ==============================
[0m07:34:10.207437 [info ] [MainThread]: Running with dbt=1.10.13
[0m07:34:10.207775 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'static_parser': 'True', 'use_colors': 'True', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'no_print': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'indirect_selection': 'eager', 'printer_width': '80', 'empty': 'False', 'version_check': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'target_path': 'None', 'fail_fast': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches', 'use_experimental_parser': 'False'}
[0m07:34:10.305463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a134d0>]}
[0m07:34:10.335601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100f4b50>]}
[0m07:34:10.336562 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m07:34:10.383309 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m07:34:10.447997 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:34:10.448205 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:34:10.470988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114048c90>]}
[0m07:34:10.512307 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:34:10.513664 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:34:10.525135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11377afd0>]}
[0m07:34:10.525376 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m07:34:10.525540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137f8210>]}
[0m07:34:10.526303 [info ] [MainThread]: 
[0m07:34:10.526461 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m07:34:10.526578 [info ] [MainThread]: 
[0m07:34:10.526823 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m07:34:10.527199 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m07:34:10.575363 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m07:34:10.575622 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m07:34:10.575760 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.605559 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.030 seconds
[0m07:34:10.606198 [debug] [ThreadPool]: On list_finny_db: Close
[0m07:34:10.608327 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m07:34:10.608575 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m07:34:10.610896 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:34:10.611054 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m07:34:10.611330 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m07:34:10.612147 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:34:10.612398 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m07:34:10.612638 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m07:34:10.612762 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m07:34:10.613519 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:34:10.614224 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:34:10.614340 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m07:34:10.615007 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:34:10.615918 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:34:10.616035 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m07:34:10.616144 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m07:34:10.616253 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m07:34:10.616370 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.616495 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m07:34:10.616608 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m07:34:10.616799 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.616933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.617193 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.617342 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:10.628175 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m07:34:10.628302 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:34:10.628428 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m07:34:10.630807 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m07:34:10.630917 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:34:10.631038 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m07:34:10.632211 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m07:34:10.632757 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m07:34:10.634626 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m07:34:10.634750 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m07:34:10.635187 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m07:34:10.635557 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m07:34:10.635666 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m07:34:10.635852 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m07:34:10.636050 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m07:34:10.636189 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m07:34:10.636317 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:34:10.636442 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:34:10.636569 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:34:10.636697 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:34:10.637245 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m07:34:10.637441 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m07:34:10.637618 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m07:34:10.637772 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m07:34:10.639333 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.001 seconds
[0m07:34:10.639528 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m07:34:10.640124 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m07:34:10.640296 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m07:34:10.640405 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m07:34:10.640846 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m07:34:10.641657 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m07:34:10.641897 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m07:34:10.642452 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m07:34:10.643177 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m07:34:10.643336 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m07:34:10.643545 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m07:34:10.647471 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.647721 [debug] [MainThread]: On master: BEGIN
[0m07:34:10.647865 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:34:10.654401 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m07:34:10.654615 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.654786 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m07:34:10.657101 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m07:34:10.658448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11470f8d0>]}
[0m07:34:10.658705 [debug] [MainThread]: On master: ROLLBACK
[0m07:34:10.659264 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.659391 [debug] [MainThread]: On master: BEGIN
[0m07:34:10.661180 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m07:34:10.661466 [debug] [MainThread]: On master: COMMIT
[0m07:34:10.661871 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.662033 [debug] [MainThread]: On master: COMMIT
[0m07:34:10.662676 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:34:10.662843 [debug] [MainThread]: On master: Close
[0m07:34:10.665520 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m07:34:10.665806 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m07:34:10.666021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m07:34:10.666158 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m07:34:10.672191 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m07:34:10.672582 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m07:34:10.694774 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.695044 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp073410684848"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m07:34:10.695221 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:34:10.741757 [debug] [Thread-1 (]: SQL status: SELECT 0 in 0.046 seconds
[0m07:34:10.748282 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.748511 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m07:34:10.749055 [debug] [Thread-1 (]: SQL status: BEGIN in 0.000 seconds
[0m07:34:10.749272 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.749448 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp073410684848'
        
      order by ordinal_position

  
[0m07:34:10.754764 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.005 seconds
[0m07:34:10.757181 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.757388 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m07:34:10.759108 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.002 seconds
[0m07:34:10.765576 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.765766 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp073410684848'
        
      order by ordinal_position

  
[0m07:34:10.767699 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.002 seconds
[0m07:34:10.769637 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.769854 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m07:34:10.771496 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.001 seconds
[0m07:34:10.775795 [debug] [Thread-1 (]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m07:34:10.781881 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m07:34:10.782289 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.782453 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp073410684848" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp073410684848"
    )
  
[0m07:34:10.783342 [debug] [Thread-1 (]: SQL status: INSERT 0 0 in 0.001 seconds
[0m07:34:10.789427 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m07:34:10.789643 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:10.789795 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m07:34:10.790540 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m07:34:10.791320 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m07:34:10.792352 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '68dada52-1e3b-4945-b5f3-2f1e2acb73b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fa2d0>]}
[0m07:34:10.792706 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mINSERT 0 0[0m in 0.13s]
[0m07:34:10.792970 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m07:34:10.794019 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.794154 [debug] [MainThread]: On master: BEGIN
[0m07:34:10.794269 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m07:34:10.799712 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m07:34:10.799884 [debug] [MainThread]: On master: COMMIT
[0m07:34:10.800027 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:10.800148 [debug] [MainThread]: On master: COMMIT
[0m07:34:10.800482 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:34:10.800698 [debug] [MainThread]: On master: Close
[0m07:34:10.800911 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:34:10.801041 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m07:34:10.801155 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m07:34:10.801262 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m07:34:10.801373 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m07:34:10.801473 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m07:34:10.801591 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m07:34:10.801731 [info ] [MainThread]: 
[0m07:34:10.801877 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m07:34:10.802194 [debug] [MainThread]: Command end result
[0m07:34:10.814055 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:34:10.814777 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:34:10.817196 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m07:34:10.817352 [info ] [MainThread]: 
[0m07:34:10.817526 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:34:10.817651 [info ] [MainThread]: 
[0m07:34:10.817783 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m07:34:10.820044 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.64976156, "process_in_blocks": "0", "process_kernel_time": 0.20109, "process_mem_max_rss": "140967936", "process_out_blocks": "0", "process_user_time": 1.013468}
[0m07:34:10.820269 [debug] [MainThread]: Command `dbt run` succeeded at 07:34:10.820235 after 0.65 seconds
[0m07:34:10.820429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132d7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d90390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dcb910>]}
[0m07:34:10.820598 [debug] [MainThread]: Flushing usage events
[0m07:34:11.224025 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:34:23.178728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062b20d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106333450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106333bd0>]}


============================== 07:34:23.180533 | 06344138-beed-42d2-a137-7a01156f9f1e ==============================
[0m07:34:23.180533 [info ] [MainThread]: Running with dbt=1.10.13
[0m07:34:23.180838 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select stg_prospect_matches', 'use_experimental_parser': 'False', 'introspect': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_format': 'default', 'printer_width': '80', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'fail_fast': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'empty': 'False', 'version_check': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'quiet': 'False'}
[0m07:34:23.261342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106322c10>]}
[0m07:34:23.290352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b1d950>]}
[0m07:34:23.290733 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m07:34:23.331205 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m07:34:23.385110 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:34:23.385322 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:34:23.408028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10733d750>]}
[0m07:34:23.446955 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:34:23.447786 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:34:23.453932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107527e90>]}
[0m07:34:23.454164 [info ] [MainThread]: Found 16 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m07:34:23.454327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10747bd10>]}
[0m07:34:23.455067 [info ] [MainThread]: 
[0m07:34:23.455226 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m07:34:23.455345 [info ] [MainThread]: 
[0m07:34:23.455549 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m07:34:23.455915 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m07:34:23.495547 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m07:34:23.495811 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m07:34:23.495944 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.516429 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.020 seconds
[0m07:34:23.517004 [debug] [ThreadPool]: On list_finny_db: Close
[0m07:34:23.519159 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m07:34:23.519430 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m07:34:23.519672 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m07:34:23.521974 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:34:23.522267 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m07:34:23.522496 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m07:34:23.522740 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m07:34:23.523588 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:34:23.524415 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:34:23.524551 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m07:34:23.525249 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:34:23.525943 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:34:23.526927 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:34:23.527052 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m07:34:23.527171 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m07:34:23.527296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m07:34:23.527422 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m07:34:23.527534 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m07:34:23.527650 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m07:34:23.527761 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.527909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.528111 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.528234 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.528352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:34:23.546752 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m07:34:23.546959 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:34:23.547093 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m07:34:23.548944 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m07:34:23.549102 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m07:34:23.549204 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m07:34:23.549312 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:34:23.549426 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:34:23.549549 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m07:34:23.549646 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m07:34:23.549757 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:34:23.549887 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m07:34:23.550026 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m07:34:23.550145 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:34:23.550250 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:34:23.550368 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m07:34:23.550554 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m07:34:23.550699 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m07:34:23.550849 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m07:34:23.551456 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m07:34:23.553648 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m07:34:23.553798 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m07:34:23.554246 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m07:34:23.554374 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m07:34:23.554560 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m07:34:23.554756 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m07:34:23.555388 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m07:34:23.555581 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.005 seconds
[0m07:34:23.556096 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m07:34:23.556243 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m07:34:23.556756 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m07:34:23.557636 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m07:34:23.557779 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m07:34:23.558132 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m07:34:23.558526 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m07:34:23.558643 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m07:34:23.562854 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.563010 [debug] [MainThread]: On master: BEGIN
[0m07:34:23.563119 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:34:23.570825 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m07:34:23.571049 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.571527 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m07:34:23.574046 [debug] [MainThread]: SQL status: SELECT 8 in 0.002 seconds
[0m07:34:23.575249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077b78d0>]}
[0m07:34:23.575530 [debug] [MainThread]: On master: ROLLBACK
[0m07:34:23.576119 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.576279 [debug] [MainThread]: On master: BEGIN
[0m07:34:23.577198 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m07:34:23.577332 [debug] [MainThread]: On master: COMMIT
[0m07:34:23.577469 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.577577 [debug] [MainThread]: On master: COMMIT
[0m07:34:23.578004 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:34:23.578344 [debug] [MainThread]: On master: Close
[0m07:34:23.581369 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m07:34:23.581674 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m07:34:23.581873 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.stg_prospect_matches)
[0m07:34:23.582019 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m07:34:23.587103 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m07:34:23.587457 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m07:34:23.608112 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.608362 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp073423598726"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m07:34:23.608528 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:34:23.658557 [debug] [Thread-1 (]: SQL status: SELECT 0 in 0.050 seconds
[0m07:34:23.665413 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.665634 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m07:34:23.666150 [debug] [Thread-1 (]: SQL status: BEGIN in 0.000 seconds
[0m07:34:23.666351 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.666527 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp073423598726'
        
      order by ordinal_position

  
[0m07:34:23.669462 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.003 seconds
[0m07:34:23.671852 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.672059 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m07:34:23.673655 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.001 seconds
[0m07:34:23.679993 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.680219 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp073423598726'
        
      order by ordinal_position

  
[0m07:34:23.681700 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.001 seconds
[0m07:34:23.683414 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.683612 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m07:34:23.685258 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.001 seconds
[0m07:34:23.689747 [debug] [Thread-1 (]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m07:34:23.695699 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m07:34:23.696187 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.696386 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp073423598726" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp073423598726"
    )
  
[0m07:34:23.697262 [debug] [Thread-1 (]: SQL status: INSERT 0 0 in 0.001 seconds
[0m07:34:23.702673 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m07:34:23.702850 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m07:34:23.702994 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m07:34:23.703667 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m07:34:23.704477 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m07:34:23.705434 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06344138-beed-42d2-a137-7a01156f9f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077f1350>]}
[0m07:34:23.705746 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mINSERT 0 0[0m in 0.12s]
[0m07:34:23.706006 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m07:34:23.706991 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.707152 [debug] [MainThread]: On master: BEGIN
[0m07:34:23.707272 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m07:34:23.712542 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m07:34:23.712726 [debug] [MainThread]: On master: COMMIT
[0m07:34:23.712872 [debug] [MainThread]: Using postgres connection "master"
[0m07:34:23.713023 [debug] [MainThread]: On master: COMMIT
[0m07:34:23.713375 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:34:23.713497 [debug] [MainThread]: On master: Close
[0m07:34:23.713673 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:34:23.713788 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m07:34:23.713896 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m07:34:23.714004 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m07:34:23.714110 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m07:34:23.714214 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m07:34:23.714316 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m07:34:23.714459 [info ] [MainThread]: 
[0m07:34:23.714598 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m07:34:23.714859 [debug] [MainThread]: Command end result
[0m07:34:23.726633 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:34:23.727365 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:34:23.730978 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m07:34:23.731170 [info ] [MainThread]: 
[0m07:34:23.731357 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:34:23.731490 [info ] [MainThread]: 
[0m07:34:23.731730 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m07:34:23.734757 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.5883553, "process_in_blocks": "0", "process_kernel_time": 0.153921, "process_mem_max_rss": "138592256", "process_out_blocks": "0", "process_user_time": 1.007439}
[0m07:34:23.735173 [debug] [MainThread]: Command `dbt run` succeeded at 07:34:23.735069 after 0.59 seconds
[0m07:34:23.735414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a50390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10635fed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100bbec10>]}
[0m07:34:23.735608 [debug] [MainThread]: Flushing usage events
[0m07:34:23.952456 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:48:48.561184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10992ae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099a7c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099a7f90>]}


============================== 07:48:48.563779 | 659d7257-7157-4347-89a8-88478faa3591 ==============================
[0m07:48:48.563779 [info ] [MainThread]: Running with dbt=1.10.13
[0m07:48:48.564055 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_format': 'default', 'debug': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select stg_entity_clusters', 'indirect_selection': 'eager', 'partial_parse': 'True', 'introspect': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'fail_fast': 'False', 'warn_error': 'None', 'printer_width': '80', 'use_colors': 'True', 'version_check': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m07:48:48.691691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099d4910>]}
[0m07:48:48.721885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068f9650>]}
[0m07:48:48.722578 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m07:48:48.771388 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m07:48:48.841271 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m07:48:48.841595 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/staging/stg_entity_clusters.sql
[0m07:48:48.966668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b52eb10>]}
[0m07:48:49.029526 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:48:49.030612 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:48:49.042590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a01e950>]}
[0m07:48:49.042826 [info ] [MainThread]: Found 17 models, 3 seeds, 13 data tests, 2 sources, 449 macros
[0m07:48:49.042985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a469f50>]}
[0m07:48:49.043665 [info ] [MainThread]: 
[0m07:48:49.043812 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m07:48:49.043927 [info ] [MainThread]: 
[0m07:48:49.044132 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m07:48:49.044523 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m07:48:49.071478 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m07:48:49.071741 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m07:48:49.071889 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.100389 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.028 seconds
[0m07:48:49.101063 [debug] [ThreadPool]: On list_finny_db: Close
[0m07:48:49.103245 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m07:48:49.103463 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m07:48:49.103690 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m07:48:49.103925 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m07:48:49.106459 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:48:49.106677 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m07:48:49.106883 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m07:48:49.107904 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:48:49.108703 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:48:49.109428 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:48:49.109563 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m07:48:49.110682 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:48:49.111485 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:48:49.111648 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m07:48:49.111769 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m07:48:49.111912 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m07:48:49.112035 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m07:48:49.112150 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m07:48:49.112258 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m07:48:49.112355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.112457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.112555 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.112745 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.112890 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:48:49.135559 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m07:48:49.135807 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m07:48:49.135943 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m07:48:49.136057 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m07:48:49.136176 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m07:48:49.136289 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m07:48:49.136390 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m07:48:49.136531 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m07:48:49.136660 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m07:48:49.136787 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m07:48:49.136911 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m07:48:49.137068 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m07:48:49.137205 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m07:48:49.137370 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m07:48:49.137515 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m07:48:49.137665 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m07:48:49.137883 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m07:48:49.138090 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m07:48:49.141235 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m07:48:49.141918 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m07:48:49.142107 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m07:48:49.142290 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m07:48:49.142494 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m07:48:49.142675 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m07:48:49.142846 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m07:48:49.143552 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m07:48:49.143696 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m07:48:49.144148 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m07:48:49.144587 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m07:48:49.145029 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m07:48:49.145423 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m07:48:49.146112 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m07:48:49.146291 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m07:48:49.146816 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m07:48:49.146941 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m07:48:49.147057 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m07:48:49.151072 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.151241 [debug] [MainThread]: On master: BEGIN
[0m07:48:49.151354 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:48:49.157668 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m07:48:49.157909 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.158100 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m07:48:49.162884 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m07:48:49.163966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2eb550>]}
[0m07:48:49.164164 [debug] [MainThread]: On master: ROLLBACK
[0m07:48:49.164724 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.165016 [debug] [MainThread]: On master: BEGIN
[0m07:48:49.169345 [debug] [MainThread]: SQL status: BEGIN in 0.004 seconds
[0m07:48:49.169546 [debug] [MainThread]: On master: COMMIT
[0m07:48:49.169680 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.169943 [debug] [MainThread]: On master: COMMIT
[0m07:48:49.170488 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:48:49.170729 [debug] [MainThread]: On master: Close
[0m07:48:49.173545 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_entity_clusters
[0m07:48:49.173860 [info ] [Thread-1 (]: 1 of 1 START sql table model public_staging.stg_entity_clusters ................ [RUN]
[0m07:48:49.174150 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_entity_clusters)
[0m07:48:49.174331 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_entity_clusters
[0m07:48:49.177618 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m07:48:49.178024 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_entity_clusters
[0m07:48:49.196028 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m07:48:49.196545 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m07:48:49.196714 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m07:48:49.196866 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:48:49.204284 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m07:48:49.204579 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m07:48:49.204765 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m07:48:49.208848 [debug] [Thread-1 (]: SQL status: SELECT 87 in 0.004 seconds
[0m07:48:49.213464 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m07:48:49.213696 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m07:48:49.214399 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m07:48:49.220039 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m07:48:49.220208 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m07:48:49.220336 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m07:48:49.221343 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m07:48:49.224420 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m07:48:49.226593 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m07:48:49.226746 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m07:48:49.227391 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m07:48:49.228940 [debug] [Thread-1 (]: On model.dbt_service.stg_entity_clusters: Close
[0m07:48:49.230034 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '659d7257-7157-4347-89a8-88478faa3591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2b9950>]}
[0m07:48:49.230392 [info ] [Thread-1 (]: 1 of 1 OK created sql table model public_staging.stg_entity_clusters ........... [[32mSELECT 87[0m in 0.06s]
[0m07:48:49.230650 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_entity_clusters
[0m07:48:49.231694 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.231888 [debug] [MainThread]: On master: BEGIN
[0m07:48:49.232005 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m07:48:49.237443 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m07:48:49.237665 [debug] [MainThread]: On master: COMMIT
[0m07:48:49.237820 [debug] [MainThread]: Using postgres connection "master"
[0m07:48:49.237960 [debug] [MainThread]: On master: COMMIT
[0m07:48:49.238252 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m07:48:49.238406 [debug] [MainThread]: On master: Close
[0m07:48:49.238626 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:48:49.238784 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m07:48:49.238880 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m07:48:49.238972 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m07:48:49.239064 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m07:48:49.239154 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m07:48:49.239248 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m07:48:49.239365 [info ] [MainThread]: 
[0m07:48:49.239496 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m07:48:49.239739 [debug] [MainThread]: Command end result
[0m07:48:49.254923 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m07:48:49.255979 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m07:48:49.258509 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m07:48:49.258651 [info ] [MainThread]: 
[0m07:48:49.258830 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:48:49.258956 [info ] [MainThread]: 
[0m07:48:49.259095 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m07:48:49.262733 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7372076, "process_in_blocks": "0", "process_kernel_time": 0.215337, "process_mem_max_rss": "141459456", "process_out_blocks": "0", "process_user_time": 1.098463}
[0m07:48:49.264034 [debug] [MainThread]: Command `dbt run` succeeded at 07:48:49.263998 after 0.74 seconds
[0m07:48:49.264259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b20390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102beec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b5b910>]}
[0m07:48:49.264433 [debug] [MainThread]: Flushing usage events
[0m07:48:49.540895 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:06:17.923424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e2b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118eb3090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118eb3850>]}


============================== 08:06:17.925924 | 81db0284-78ef-4c5a-9a78-e25045f27ce2 ==============================
[0m08:06:17.925924 [info ] [MainThread]: Running with dbt=1.10.13
[0m08:06:17.926232 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run-operation merge_entities', 'partial_parse': 'True', 'fail_fast': 'False', 'empty': 'None', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'printer_width': '80', 'warn_error': 'None', 'log_format': 'default', 'quiet': 'False', 'target_path': 'None', 'introspect': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m08:06:18.054382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '81db0284-78ef-4c5a-9a78-e25045f27ce2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119524fd0>]}
[0m08:06:18.083375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '81db0284-78ef-4c5a-9a78-e25045f27ce2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119695cd0>]}
[0m08:06:18.084042 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m08:06:18.131021 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m08:06:18.198550 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m08:06:18.198880 [debug] [MainThread]: Partial parsing: added file: dbt_service://macros/merge_entities.sql
[0m08:06:18.199057 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_entity_clusters.sql
[0m08:06:18.323213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '81db0284-78ef-4c5a-9a78-e25045f27ce2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f08990>]}
[0m08:06:18.386947 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m08:06:18.388064 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m08:06:18.398194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '81db0284-78ef-4c5a-9a78-e25045f27ce2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ae36550>]}
[0m08:06:18.398432 [info ] [MainThread]: Found 17 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m08:06:18.398592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81db0284-78ef-4c5a-9a78-e25045f27ce2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa668d0>]}
[0m08:06:18.398837 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m08:06:18.398985 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:18.399101 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m08:06:18.399219 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:06:18.431750 [debug] [MainThread]: SQL status: BEGIN in 0.032 seconds
[0m08:06:18.431963 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:18.432090 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:18.432228 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:18.432776 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m08:06:18.458670 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:18.458935 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
        SELECT merge_similar_entities();
    
  
[0m08:06:19.619815 [debug] [MainThread]: SQL status: SELECT 1 in 1.161 seconds
[0m08:06:19.622294 [info ] [MainThread]: Entity merge complete.
[0m08:06:19.622887 [debug] [MainThread]: On macro_merge_entities: Close
[0m08:06:19.626470 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m08:06:19.628793 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.7420748, "process_in_blocks": "0", "process_kernel_time": 0.202939, "process_mem_max_rss": "136478720", "process_out_blocks": "0", "process_user_time": 0.942989}
[0m08:06:19.629101 [debug] [MainThread]: Command `dbt run-operation` succeeded at 08:06:19.629023 after 1.74 seconds
[0m08:06:19.629444 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m08:06:19.629755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103090390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fe3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f4fd90>]}
[0m08:06:19.630096 [debug] [MainThread]: Flushing usage events
[0m08:06:19.972990 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:06:38.175131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108229310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082ab3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082abb50>]}


============================== 08:06:38.177305 | 5c15e5f2-9131-4490-a96c-b9007f94be5e ==============================
[0m08:06:38.177305 [info ] [MainThread]: Running with dbt=1.10.13
[0m08:06:38.177600 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'debug': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'empty': 'None', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'indirect_selection': 'eager', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'invocation_command': 'dbt run-operation merge_entities', 'introspect': 'True', 'warn_error': 'None', 'target_path': 'None', 'version_check': 'True', 'static_parser': 'True'}
[0m08:06:38.264372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c15e5f2-9131-4490-a96c-b9007f94be5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083a9dd0>]}
[0m08:06:38.293354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c15e5f2-9131-4490-a96c-b9007f94be5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088486d0>]}
[0m08:06:38.294134 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m08:06:38.337034 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m08:06:38.402617 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:06:38.402843 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:06:38.425536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c15e5f2-9131-4490-a96c-b9007f94be5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10914c750>]}
[0m08:06:38.465308 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m08:06:38.466434 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m08:06:38.475137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c15e5f2-9131-4490-a96c-b9007f94be5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10878c7d0>]}
[0m08:06:38.475374 [info ] [MainThread]: Found 17 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m08:06:38.475531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c15e5f2-9131-4490-a96c-b9007f94be5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108848890>]}
[0m08:06:38.475791 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m08:06:38.475943 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:38.476058 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m08:06:38.476179 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:06:38.496043 [debug] [MainThread]: SQL status: BEGIN in 0.020 seconds
[0m08:06:38.496238 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:38.496355 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:38.496461 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:38.497300 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m08:06:38.549847 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:38.550136 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
        SELECT merge_similar_entities();
    
  
[0m08:06:39.702894 [debug] [MainThread]: SQL status: SELECT 1 in 1.152 seconds
[0m08:06:39.705802 [info ] [MainThread]: Entity merge complete.
[0m08:06:39.706456 [debug] [MainThread]: On macro_merge_entities: Close
[0m08:06:39.714445 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m08:06:39.717432 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.5769258, "process_in_blocks": "0", "process_kernel_time": 0.170456, "process_mem_max_rss": "133169152", "process_out_blocks": "0", "process_user_time": 0.843314}
[0m08:06:39.718055 [debug] [MainThread]: Command `dbt run-operation` succeeded at 08:06:39.717956 after 1.58 seconds
[0m08:06:39.718400 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m08:06:39.718746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102490110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109157650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1024c7790>]}
[0m08:06:39.719121 [debug] [MainThread]: Flushing usage events
[0m08:06:39.948196 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:06:50.138942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c2ae90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c6f9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ca7fd0>]}


============================== 08:06:50.141454 | 21767e3c-e5d8-4413-869b-2a9ba373fade ==============================
[0m08:06:50.141454 [info ] [MainThread]: Running with dbt=1.10.13
[0m08:06:50.141862 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'printer_width': '80', 'no_print': 'None', 'empty': 'False', 'log_format': 'default', 'warn_error': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'write_json': 'True', 'version_check': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'invocation_command': 'dbt run --select entity_clusters', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'cache_selected_only': 'False'}
[0m08:06:50.227890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21767e3c-e5d8-4413-869b-2a9ba373fade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ba8fd0>]}
[0m08:06:50.257124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21767e3c-e5d8-4413-869b-2a9ba373fade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c80850>]}
[0m08:06:50.257548 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m08:06:50.299471 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m08:06:50.355811 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:06:50.356014 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:06:50.380068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21767e3c-e5d8-4413-869b-2a9ba373fade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ab7550>]}
[0m08:06:50.419895 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m08:06:50.420729 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m08:06:50.429068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21767e3c-e5d8-4413-869b-2a9ba373fade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c281d0>]}
[0m08:06:50.429320 [info ] [MainThread]: Found 17 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m08:06:50.429491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21767e3c-e5d8-4413-869b-2a9ba373fade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c25710>]}
[0m08:06:50.429844 [warn ] [MainThread]: The selection criterion 'entity_clusters' does not match any enabled nodes
[0m08:06:50.430376 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m08:06:50.431119 [debug] [MainThread]: Command end result
[0m08:06:50.468830 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m08:06:50.469637 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m08:06:50.470908 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m08:06:50.472195 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.37137824, "process_in_blocks": "0", "process_kernel_time": 0.157355, "process_mem_max_rss": "126730240", "process_out_blocks": "0", "process_user_time": 0.838959}
[0m08:06:50.472407 [debug] [MainThread]: Command `dbt run` succeeded at 08:06:50.472373 after 0.37 seconds
[0m08:06:50.472559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102370350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023ab8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023bca50>]}
[0m08:06:50.472708 [debug] [MainThread]: Flushing usage events
[0m08:06:50.688239 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:06:54.693023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c2a810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c992d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c9b1d0>]}


============================== 08:06:54.694831 | d6437aa7-4ca5-4290-b53c-7865125c3030 ==============================
[0m08:06:54.694831 [info ] [MainThread]: Running with dbt=1.10.13
[0m08:06:54.695145 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'version_check': 'True', 'warn_error': 'None', 'quiet': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run-operation merge_entities', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'log_format': 'default', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'indirect_selection': 'eager', 'printer_width': '80', 'debug': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'empty': 'None', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'no_print': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m08:06:54.779050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd6437aa7-4ca5-4290-b53c-7865125c3030', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c33750>]}
[0m08:06:54.808274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd6437aa7-4ca5-4290-b53c-7865125c3030', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ff9610>]}
[0m08:06:54.808697 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m08:06:54.849546 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m08:06:54.905034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:06:54.905237 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:06:54.928189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd6437aa7-4ca5-4290-b53c-7865125c3030', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089e2650>]}
[0m08:06:54.967728 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m08:06:54.968605 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m08:06:54.975086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd6437aa7-4ca5-4290-b53c-7865125c3030', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082f51d0>]}
[0m08:06:54.975305 [info ] [MainThread]: Found 17 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m08:06:54.975461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6437aa7-4ca5-4290-b53c-7865125c3030', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a9bb10>]}
[0m08:06:54.975719 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m08:06:54.975871 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:54.975987 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m08:06:54.976105 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:06:54.996894 [debug] [MainThread]: SQL status: BEGIN in 0.021 seconds
[0m08:06:54.997158 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:54.997327 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:54.997471 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m08:06:54.997954 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m08:06:55.054993 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m08:06:55.055308 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
        SELECT merge_similar_entities();
    
  
[0m08:06:56.217351 [debug] [MainThread]: SQL status: SELECT 1 in 1.162 seconds
[0m08:06:56.220035 [info ] [MainThread]: Entity merge complete.
[0m08:06:56.220672 [debug] [MainThread]: On macro_merge_entities: Close
[0m08:06:56.228354 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m08:06:56.230911 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.5716469, "process_in_blocks": "0", "process_kernel_time": 0.167982, "process_mem_max_rss": "132005888", "process_out_blocks": "0", "process_user_time": 0.849755}
[0m08:06:56.231396 [debug] [MainThread]: Command `dbt run-operation` succeeded at 08:06:56.231297 after 1.57 seconds
[0m08:06:56.231731 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m08:06:56.232076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100c640d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cd7cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cd7f50>]}
[0m08:06:56.232465 [debug] [MainThread]: Flushing usage events
[0m08:06:56.463140 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:26:29.786299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b36690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107382790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bb7fd0>]}


============================== 09:26:29.789156 | 98bc45be-4c7c-4b62-8825-208be702f38d ==============================
[0m09:26:29.789156 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:26:29.789455 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'introspect': 'True', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select prospect_matching_ratio', 'use_experimental_parser': 'False', 'target_path': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'quiet': 'False', 'empty': 'False', 'printer_width': '80', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'debug': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default'}
[0m09:26:29.914190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108284c10>]}
[0m09:26:29.942848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051cc950>]}
[0m09:26:29.943478 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:26:29.988502 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:26:30.056108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m09:26:30.056421 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/marts/prospect_matching_ratio.sql
[0m09:26:30.178766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d91b50>]}
[0m09:26:30.242050 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:26:30.243223 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:26:30.255483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082186d0>]}
[0m09:26:30.255726 [info ] [MainThread]: Found 18 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m09:26:30.255886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108af61d0>]}
[0m09:26:30.256615 [info ] [MainThread]: 
[0m09:26:30.256772 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m09:26:30.256890 [info ] [MainThread]: 
[0m09:26:30.257094 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:26:30.257467 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m09:26:30.283419 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m09:26:30.283655 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m09:26:30.283790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.378674 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.095 seconds
[0m09:26:30.379533 [debug] [ThreadPool]: On list_finny_db: Close
[0m09:26:30.382392 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m09:26:30.382725 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m09:26:30.385661 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m09:26:30.385956 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m09:26:30.387170 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m09:26:30.387505 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m09:26:30.387641 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m09:26:30.387859 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m09:26:30.388751 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m09:26:30.388938 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m09:26:30.389122 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m09:26:30.390048 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m09:26:30.390226 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:26:30.391040 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m09:26:30.391205 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m09:26:30.391996 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m09:26:30.392140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.392276 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m09:26:30.392503 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m09:26:30.392645 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.392780 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m09:26:30.392985 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.393122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.393339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:30.406962 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m09:26:30.407342 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m09:26:30.407529 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m09:26:30.408980 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m09:26:30.409292 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m09:26:30.409637 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m09:26:30.409860 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m09:26:30.410032 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m09:26:30.410207 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m09:26:30.412405 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m09:26:30.412642 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m09:26:30.412822 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m09:26:30.412960 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.005 seconds
[0m09:26:30.413138 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m09:26:30.413280 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m09:26:30.413514 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m09:26:30.414341 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m09:26:30.414584 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m09:26:30.414807 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m09:26:30.414983 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m09:26:30.415205 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m09:26:30.415388 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m09:26:30.416155 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m09:26:30.416386 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m09:26:30.417110 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m09:26:30.418228 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m09:26:30.418429 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m09:26:30.421157 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m09:26:30.421353 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m09:26:30.421549 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.004 seconds
[0m09:26:30.422195 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m09:26:30.422673 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m09:26:30.423201 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m09:26:30.423676 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m09:26:30.423811 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m09:26:30.423956 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m09:26:30.427951 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.428141 [debug] [MainThread]: On master: BEGIN
[0m09:26:30.428271 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:26:30.435809 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m09:26:30.436040 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.436226 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:26:30.439732 [debug] [MainThread]: SQL status: SELECT 8 in 0.003 seconds
[0m09:26:30.440991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e89f50>]}
[0m09:26:30.441249 [debug] [MainThread]: On master: ROLLBACK
[0m09:26:30.441661 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.441841 [debug] [MainThread]: On master: BEGIN
[0m09:26:30.442532 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m09:26:30.442738 [debug] [MainThread]: On master: COMMIT
[0m09:26:30.442918 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.443038 [debug] [MainThread]: On master: COMMIT
[0m09:26:30.443398 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:26:30.443602 [debug] [MainThread]: On master: Close
[0m09:26:30.446276 [debug] [Thread-1 (]: Began running node model.dbt_service.prospect_matching_ratio
[0m09:26:30.446595 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.prospect_matching_ratio ............... [RUN]
[0m09:26:30.446870 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.prospect_matching_ratio)
[0m09:26:30.447039 [debug] [Thread-1 (]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m09:26:30.450860 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.452154 [debug] [Thread-1 (]: Began executing node model.dbt_service.prospect_matching_ratio
[0m09:26:30.468491 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.469296 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.469571 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m09:26:30.469768 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:26:30.475919 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m09:26:30.476109 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.476296 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m09:26:30.479596 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m09:26:30.482570 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.482812 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m09:26:30.483786 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:26:30.489102 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m09:26:30.489297 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.489433 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m09:26:30.490409 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m09:26:30.493176 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m09:26:30.494967 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m09:26:30.495144 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m09:26:30.495727 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m09:26:30.496858 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: Close
[0m09:26:30.497809 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98bc45be-4c7c-4b62-8825-208be702f38d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091b41d0>]}
[0m09:26:30.498193 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.prospect_matching_ratio .......... [[32mCREATE VIEW[0m in 0.05s]
[0m09:26:30.498428 [debug] [Thread-1 (]: Finished running node model.dbt_service.prospect_matching_ratio
[0m09:26:30.499473 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.499740 [debug] [MainThread]: On master: BEGIN
[0m09:26:30.499885 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:26:30.505836 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m09:26:30.506193 [debug] [MainThread]: On master: COMMIT
[0m09:26:30.506339 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:30.506469 [debug] [MainThread]: On master: COMMIT
[0m09:26:30.506854 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:26:30.507003 [debug] [MainThread]: On master: Close
[0m09:26:30.507188 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:26:30.507314 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m09:26:30.507430 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m09:26:30.507551 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m09:26:30.507661 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m09:26:30.507780 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m09:26:30.507876 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m09:26:30.508015 [info ] [MainThread]: 
[0m09:26:30.508172 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m09:26:30.508491 [debug] [MainThread]: Command end result
[0m09:26:30.521786 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:26:30.522483 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:26:30.524782 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m09:26:30.524903 [info ] [MainThread]: 
[0m09:26:30.525057 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:26:30.525172 [info ] [MainThread]: 
[0m09:26:30.525292 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m09:26:30.527695 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.78069377, "process_in_blocks": "0", "process_kernel_time": 0.237148, "process_mem_max_rss": "143294464", "process_out_blocks": "0", "process_user_time": 1.082059}
[0m09:26:30.528171 [debug] [MainThread]: Command `dbt run` succeeded at 09:26:30.528108 after 0.78 seconds
[0m09:26:30.528424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107babe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10311c390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10326ac10>]}
[0m09:26:30.528634 [debug] [MainThread]: Flushing usage events
[0m09:26:30.818249 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:31:28.291555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10792ad50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079ab950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079a1fd0>]}


============================== 09:31:28.294105 | 800486cf-8abd-48ce-bb93-002cd908c9dc ==============================
[0m09:31:28.294105 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:31:28.294401 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'printer_width': '80', 'debug': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'no_print': 'None', 'empty': 'None', 'target_path': 'None', 'write_json': 'True', 'invocation_command': 'dbt list', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True'}
[0m09:31:28.389900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '800486cf-8abd-48ce-bb93-002cd908c9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b98f90>]}
[0m09:31:28.419786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '800486cf-8abd-48ce-bb93-002cd908c9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1035f5ad0>]}
[0m09:31:28.420618 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:31:28.468844 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:31:28.533969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:31:28.534223 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:31:28.557057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '800486cf-8abd-48ce-bb93-002cd908c9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a24a10>]}
[0m09:31:28.596759 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:31:28.597807 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:31:28.607734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800486cf-8abd-48ce-bb93-002cd908c9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088d4390>]}
[0m09:31:28.608266 [info ] [MainThread]: dbt_service.staging_analysis.company_analysis
[0m09:31:28.608431 [info ] [MainThread]: dbt_service.marts.data_overview
[0m09:31:28.608551 [info ] [MainThread]: dbt_service.marts.location_analysis
[0m09:31:28.608661 [info ] [MainThread]: dbt_service.marts.prospect_matching_ratio
[0m09:31:28.608771 [info ] [MainThread]: dbt_service.raw_analysis.raw_company_profiling
[0m09:31:28.608876 [info ] [MainThread]: dbt_service.raw_analysis.raw_data_profiling
[0m09:31:28.608981 [info ] [MainThread]: dbt_service.raw.raw_fxf_data
[0m09:31:28.609087 [info ] [MainThread]: dbt_service.raw_analysis.raw_location_profiling
[0m09:31:28.609191 [info ] [MainThread]: dbt_service.raw.raw_pdl_data
[0m09:31:28.609297 [info ] [MainThread]: dbt_service.staging_analysis.staging_city_state_profiling
[0m09:31:28.609400 [info ] [MainThread]: dbt_service.staging_analysis.staging_company_profiling
[0m09:31:28.609505 [info ] [MainThread]: dbt_service.staging_analysis.staging_data_profiling
[0m09:31:28.609609 [info ] [MainThread]: dbt_service.staging_analysis.staging_location_profiling
[0m09:31:28.609712 [info ] [MainThread]: dbt_service.staging.stg_entity_clusters
[0m09:31:28.609814 [info ] [MainThread]: dbt_service.staging.stg_fxf_data
[0m09:31:28.609916 [info ] [MainThread]: dbt_service.staging.stg_pdl_data
[0m09:31:28.610018 [info ] [MainThread]: dbt_service.staging.stg_prospect_matches
[0m09:31:28.610118 [info ] [MainThread]: dbt_service.staging.stg_unified_prospects
[0m09:31:28.610226 [info ] [MainThread]: dbt_service.fxf_data
[0m09:31:28.610328 [info ] [MainThread]: dbt_service.pdl_data
[0m09:31:28.610429 [info ] [MainThread]: dbt_service.state_iso_mapping
[0m09:31:28.610532 [info ] [MainThread]: source:dbt_service.public.fxf_data
[0m09:31:28.610632 [info ] [MainThread]: source:dbt_service.public.pdl_data
[0m09:31:28.610734 [info ] [MainThread]: dbt_service.marts.not_null_company_analysis_company
[0m09:31:28.610837 [info ] [MainThread]: dbt_service.marts.not_null_location_analysis_location
[0m09:31:28.610938 [info ] [MainThread]: dbt_service.raw.not_null_raw_fxf_data_fxf_id
[0m09:31:28.611038 [info ] [MainThread]: dbt_service.raw.not_null_raw_pdl_data_pdl_id
[0m09:31:28.611137 [info ] [MainThread]: dbt_service.staging.not_null_stg_fxf_data_email
[0m09:31:28.611240 [info ] [MainThread]: dbt_service.staging.not_null_stg_fxf_data_fxf_id
[0m09:31:28.611339 [info ] [MainThread]: dbt_service.staging.not_null_stg_pdl_data_pdl_id
[0m09:31:28.611488 [info ] [MainThread]: dbt_service.marts.unique_company_analysis_company
[0m09:31:28.611643 [info ] [MainThread]: dbt_service.marts.unique_location_analysis_location
[0m09:31:28.611763 [info ] [MainThread]: dbt_service.raw.unique_raw_fxf_data_fxf_id
[0m09:31:28.611878 [info ] [MainThread]: dbt_service.raw.unique_raw_pdl_data_pdl_id
[0m09:31:28.611985 [info ] [MainThread]: dbt_service.staging.unique_stg_fxf_data_fxf_id
[0m09:31:28.612095 [info ] [MainThread]: dbt_service.staging.unique_stg_pdl_data_pdl_id
[0m09:31:28.614165 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 0.35811985, "process_in_blocks": "0", "process_kernel_time": 0.172867, "process_mem_max_rss": "124485632", "process_out_blocks": "0", "process_user_time": 0.804688}
[0m09:31:28.614472 [debug] [MainThread]: Command `dbt list` succeeded at 09:31:28.614427 after 0.36 seconds
[0m09:31:28.614673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100940290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079d61d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10097b7d0>]}
[0m09:31:28.614821 [debug] [MainThread]: Flushing usage events
[0m09:31:28.842062 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:52:52.066254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10743a190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074b3a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074bbf90>]}


============================== 09:52:52.070405 | fc0cdda2-05b8-4819-8f1d-facc98f66bee ==============================
[0m09:52:52.070405 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:52:52.070904 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'use_colors': 'True', 'version_check': 'True', 'partial_parse': 'True', 'introspect': 'True', 'empty': 'None', 'write_json': 'True', 'target_path': 'None', 'log_format': 'default', 'printer_width': '80', 'invocation_command': 'dbt ls --select +marts --resource-type model', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'static_parser': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'fail_fast': 'False'}
[0m09:52:52.208189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc0cdda2-05b8-4819-8f1d-facc98f66bee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b88e90>]}
[0m09:52:52.237170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc0cdda2-05b8-4819-8f1d-facc98f66bee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104abd490>]}
[0m09:52:52.237784 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:52:52.284356 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:52:52.355374 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:52:52.355633 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:52:52.378820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc0cdda2-05b8-4819-8f1d-facc98f66bee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11851c090>]}
[0m09:52:52.419277 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:52:52.420444 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:52:52.431594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc0cdda2-05b8-4819-8f1d-facc98f66bee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11866c8d0>]}
[0m09:52:52.431827 [info ] [MainThread]: Found 18 models, 3 seeds, 13 data tests, 2 sources, 450 macros
[0m09:52:52.431988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc0cdda2-05b8-4819-8f1d-facc98f66bee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118660c90>]}
[0m09:52:52.432494 [info ] [MainThread]: dbt_service.staging_analysis.company_analysis
[0m09:52:52.432641 [info ] [MainThread]: dbt_service.marts.data_overview
[0m09:52:52.432753 [info ] [MainThread]: dbt_service.marts.location_analysis
[0m09:52:52.432861 [info ] [MainThread]: dbt_service.marts.prospect_matching_ratio
[0m09:52:52.432972 [info ] [MainThread]: dbt_service.raw.raw_fxf_data
[0m09:52:52.433075 [info ] [MainThread]: dbt_service.raw.raw_pdl_data
[0m09:52:52.433179 [info ] [MainThread]: dbt_service.staging.stg_fxf_data
[0m09:52:52.433281 [info ] [MainThread]: dbt_service.staging.stg_pdl_data
[0m09:52:52.433382 [info ] [MainThread]: dbt_service.staging.stg_prospect_matches
[0m09:52:52.433490 [info ] [MainThread]: dbt_service.staging.stg_unified_prospects
[0m09:52:52.435416 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 0.4196593, "process_in_blocks": "0", "process_kernel_time": 0.191318, "process_mem_max_rss": "122142720", "process_out_blocks": "0", "process_user_time": 0.816826}
[0m09:52:52.435642 [debug] [MainThread]: Command `dbt ls` succeeded at 09:52:52.435606 after 0.42 seconds
[0m09:52:52.435814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074bba10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a1c310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ae6c50>]}
[0m09:52:52.435950 [debug] [MainThread]: Flushing usage events
[0m09:52:52.891170 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:58:30.842987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e0b010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e879d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e87f90>]}


============================== 09:58:30.845343 | 6e628af5-e458-408f-b128-e4a0980b45ca ==============================
[0m09:58:30.845343 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:58:30.845620 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'no_print': 'None', 'use_experimental_parser': 'False', 'write_json': 'True', 'use_colors': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'printer_width': '80', 'debug': 'False', 'invocation_command': 'dbt run --select data_overview', 'empty': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'partial_parse': 'True', 'target_path': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'version_check': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True'}
[0m09:58:30.942372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109529b10>]}
[0m09:58:30.971535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bcf3d0>]}
[0m09:58:30.972208 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:58:31.018256 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:58:31.083667 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 2 files changed.
[0m09:58:31.084003 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/schema.yml
[0m09:58:31.084136 [debug] [MainThread]: Partial parsing: deleted file: dbt_service://models/marts/location_analysis.sql
[0m09:58:31.084282 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/data_overview.sql
[0m09:58:31.233980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ed9110>]}
[0m09:58:31.271949 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:58:31.272892 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:58:31.283696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b8b3d0>]}
[0m09:58:31.283903 [info ] [MainThread]: Found 17 models, 3 seeds, 11 data tests, 2 sources, 450 macros
[0m09:58:31.284057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a179a90>]}
[0m09:58:31.284731 [info ] [MainThread]: 
[0m09:58:31.284877 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m09:58:31.284995 [info ] [MainThread]: 
[0m09:58:31.285181 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:58:31.285518 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m09:58:31.311275 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m09:58:31.311491 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m09:58:31.311609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.404991 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.093 seconds
[0m09:58:31.405809 [debug] [ThreadPool]: On list_finny_db: Close
[0m09:58:31.408515 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m09:58:31.408797 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m09:58:31.411655 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m09:58:31.411876 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m09:58:31.412212 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m09:58:31.412448 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m09:58:31.413321 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m09:58:31.413508 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m09:58:31.413691 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m09:58:31.414490 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m09:58:31.415249 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m09:58:31.415996 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m09:58:31.416172 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m09:58:31.417325 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m09:58:31.417493 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:31.417631 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m09:58:31.417758 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m09:58:31.417890 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m09:58:31.418017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.418145 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m09:58:31.418360 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.418505 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.418633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.418876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:31.435931 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m09:58:31.436094 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m09:58:31.436242 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m09:58:31.440388 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m09:58:31.440953 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m09:58:31.441120 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m09:58:31.441287 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m09:58:31.441406 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m09:58:31.441526 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m09:58:31.441650 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m09:58:31.441790 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m09:58:31.441945 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m09:58:31.442080 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m09:58:31.442182 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m09:58:31.442445 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m09:58:31.443049 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m09:58:31.443188 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m09:58:31.443306 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m09:58:31.444825 [debug] [ThreadPool]: SQL status: BEGIN in 0.026 seconds
[0m09:58:31.445078 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m09:58:31.445274 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m09:58:31.445444 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m09:58:31.446010 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m09:58:31.446170 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m09:58:31.446314 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m09:58:31.446453 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.003 seconds
[0m09:58:31.447067 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m09:58:31.447207 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m09:58:31.447690 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m09:58:31.448112 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m09:58:31.448711 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m09:58:31.448854 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m09:58:31.449447 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m09:58:31.449632 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m09:58:31.450349 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m09:58:31.451116 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m09:58:31.455712 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.455922 [debug] [MainThread]: On master: BEGIN
[0m09:58:31.456059 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:58:31.464570 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m09:58:31.464898 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.465102 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:58:31.467702 [debug] [MainThread]: SQL status: SELECT 10 in 0.002 seconds
[0m09:58:31.469206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ebca10>]}
[0m09:58:31.469452 [debug] [MainThread]: On master: ROLLBACK
[0m09:58:31.469912 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.470108 [debug] [MainThread]: On master: BEGIN
[0m09:58:31.470824 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m09:58:31.471007 [debug] [MainThread]: On master: COMMIT
[0m09:58:31.471132 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.471250 [debug] [MainThread]: On master: COMMIT
[0m09:58:31.471588 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:58:31.471752 [debug] [MainThread]: On master: Close
[0m09:58:31.477078 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m09:58:31.477396 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.data_overview ......................... [RUN]
[0m09:58:31.477620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.data_overview)
[0m09:58:31.477772 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m09:58:31.481228 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m09:58:31.482092 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m09:58:31.496578 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m09:58:31.497441 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.497683 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m09:58:31.497831 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:58:31.504122 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m09:58:31.504407 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.504622 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unique prospects after deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m09:58:31.506371 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m09:58:31.508971 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.509140 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview" rename to "data_overview__dbt_backup"
[0m09:58:31.509883 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:58:31.511623 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.511825 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m09:58:31.512526 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m09:58:31.518091 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m09:58:31.518243 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.518370 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m09:58:31.519690 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m09:58:31.522382 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m09:58:31.523963 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m09:58:31.524098 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m09:58:31.525426 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m09:58:31.526780 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m09:58:31.527867 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e628af5-e458-408f-b128-e4a0980b45ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a60ee50>]}
[0m09:58:31.528293 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.05s]
[0m09:58:31.528583 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m09:58:31.529670 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.529817 [debug] [MainThread]: On master: BEGIN
[0m09:58:31.529944 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:58:31.535147 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m09:58:31.535320 [debug] [MainThread]: On master: COMMIT
[0m09:58:31.535452 [debug] [MainThread]: Using postgres connection "master"
[0m09:58:31.535572 [debug] [MainThread]: On master: COMMIT
[0m09:58:31.535848 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:58:31.536025 [debug] [MainThread]: On master: Close
[0m09:58:31.536234 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:58:31.536363 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m09:58:31.536476 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m09:58:31.536583 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m09:58:31.536686 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m09:58:31.536790 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m09:58:31.536891 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m09:58:31.537022 [info ] [MainThread]: 
[0m09:58:31.537160 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m09:58:31.537418 [debug] [MainThread]: Command end result
[0m09:58:31.549253 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m09:58:31.549995 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m09:58:31.554226 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m09:58:31.554442 [info ] [MainThread]: 
[0m09:58:31.554643 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:58:31.554981 [info ] [MainThread]: 
[0m09:58:31.555787 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m09:58:31.559112 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.74913347, "process_in_blocks": "0", "process_kernel_time": 0.195964, "process_mem_max_rss": "140673024", "process_out_blocks": "0", "process_user_time": 1.077306}
[0m09:58:31.559419 [debug] [MainThread]: Command `dbt run` succeeded at 09:58:31.559382 after 0.75 seconds
[0m09:58:31.559583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043f4350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108eb40d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044c2c90>]}
[0m09:58:31.559742 [debug] [MainThread]: Flushing usage events
[0m09:58:31.879448 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:00:16.868650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105338e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053bf950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053bff90>]}


============================== 10:00:16.871272 | 9028a02a-a977-4f12-9050-267e22ce9f50 ==============================
[0m10:00:16.871272 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:00:16.871565 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'static_parser': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'write_json': 'True', 'debug': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select data_overview', 'printer_width': '80', 'version_check': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'log_format': 'default', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False'}
[0m10:00:16.966662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049d87d0>]}
[0m10:00:16.995933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029998d0>]}
[0m10:00:16.996482 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:00:17.040788 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:00:17.107058 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:00:17.107549 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/marts/data_overview.sql
[0m10:00:17.234159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065a4290>]}
[0m10:00:17.302833 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:00:17.303972 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:00:17.314869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106536950>]}
[0m10:00:17.315112 [info ] [MainThread]: Found 17 models, 3 seeds, 11 data tests, 2 sources, 450 macros
[0m10:00:17.315282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028c7b10>]}
[0m10:00:17.316011 [info ] [MainThread]: 
[0m10:00:17.316168 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:00:17.316282 [info ] [MainThread]: 
[0m10:00:17.316493 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:00:17.316882 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:00:17.341303 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:00:17.341513 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:00:17.341632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.370034 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.028 seconds
[0m10:00:17.370690 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:00:17.373120 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m10:00:17.373405 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m10:00:17.376088 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:00:17.376255 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m10:00:17.376563 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m10:00:17.376836 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m10:00:17.377552 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:00:17.377743 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m10:00:17.377905 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:00:17.378584 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:00:17.379248 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:00:17.379896 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:00:17.380017 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:00:17.380623 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:00:17.380744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:00:17.380852 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:00:17.380956 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:00:17.381059 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:00:17.381165 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.381268 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:00:17.381462 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.381590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.381754 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.381951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:00:17.390315 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m10:00:17.390451 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:00:17.390592 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:00:17.393253 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m10:00:17.393793 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:00:17.394259 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:00:17.396393 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:00:17.396620 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:00:17.396863 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:00:17.397936 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:00:17.398134 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:00:17.398301 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:00:17.398422 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:00:17.398521 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m10:00:17.398650 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:00:17.398762 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:00:17.398880 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:00:17.398999 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:00:17.399453 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:00:17.399586 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:00:17.399725 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:00:17.399861 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:00:17.399994 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:00:17.400464 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:00:17.401677 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m10:00:17.401925 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m10:00:17.402089 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m10:00:17.402293 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:00:17.402880 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:00:17.403481 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:00:17.403889 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:00:17.404270 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:00:17.404695 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:00:17.404860 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:00:17.404991 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:00:17.405118 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:00:17.408929 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.409075 [debug] [MainThread]: On master: BEGIN
[0m10:00:17.409182 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:00:17.426785 [debug] [MainThread]: SQL status: BEGIN in 0.017 seconds
[0m10:00:17.427121 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.427317 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:00:17.432974 [debug] [MainThread]: SQL status: SELECT 11 in 0.005 seconds
[0m10:00:17.434260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069f3950>]}
[0m10:00:17.434470 [debug] [MainThread]: On master: ROLLBACK
[0m10:00:17.434962 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.435134 [debug] [MainThread]: On master: BEGIN
[0m10:00:17.435857 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:00:17.436095 [debug] [MainThread]: On master: COMMIT
[0m10:00:17.436261 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.436383 [debug] [MainThread]: On master: COMMIT
[0m10:00:17.436760 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:00:17.436926 [debug] [MainThread]: On master: Close
[0m10:00:17.441659 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m10:00:17.441998 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.data_overview ......................... [RUN]
[0m10:00:17.442326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.data_overview)
[0m10:00:17.442521 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m10:00:17.445848 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m10:00:17.447298 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m10:00:17.462603 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m10:00:17.463053 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.463204 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m10:00:17.463333 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:00:17.469378 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m10:00:17.469626 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.469872 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m10:00:17.471562 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:00:17.474817 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.475002 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview" rename to "data_overview__dbt_backup"
[0m10:00:17.475620 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:00:17.476826 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.476984 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m10:00:17.477484 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:00:17.483342 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m10:00:17.483507 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.483632 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m10:00:17.484328 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:00:17.486920 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m10:00:17.488523 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m10:00:17.488663 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m10:00:17.489634 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m10:00:17.490900 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m10:00:17.491894 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9028a02a-a977-4f12-9050-267e22ce9f50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065d9810>]}
[0m10:00:17.492234 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.05s]
[0m10:00:17.492489 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m10:00:17.493516 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.493648 [debug] [MainThread]: On master: BEGIN
[0m10:00:17.493763 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:00:17.498839 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m10:00:17.499011 [debug] [MainThread]: On master: COMMIT
[0m10:00:17.499143 [debug] [MainThread]: Using postgres connection "master"
[0m10:00:17.499253 [debug] [MainThread]: On master: COMMIT
[0m10:00:17.499587 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:00:17.499894 [debug] [MainThread]: On master: Close
[0m10:00:17.500131 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:00:17.500302 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m10:00:17.500423 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m10:00:17.500533 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m10:00:17.500640 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m10:00:17.500751 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m10:00:17.500858 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m10:00:17.501001 [info ] [MainThread]: 
[0m10:00:17.501171 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m10:00:17.501457 [debug] [MainThread]: Command end result
[0m10:00:17.513218 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:00:17.513930 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:00:17.516139 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:00:17.516261 [info ] [MainThread]: 
[0m10:00:17.516419 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:00:17.516532 [info ] [MainThread]: 
[0m10:00:17.516653 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:00:17.518920 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.68486726, "process_in_blocks": "0", "process_kernel_time": 0.197216, "process_mem_max_rss": "142114816", "process_out_blocks": "0", "process_user_time": 1.078233}
[0m10:00:17.519298 [debug] [MainThread]: Command `dbt run` succeeded at 10:00:17.519253 after 0.69 seconds
[0m10:00:17.519473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1009280d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10533a610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10095f750>]}
[0m10:00:17.519651 [debug] [MainThread]: Flushing usage events
[0m10:00:18.662071 [debug] [MainThread]: An error was encountered while trying to flush usage events
==================== 2025-11-09 15:14:20.663206 | 019a692e-b76c-7ce1-a314-144126d51643 ====================
15:14:25.744028 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [5.1s]
==================== 2025-11-09 15:14:46.900858 | 019a692f-1df4-7b03-8297-ade849911ae9 ====================
==================== 2025-11-09 15:14:54.679655 | 019a692f-3c56-7781-9567-66d91175f84d ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
15:14:55.142843 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [8.2s]
==================== 2025-11-09 15:16:04.302316 | 019a6930-4c4b-7cc3-8c45-6f0c1388b6f4 ====================
==================== 2025-11-09 15:16:21.127220 | 019a6930-8e06-79b2-918a-f99bc26277fe ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
15:16:21.552064 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [17.2s]
[0m10:16:42.201323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105793750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105806210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105813c90>]}


============================== 10:16:42.204091 | 7a649731-4d9c-4cf6-b5ae-a15671af7022 ==============================
[0m10:16:42.204091 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:16:42.204411 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'cache_selected_only': 'False', 'fail_fast': 'False', 'no_print': 'None', 'introspect': 'True', 'empty': 'False', 'quiet': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'debug': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run', 'warn_error': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80'}
[0m10:16:42.331851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105813750>]}
[0m10:16:42.362700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e15a50>]}
[0m10:16:42.363375 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:16:42.409743 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:16:42.474415 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:16:42.474659 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:16:42.497410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f2f610>]}
[0m10:16:42.536453 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:16:42.537564 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:16:42.549881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a3bdd0>]}
[0m10:16:42.550124 [info ] [MainThread]: Found 17 models, 3 seeds, 11 data tests, 2 sources, 450 macros
[0m10:16:42.550284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a11b50>]}
[0m10:16:42.551272 [info ] [MainThread]: 
[0m10:16:42.551436 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:16:42.551556 [info ] [MainThread]: 
[0m10:16:42.551766 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:16:42.553592 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:16:42.553839 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:16:42.554676 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:16:42.558853 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:16:42.560044 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:16:42.607042 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:16:42.607217 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:16:42.607379 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:16:42.607529 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:16:42.607673 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:16:42.607803 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:16:42.607956 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:16:42.608092 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:16:42.608227 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:16:42.608360 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:16:42.608487 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.608607 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.608715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.608823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.608951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.659937 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.051 seconds
[0m10:16:42.660131 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.051 seconds
[0m10:16:42.660275 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.052 seconds
[0m10:16:42.660447 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.052 seconds
[0m10:16:42.660584 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.052 seconds
[0m10:16:42.661091 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:16:42.661522 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:16:42.661886 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:16:42.662255 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:16:42.662600 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:16:42.663972 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m10:16:42.664176 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m10:16:42.666480 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:16:42.666680 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m10:16:42.666856 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m10:16:42.667040 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m10:16:42.667238 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m10:16:42.667951 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:16:42.668107 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:16:42.668792 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:16:42.669449 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:16:42.670055 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:16:42.670656 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:16:42.670775 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:16:42.670882 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:42.670991 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:16:42.671106 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:16:42.671216 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:16:42.671317 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:16:42.671417 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:42.671604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:42.671749 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:42.671888 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:42.672013 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:42.687059 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:16:42.687191 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:16:42.687319 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:16:42.687673 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:16:42.687798 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:16:42.687925 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:16:42.690385 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m10:16:42.690544 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m10:16:42.690741 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m10:16:42.690856 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m10:16:42.690987 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:16:42.691149 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:16:42.691290 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:16:42.691410 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:16:42.691549 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:16:42.691740 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:16:42.691893 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:16:42.692043 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:16:42.696059 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.004 seconds
[0m10:16:42.696744 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:16:42.697352 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:16:42.697636 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.010 seconds
[0m10:16:42.698275 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.010 seconds
[0m10:16:42.698847 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:16:42.698989 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.007 seconds
[0m10:16:42.699117 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.007 seconds
[0m10:16:42.699283 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.007 seconds
[0m10:16:42.699891 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:16:42.700458 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:16:42.700633 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:16:42.701181 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:16:42.701621 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:16:42.702318 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:16:42.702486 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:16:42.702647 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:16:42.702998 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:16:42.707369 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:42.707590 [debug] [MainThread]: On master: BEGIN
[0m10:16:42.707715 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:16:42.713541 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:16:42.713713 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:42.713882 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:16:42.717114 [debug] [MainThread]: SQL status: SELECT 11 in 0.003 seconds
[0m10:16:42.718545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e15450>]}
[0m10:16:42.718787 [debug] [MainThread]: On master: ROLLBACK
[0m10:16:42.719311 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:42.719470 [debug] [MainThread]: On master: BEGIN
[0m10:16:42.720059 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:16:42.720216 [debug] [MainThread]: On master: COMMIT
[0m10:16:42.720364 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:42.720472 [debug] [MainThread]: On master: COMMIT
[0m10:16:42.720849 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:16:42.721094 [debug] [MainThread]: On master: Close
[0m10:16:42.723575 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m10:16:42.723783 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m10:16:42.724050 [info ] [Thread-1 (]: 1 of 17 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m10:16:42.724273 [info ] [Thread-2 (]: 2 of 17 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m10:16:42.724520 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_fxf_data)
[0m10:16:42.724705 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_pdl_data)
[0m10:16:42.724864 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m10:16:42.725017 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m10:16:42.729321 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m10:16:42.730366 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m10:16:42.730923 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m10:16:42.731141 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m10:16:42.753825 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m10:16:42.755387 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m10:16:42.755907 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.756094 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.756274 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m10:16:42.756429 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m10:16:42.756627 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:16:42.756819 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:16:42.763188 [debug] [Thread-2 (]: SQL status: BEGIN in 0.006 seconds
[0m10:16:42.763394 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m10:16:42.763700 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.763884 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.764070 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m10:16:42.764287 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m10:16:42.819995 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.055 seconds
[0m10:16:42.824545 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.824768 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.060 seconds
[0m10:16:42.825005 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m10:16:42.826479 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.826765 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m10:16:42.827398 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:42.827586 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:42.829016 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.830516 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.830706 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m10:16:42.830866 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m10:16:42.831470 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:42.831717 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:42.838984 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:16:42.839673 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:16:42.839938 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.840140 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.840293 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:16:42.840447 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:16:42.851840 [debug] [Thread-2 (]: SQL status: COMMIT in 0.011 seconds
[0m10:16:42.852064 [debug] [Thread-1 (]: SQL status: COMMIT in 0.011 seconds
[0m10:16:42.855884 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m10:16:42.856935 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m10:16:42.858975 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:16:42.859221 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:16:42.859386 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m10:16:42.859552 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m10:16:42.862802 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.003 seconds
[0m10:16:42.862990 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m10:16:42.864100 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m10:16:42.864617 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m10:16:42.866137 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f33410>]}
[0m10:16:42.866292 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065074d0>]}
[0m10:16:42.866642 [info ] [Thread-2 (]: 2 of 17 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.14s]
[0m10:16:42.866883 [info ] [Thread-1 (]: 1 of 17 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.14s]
[0m10:16:42.867173 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m10:16:42.867390 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m10:16:42.867682 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_pdl_data
[0m10:16:42.867947 [info ] [Thread-1 (]: 3 of 17 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m10:16:42.868188 [debug] [Thread-5 (]: Began running node model.dbt_service.raw_company_profiling
[0m10:16:42.868342 [debug] [Thread-6 (]: Began running node model.dbt_service.raw_data_profiling
[0m10:16:42.868526 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_fxf_data, now model.dbt_service.stg_pdl_data)
[0m10:16:42.868686 [debug] [Thread-7 (]: Began running node model.dbt_service.raw_location_profiling
[0m10:16:42.868834 [debug] [Thread-8 (]: Began running node model.dbt_service.stg_fxf_data
[0m10:16:42.869039 [info ] [Thread-5 (]: 4 of 17 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m10:16:42.869262 [info ] [Thread-6 (]: 5 of 17 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m10:16:42.869451 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m10:16:42.869658 [info ] [Thread-7 (]: 6 of 17 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m10:16:42.869891 [info ] [Thread-8 (]: 7 of 17 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m10:16:42.870088 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_company_profiling)
[0m10:16:42.870262 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_data_profiling)
[0m10:16:42.871756 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m10:16:42.871974 [debug] [Thread-7 (]: Acquiring new postgres connection 'model.dbt_service.raw_location_profiling'
[0m10:16:42.872152 [debug] [Thread-8 (]: Acquiring new postgres connection 'model.dbt_service.stg_fxf_data'
[0m10:16:42.872303 [debug] [Thread-5 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m10:16:42.872448 [debug] [Thread-6 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m10:16:42.872683 [debug] [Thread-7 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m10:16:42.872846 [debug] [Thread-8 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m10:16:42.874315 [debug] [Thread-5 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m10:16:42.875889 [debug] [Thread-6 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m10:16:42.876064 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_pdl_data
[0m10:16:42.877981 [debug] [Thread-7 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m10:16:42.879865 [debug] [Thread-8 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m10:16:42.887408 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m10:16:42.887789 [debug] [Thread-5 (]: Began executing node model.dbt_service.raw_company_profiling
[0m10:16:42.887958 [debug] [Thread-6 (]: Began executing node model.dbt_service.raw_data_profiling
[0m10:16:42.889582 [debug] [Thread-5 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m10:16:42.889758 [debug] [Thread-7 (]: Began executing node model.dbt_service.raw_location_profiling
[0m10:16:42.889934 [debug] [Thread-8 (]: Began executing node model.dbt_service.stg_fxf_data
[0m10:16:42.892293 [debug] [Thread-6 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m10:16:42.892563 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:16:42.894138 [debug] [Thread-7 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m10:16:42.895582 [debug] [Thread-8 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m10:16:42.895861 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:42.896082 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m10:16:42.896357 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:42.896580 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m10:16:42.896750 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:16:42.896954 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:42.897135 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m10:16:42.897304 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:16:42.897474 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m10:16:42.897732 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m10:16:42.897895 [debug] [Thread-6 (]: Opening a new connection, currently in state closed
[0m10:16:42.898053 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m10:16:42.898269 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m10:16:42.898488 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m10:16:42.909059 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m10:16:42.909324 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:16:42.909594 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m10:16:42.910161 [debug] [Thread-7 (]: SQL status: BEGIN in 0.012 seconds
[0m10:16:42.910365 [debug] [Thread-5 (]: SQL status: BEGIN in 0.013 seconds
[0m10:16:42.910591 [debug] [Thread-6 (]: SQL status: BEGIN in 0.013 seconds
[0m10:16:42.910792 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:42.910933 [debug] [Thread-8 (]: SQL status: BEGIN in 0.012 seconds
[0m10:16:42.911110 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:16:42.911367 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:42.911530 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:42.911849 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m10:16:42.912181 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:16:42.913591 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:16:42.913819 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:16:42.914081 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m10:16:42.914369 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m10:16:42.914587 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m10:16:42.915218 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:42.915810 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:16:42.915993 [debug] [Thread-8 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:16:42.916162 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:16:42.917769 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:16:42.917941 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:16:42.918100 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m10:16:42.918780 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:42.919320 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:16:42.919488 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:16:42.919632 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:16:42.919855 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m10:16:42.921004 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m10:16:42.921418 [debug] [Thread-8 (]: SQL status: COMMIT in 0.002 seconds
[0m10:16:42.922840 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:16:42.923986 [debug] [Thread-8 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m10:16:42.924222 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m10:16:42.924627 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:16:42.924852 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m10:16:42.925329 [debug] [Thread-8 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:16:42.925502 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m10:16:42.926088 [debug] [Thread-8 (]: On model.dbt_service.stg_fxf_data: Close
[0m10:16:42.926693 [debug] [Thread-1 (]: On model.dbt_service.stg_pdl_data: Close
[0m10:16:42.926991 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e80bd0>]}
[0m10:16:42.927269 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ed7b90>]}
[0m10:16:42.927564 [info ] [Thread-1 (]: 3 of 17 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m10:16:42.928068 [info ] [Thread-8 (]: 7 of 17 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m10:16:42.928415 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_pdl_data
[0m10:16:42.928825 [debug] [Thread-8 (]: Finished running node model.dbt_service.stg_fxf_data
[0m10:16:42.929265 [debug] [Thread-10 ]: Began running node model.dbt_service.company_analysis
[0m10:16:42.929450 [debug] [Thread-11 ]: Began running node model.dbt_service.staging_city_state_profiling
[0m10:16:42.929618 [debug] [Thread-12 ]: Began running node model.dbt_service.staging_company_profiling
[0m10:16:42.929801 [debug] [Thread-13 ]: Began running node model.dbt_service.staging_data_profiling
[0m10:16:42.930192 [debug] [Thread-14 ]: Began running node model.dbt_service.staging_location_profiling
[0m10:16:42.930012 [info ] [Thread-10 ]: 8 of 17 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m10:16:42.930483 [debug] [Thread-15 ]: Began running node model.dbt_service.stg_unified_prospects
[0m10:16:42.930795 [info ] [Thread-11 ]: 9 of 17 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m10:16:42.931041 [info ] [Thread-12 ]: 10 of 17 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m10:16:42.931247 [info ] [Thread-13 ]: 11 of 17 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m10:16:42.931528 [info ] [Thread-14 ]: 12 of 17 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m10:16:42.931821 [debug] [Thread-10 ]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m10:16:42.932038 [info ] [Thread-15 ]: 13 of 17 START sql table model public_staging.stg_unified_prospects ............ [RUN]
[0m10:16:42.932341 [debug] [Thread-11 ]: Acquiring new postgres connection 'model.dbt_service.staging_city_state_profiling'
[0m10:16:42.932531 [debug] [Thread-12 ]: Acquiring new postgres connection 'model.dbt_service.staging_company_profiling'
[0m10:16:42.932711 [debug] [Thread-13 ]: Acquiring new postgres connection 'model.dbt_service.staging_data_profiling'
[0m10:16:42.933148 [debug] [Thread-14 ]: Acquiring new postgres connection 'model.dbt_service.staging_location_profiling'
[0m10:16:42.933350 [debug] [Thread-10 ]: Began compiling node model.dbt_service.company_analysis
[0m10:16:42.933921 [debug] [Thread-15 ]: Acquiring new postgres connection 'model.dbt_service.stg_unified_prospects'
[0m10:16:42.934127 [debug] [Thread-11 ]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m10:16:42.934545 [debug] [Thread-12 ]: Began compiling node model.dbt_service.staging_company_profiling
[0m10:16:42.935130 [debug] [Thread-13 ]: Began compiling node model.dbt_service.staging_data_profiling
[0m10:16:42.935406 [debug] [Thread-14 ]: Began compiling node model.dbt_service.staging_location_profiling
[0m10:16:42.937346 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m10:16:42.937602 [debug] [Thread-15 ]: Began compiling node model.dbt_service.stg_unified_prospects
[0m10:16:42.939389 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m10:16:42.940922 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m10:16:42.943247 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m10:16:42.944919 [debug] [Thread-14 ]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m10:16:42.946274 [debug] [Thread-15 ]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m10:16:42.946573 [debug] [Thread-10 ]: Began executing node model.dbt_service.company_analysis
[0m10:16:42.946892 [debug] [Thread-12 ]: Began executing node model.dbt_service.staging_company_profiling
[0m10:16:42.947102 [debug] [Thread-11 ]: Began executing node model.dbt_service.staging_city_state_profiling
[0m10:16:42.947268 [debug] [Thread-13 ]: Began executing node model.dbt_service.staging_data_profiling
[0m10:16:42.947412 [debug] [Thread-14 ]: Began executing node model.dbt_service.staging_location_profiling
[0m10:16:42.948914 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m10:16:42.949093 [debug] [Thread-15 ]: Began executing node model.dbt_service.stg_unified_prospects
[0m10:16:42.950592 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m10:16:42.952029 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m10:16:42.953381 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m10:16:42.954617 [debug] [Thread-14 ]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m10:16:42.956127 [debug] [Thread-15 ]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m10:16:42.956478 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:42.956784 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:42.956961 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:42.957208 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:42.957417 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: BEGIN
[0m10:16:42.957602 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:42.957784 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: BEGIN
[0m10:16:42.958019 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:42.958177 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m10:16:42.958353 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: BEGIN
[0m10:16:42.958531 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m10:16:42.958706 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: BEGIN
[0m10:16:42.958864 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m10:16:42.959019 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m10:16:42.959173 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m10:16:42.959328 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m10:16:42.959550 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m10:16:42.959749 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m10:16:42.975389 [debug] [Thread-13 ]: SQL status: BEGIN in 0.016 seconds
[0m10:16:42.975707 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:42.976018 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m10:16:42.976315 [debug] [Thread-12 ]: SQL status: BEGIN in 0.017 seconds
[0m10:16:42.976613 [debug] [Thread-11 ]: SQL status: BEGIN in 0.017 seconds
[0m10:16:42.976873 [debug] [Thread-10 ]: SQL status: BEGIN in 0.018 seconds
[0m10:16:42.977063 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:42.977253 [debug] [Thread-14 ]: SQL status: BEGIN in 0.018 seconds
[0m10:16:42.977454 [debug] [Thread-15 ]: SQL status: BEGIN in 0.018 seconds
[0m10:16:42.977624 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:42.977817 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:42.978091 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:16:42.978351 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:42.978502 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:42.978757 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m10:16:42.979068 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m10:16:42.979544 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m10:16:42.979930 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m10:16:43.019056 [debug] [Thread-5 (]: SQL status: SELECT 180 in 0.104 seconds
[0m10:16:43.023972 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:43.024226 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m10:16:43.026672 [debug] [Thread-5 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:16:43.028224 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:43.028445 [debug] [Thread-6 (]: SQL status: SELECT 2 in 0.114 seconds
[0m10:16:43.028596 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m10:16:43.029901 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:43.030331 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m10:16:43.030769 [debug] [Thread-5 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.032043 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:16:43.032386 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.032952 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:43.034590 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:43.034783 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:16:43.034943 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m10:16:43.036162 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.036835 [debug] [Thread-5 (]: SQL status: COMMIT in 0.002 seconds
[0m10:16:43.038247 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:16:43.040623 [debug] [Thread-5 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m10:16:43.040995 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:43.041686 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:16:43.042004 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:16:43.042291 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m10:16:43.046117 [debug] [Thread-6 (]: SQL status: COMMIT in 0.003 seconds
[0m10:16:43.048529 [debug] [Thread-6 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m10:16:43.048886 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:16:43.049048 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m10:16:43.049724 [debug] [Thread-5 (]: SQL status: DROP TABLE in 0.007 seconds
[0m10:16:43.051134 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: Close
[0m10:16:43.051881 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e2d910>]}
[0m10:16:43.052577 [info ] [Thread-5 (]: 4 of 17 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.18s]
[0m10:16:43.052917 [debug] [Thread-5 (]: Finished running node model.dbt_service.raw_company_profiling
[0m10:16:43.054306 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.005 seconds
[0m10:16:43.055689 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: Close
[0m10:16:43.056049 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e14790>]}
[0m10:16:43.056513 [info ] [Thread-6 (]: 5 of 17 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.19s]
[0m10:16:43.056995 [debug] [Thread-6 (]: Finished running node model.dbt_service.raw_data_profiling
[0m10:16:43.109829 [debug] [Thread-7 (]: SQL status: SELECT 152 in 0.195 seconds
[0m10:16:43.111717 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:43.111959 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m10:16:43.112698 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.113860 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:43.114072 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m10:16:43.114516 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.115007 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:16:43.115160 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:43.115295 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:16:43.116235 [debug] [Thread-7 (]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.117267 [debug] [Thread-7 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m10:16:43.117499 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:16:43.117805 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m10:16:43.118949 [debug] [Thread-7 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.119508 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: Close
[0m10:16:43.119775 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e72cd0>]}
[0m10:16:43.120046 [info ] [Thread-7 (]: 6 of 17 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.25s]
[0m10:16:43.120281 [debug] [Thread-7 (]: Finished running node model.dbt_service.raw_location_profiling
[0m10:16:43.124904 [debug] [Thread-13 ]: SQL status: SELECT 2 in 0.148 seconds
[0m10:16:43.126269 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:43.126444 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m10:16:43.127169 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.128789 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:43.129000 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m10:16:43.129516 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.130035 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:16:43.130188 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:43.130322 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:16:43.130997 [debug] [Thread-13 ]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.132935 [debug] [Thread-13 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m10:16:43.133191 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:16:43.133347 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m10:16:43.134410 [debug] [Thread-13 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.134887 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: Close
[0m10:16:43.135135 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa26010>]}
[0m10:16:43.135396 [info ] [Thread-13 ]: 11 of 17 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.20s]
[0m10:16:43.135760 [debug] [Thread-13 ]: Finished running node model.dbt_service.staging_data_profiling
[0m10:16:43.150792 [debug] [Thread-11 ]: SQL status: SELECT 76 in 0.171 seconds
[0m10:16:43.152389 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:43.152579 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m10:16:43.153161 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.154242 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:43.154388 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m10:16:43.154804 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.155261 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:16:43.155420 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:43.155555 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:16:43.156166 [debug] [Thread-11 ]: SQL status: COMMIT in 0.000 seconds
[0m10:16:43.157153 [debug] [Thread-11 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m10:16:43.157376 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:16:43.157526 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m10:16:43.158656 [debug] [Thread-11 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.159156 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: Close
[0m10:16:43.159461 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa150d0>]}
[0m10:16:43.159766 [info ] [Thread-11 ]: 9 of 17 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.23s]
[0m10:16:43.160011 [debug] [Thread-11 ]: Finished running node model.dbt_service.staging_city_state_profiling
[0m10:16:43.175112 [debug] [Thread-12 ]: SQL status: SELECT 180 in 0.196 seconds
[0m10:16:43.176524 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:43.176692 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m10:16:43.177278 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.178364 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:43.178519 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m10:16:43.178983 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.179580 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:16:43.179746 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:43.179882 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:16:43.180571 [debug] [Thread-12 ]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.181672 [debug] [Thread-12 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m10:16:43.181886 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:16:43.182023 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m10:16:43.183147 [debug] [Thread-12 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.183726 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: Close
[0m10:16:43.183990 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a86d50>]}
[0m10:16:43.184261 [info ] [Thread-12 ]: 10 of 17 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.25s]
[0m10:16:43.184492 [debug] [Thread-12 ]: Finished running node model.dbt_service.staging_company_profiling
[0m10:16:43.254785 [debug] [Thread-10 ]: SQL status: SELECT 91 in 0.275 seconds
[0m10:16:43.256313 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:43.256473 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m10:16:43.256998 [debug] [Thread-10 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.258122 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:43.258273 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m10:16:43.258674 [debug] [Thread-10 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.259116 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:16:43.259248 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:43.259368 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:16:43.260280 [debug] [Thread-10 ]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.261144 [debug] [Thread-10 ]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m10:16:43.261348 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:16:43.261498 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m10:16:43.262556 [debug] [Thread-10 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.263055 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: Close
[0m10:16:43.263331 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ed50d0>]}
[0m10:16:43.263594 [info ] [Thread-10 ]: 8 of 17 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.33s]
[0m10:16:43.263835 [debug] [Thread-10 ]: Finished running node model.dbt_service.company_analysis
[0m10:16:43.297496 [debug] [Thread-14 ]: SQL status: SELECT 152 in 0.317 seconds
[0m10:16:43.299218 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:43.299372 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m10:16:43.299919 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.300838 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:43.302133 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m10:16:43.302650 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.303144 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:16:43.303297 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:43.303430 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:16:43.304808 [debug] [Thread-14 ]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.305792 [debug] [Thread-14 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m10:16:43.306016 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:16:43.306164 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m10:16:43.308267 [debug] [Thread-14 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:16:43.308852 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: Close
[0m10:16:43.309112 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e26e50>]}
[0m10:16:43.309389 [info ] [Thread-14 ]: 12 of 17 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.38s]
[0m10:16:43.309628 [debug] [Thread-14 ]: Finished running node model.dbt_service.staging_location_profiling
[0m10:16:43.345248 [debug] [Thread-15 ]: SQL status: SELECT 100010 in 0.365 seconds
[0m10:16:43.347036 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:43.347201 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m10:16:43.347783 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.349809 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:43.350079 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m10:16:43.350544 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.351001 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:16:43.351141 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:43.351264 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:16:43.386108 [debug] [Thread-15 ]: SQL status: COMMIT in 0.035 seconds
[0m10:16:43.388970 [debug] [Thread-15 ]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m10:16:43.389536 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:16:43.389862 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m10:16:43.393074 [debug] [Thread-15 ]: SQL status: DROP TABLE in 0.003 seconds
[0m10:16:43.394267 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: Close
[0m10:16:43.394799 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f870d10>]}
[0m10:16:43.395330 [info ] [Thread-15 ]: 13 of 17 OK created sql table model public_staging.stg_unified_prospects ....... [[32mSELECT 100010[0m in 0.46s]
[0m10:16:43.395816 [debug] [Thread-15 ]: Finished running node model.dbt_service.stg_unified_prospects
[0m10:16:43.396523 [debug] [Thread-17 ]: Began running node model.dbt_service.stg_prospect_matches
[0m10:16:43.396941 [info ] [Thread-17 ]: 14 of 17 START sql incremental model public_staging.stg_prospect_matches ....... [RUN]
[0m10:16:43.397586 [debug] [Thread-17 ]: Acquiring new postgres connection 'model.dbt_service.stg_prospect_matches'
[0m10:16:43.397939 [debug] [Thread-17 ]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:16:43.404626 [debug] [Thread-17 ]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:16:43.405657 [debug] [Thread-17 ]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:16:43.422523 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.422847 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp101643419731"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m10:16:43.423131 [debug] [Thread-17 ]: Opening a new connection, currently in state init
[0m10:16:43.476465 [debug] [Thread-17 ]: SQL status: SELECT 0 in 0.053 seconds
[0m10:16:43.482819 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.483141 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:16:43.483710 [debug] [Thread-17 ]: SQL status: BEGIN in 0.000 seconds
[0m10:16:43.483921 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.484087 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp101643419731'
        
      order by ordinal_position

  
[0m10:16:43.487457 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.003 seconds
[0m10:16:43.489579 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.489753 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:16:43.491080 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:16:43.496810 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.497101 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp101643419731'
        
      order by ordinal_position

  
[0m10:16:43.498509 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:16:43.501449 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.501689 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:16:43.503010 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:16:43.508177 [debug] [Thread-17 ]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:16:43.515254 [debug] [Thread-17 ]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:16:43.515718 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.515868 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp101643419731" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp101643419731"
    )
  
[0m10:16:43.516553 [debug] [Thread-17 ]: SQL status: INSERT 0 0 in 0.001 seconds
[0m10:16:43.517024 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:16:43.517163 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:16:43.517286 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:16:43.517808 [debug] [Thread-17 ]: SQL status: COMMIT in 0.000 seconds
[0m10:16:43.518115 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: Close
[0m10:16:43.518387 [debug] [Thread-17 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6a210>]}
[0m10:16:43.518671 [info ] [Thread-17 ]: 14 of 17 OK created sql incremental model public_staging.stg_prospect_matches .. [[32mINSERT 0 0[0m in 0.12s]
[0m10:16:43.518922 [debug] [Thread-17 ]: Finished running node model.dbt_service.stg_prospect_matches
[0m10:16:43.519243 [debug] [Thread-19 ]: Began running node model.dbt_service.prospect_matching_ratio
[0m10:16:43.519422 [debug] [Thread-20 ]: Began running node model.dbt_service.stg_entity_clusters
[0m10:16:43.519663 [info ] [Thread-19 ]: 15 of 17 START sql view model public_marts.prospect_matching_ratio ............. [RUN]
[0m10:16:43.519875 [info ] [Thread-20 ]: 16 of 17 START sql table model public_staging.stg_entity_clusters .............. [RUN]
[0m10:16:43.520093 [debug] [Thread-19 ]: Acquiring new postgres connection 'model.dbt_service.prospect_matching_ratio'
[0m10:16:43.520275 [debug] [Thread-20 ]: Acquiring new postgres connection 'model.dbt_service.stg_entity_clusters'
[0m10:16:43.520439 [debug] [Thread-19 ]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m10:16:43.520627 [debug] [Thread-20 ]: Began compiling node model.dbt_service.stg_entity_clusters
[0m10:16:43.522377 [debug] [Thread-19 ]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.523673 [debug] [Thread-20 ]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m10:16:43.524238 [debug] [Thread-19 ]: Began executing node model.dbt_service.prospect_matching_ratio
[0m10:16:43.524376 [debug] [Thread-20 ]: Began executing node model.dbt_service.stg_entity_clusters
[0m10:16:43.525927 [debug] [Thread-19 ]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.527328 [debug] [Thread-20 ]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m10:16:43.527819 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.527986 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.528222 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m10:16:43.528542 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m10:16:43.528753 [debug] [Thread-19 ]: Opening a new connection, currently in state init
[0m10:16:43.528935 [debug] [Thread-20 ]: Opening a new connection, currently in state init
[0m10:16:43.535516 [debug] [Thread-19 ]: SQL status: BEGIN in 0.007 seconds
[0m10:16:43.535722 [debug] [Thread-20 ]: SQL status: BEGIN in 0.007 seconds
[0m10:16:43.535897 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.536080 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.536327 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m10:16:43.536596 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged
-- Results:

-- 87 duplicate pairs identified from high-confidence matches (score > 0.8)
-- Canonical ID logic: Uses the higher prospect_id as canonical (keeps the "later" record)
-- Merged ID logic: Lower prospect_id will be marked as duplicate
-- Range: Processing prospects from 658 to 9999
-- How it works:

-- Example: Prospect 658 is canonical, Prospect 291 should be merged into it
-- etc.



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m10:16:43.538706 [debug] [Thread-19 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:16:43.540077 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.540245 [debug] [Thread-20 ]: SQL status: SELECT 87 in 0.003 seconds
[0m10:16:43.540436 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m10:16:43.541782 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.541990 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters" rename to "stg_entity_clusters__dbt_backup"
[0m10:16:43.542478 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.542670 [debug] [Thread-19 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:16:43.543969 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.544457 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:16:43.544621 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m10:16:43.544782 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.544975 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:16:43.545210 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.545711 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:16:43.545874 [debug] [Thread-19 ]: SQL status: COMMIT in 0.001 seconds
[0m10:16:43.546021 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.546960 [debug] [Thread-19 ]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m10:16:43.547142 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:16:43.547407 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:16:43.547668 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m10:16:43.548076 [debug] [Thread-20 ]: SQL status: COMMIT in 0.000 seconds
[0m10:16:43.548264 [debug] [Thread-19 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:16:43.550266 [debug] [Thread-20 ]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m10:16:43.550893 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: Close
[0m10:16:43.551155 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:16:43.551377 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m10:16:43.551636 [debug] [Thread-19 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa1f990>]}
[0m10:16:43.551921 [info ] [Thread-19 ]: 15 of 17 OK created sql view model public_marts.prospect_matching_ratio ........ [[32mCREATE VIEW[0m in 0.03s]
[0m10:16:43.552153 [debug] [Thread-19 ]: Finished running node model.dbt_service.prospect_matching_ratio
[0m10:16:43.552753 [debug] [Thread-20 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:16:43.553292 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: Close
[0m10:16:43.553524 [debug] [Thread-20 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa578d0>]}
[0m10:16:43.553769 [info ] [Thread-20 ]: 16 of 17 OK created sql table model public_staging.stg_entity_clusters ......... [[32mSELECT 87[0m in 0.03s]
[0m10:16:43.553995 [debug] [Thread-20 ]: Finished running node model.dbt_service.stg_entity_clusters
[0m10:16:43.554291 [debug] [Thread-22 ]: Began running node model.dbt_service.data_overview
[0m10:16:43.554569 [info ] [Thread-22 ]: 17 of 17 START sql view model public_marts.data_overview ....................... [RUN]
[0m10:16:43.554787 [debug] [Thread-22 ]: Acquiring new postgres connection 'model.dbt_service.data_overview'
[0m10:16:43.554938 [debug] [Thread-22 ]: Began compiling node model.dbt_service.data_overview
[0m10:16:43.556568 [debug] [Thread-22 ]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m10:16:43.557049 [debug] [Thread-22 ]: Began executing node model.dbt_service.data_overview
[0m10:16:43.558258 [debug] [Thread-22 ]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m10:16:43.558539 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:16:43.558681 [debug] [Thread-22 ]: On model.dbt_service.data_overview: BEGIN
[0m10:16:43.558812 [debug] [Thread-22 ]: Opening a new connection, currently in state init
[0m10:16:43.564192 [debug] [Thread-22 ]: SQL status: BEGIN in 0.005 seconds
[0m10:16:43.564380 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:16:43.564561 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m10:16:43.565713 [debug] [Thread-22 ]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:16:43.566891 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:16:43.567039 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m10:16:43.567471 [debug] [Thread-22 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:16:43.567907 [debug] [Thread-22 ]: On model.dbt_service.data_overview: COMMIT
[0m10:16:43.568040 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:16:43.568163 [debug] [Thread-22 ]: On model.dbt_service.data_overview: COMMIT
[0m10:16:43.568728 [debug] [Thread-22 ]: SQL status: COMMIT in 0.000 seconds
[0m10:16:43.569705 [debug] [Thread-22 ]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m10:16:43.569924 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:16:43.570071 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m10:16:43.570471 [debug] [Thread-22 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:16:43.571134 [debug] [Thread-22 ]: On model.dbt_service.data_overview: Close
[0m10:16:43.571406 [debug] [Thread-22 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a649731-4d9c-4cf6-b5ae-a15671af7022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa78a10>]}
[0m10:16:43.571681 [info ] [Thread-22 ]: 17 of 17 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.02s]
[0m10:16:43.571917 [debug] [Thread-22 ]: Finished running node model.dbt_service.data_overview
[0m10:16:43.572976 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:43.573142 [debug] [MainThread]: On master: BEGIN
[0m10:16:43.573254 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:16:43.578711 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m10:16:43.578884 [debug] [MainThread]: On master: COMMIT
[0m10:16:43.579007 [debug] [MainThread]: Using postgres connection "master"
[0m10:16:43.579121 [debug] [MainThread]: On master: COMMIT
[0m10:16:43.579486 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:16:43.579645 [debug] [MainThread]: On master: Close
[0m10:16:43.579827 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:16:43.579945 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m10:16:43.580054 [debug] [MainThread]: Connection 'model.dbt_service.raw_pdl_data' was properly closed.
[0m10:16:43.580150 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m10:16:43.580245 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m10:16:43.580345 [debug] [MainThread]: Connection 'model.dbt_service.raw_company_profiling' was properly closed.
[0m10:16:43.580438 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m10:16:43.580545 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m10:16:43.580646 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m10:16:43.580740 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m10:16:43.580829 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m10:16:43.580925 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m10:16:43.581013 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m10:16:43.581104 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m10:16:43.581193 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m10:16:43.581285 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m10:16:43.581375 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m10:16:43.581463 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m10:16:43.581551 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m10:16:43.581726 [info ] [MainThread]: 
[0m10:16:43.581858 [info ] [MainThread]: Finished running 1 incremental model, 12 table models, 4 view models in 0 hours 0 minutes and 1.03 seconds (1.03s).
[0m10:16:43.582749 [debug] [MainThread]: Command end result
[0m10:16:43.595497 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:16:43.596551 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:16:43.599730 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:16:43.599886 [info ] [MainThread]: 
[0m10:16:43.600042 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:16:43.600172 [info ] [MainThread]: 
[0m10:16:43.600300 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=17
[0m10:16:43.602672 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.4389035, "process_in_blocks": "0", "process_kernel_time": 0.25441, "process_mem_max_rss": "139526144", "process_out_blocks": "0", "process_user_time": 1.456241}
[0m10:16:43.603043 [debug] [MainThread]: Command `dbt run` succeeded at 10:16:43.602997 after 1.44 seconds
[0m10:16:43.603260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d90190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ff91d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dcf690>]}
[0m10:16:43.603460 [debug] [MainThread]: Flushing usage events
[0m10:16:43.957887 [debug] [MainThread]: An error was encountered while trying to flush usage events
==================== 2025-11-09 15:17:11.356624 | 019a6931-522d-76d3-8e83-617998ac8bc6 ====================
==================== 2025-11-09 15:17:24.984281 | 019a6931-8777-7040-8e07-a5f7a79fa16c ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
15:17:25.457582 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [14.1s]
==================== 2025-11-09 15:18:08.020378 | 019a6932-2f88-7130-aaa7-2618b757b818 ====================
==================== 2025-11-09 15:18:16.029427 | 019a6932-4edc-7c81-95c9-70e7bd192571 ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
15:18:16.500560 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [8.5s]
[0m10:18:35.806502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109113390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109186210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10918ff10>]}


============================== 10:18:35.808696 | 0e51820c-d414-4154-bf5b-53e60240bf4d ==============================
[0m10:18:35.808696 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:18:35.808978 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'cache_selected_only': 'False', 'empty': 'False', 'fail_fast': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'printer_width': '80', 'warn_error': 'None', 'version_check': 'True', 'log_format': 'default', 'target_path': 'None', 'quiet': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_cache_events': 'False', 'static_parser': 'True', 'write_json': 'True'}
[0m10:18:35.903641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091863d0>]}
[0m10:18:35.933101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067f5290>]}
[0m10:18:35.933936 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:18:35.979317 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:18:36.040825 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:18:36.041011 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:18:36.063581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a09d690>]}
[0m10:18:36.102327 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:18:36.103380 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:18:36.114006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a307e10>]}
[0m10:18:36.114254 [info ] [MainThread]: Found 17 models, 3 seeds, 11 data tests, 2 sources, 450 macros
[0m10:18:36.114416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a09d5d0>]}
[0m10:18:36.115431 [info ] [MainThread]: 
[0m10:18:36.115589 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:18:36.115706 [info ] [MainThread]: 
[0m10:18:36.115923 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:18:36.117866 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:18:36.118111 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:18:36.119018 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:18:36.123416 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:18:36.124391 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:18:36.169153 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:18:36.169343 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:18:36.169498 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:18:36.169671 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:18:36.169812 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:18:36.169944 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:18:36.170094 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:18:36.170237 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:18:36.170372 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:18:36.170505 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:18:36.170641 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.170766 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.170891 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.171006 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.171121 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.211635 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.041 seconds
[0m10:18:36.211884 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.041 seconds
[0m10:18:36.212082 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.041 seconds
[0m10:18:36.212239 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.041 seconds
[0m10:18:36.212797 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:18:36.212958 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.042 seconds
[0m10:18:36.213404 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:18:36.213813 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:18:36.214223 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:18:36.214787 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:18:36.216010 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m10:18:36.218343 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:18:36.218584 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m10:18:36.218695 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:18:36.218871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m10:18:36.219068 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m10:18:36.219775 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:18:36.219952 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m10:18:36.220143 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m10:18:36.220298 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:18:36.220981 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:18:36.221640 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:18:36.221757 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:18:36.222388 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:18:36.222996 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:18:36.223197 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:18:36.223338 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:18:36.223457 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:18:36.223582 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:18:36.223703 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:18:36.223835 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:18:36.223980 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:18:36.224173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:18:36.224294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:36.230972 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m10:18:36.231103 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:18:36.231238 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:18:36.234182 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m10:18:36.234651 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:18:36.234809 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m10:18:36.234986 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:18:36.235129 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:18:36.235298 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:18:36.236863 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.002 seconds
[0m10:18:36.237611 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:18:36.237828 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:18:36.238075 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:18:36.238251 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:18:36.238497 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:18:36.238688 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:18:36.238859 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:18:36.239039 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:18:36.239201 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:18:36.239430 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:18:36.239592 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:18:36.240242 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:18:36.240414 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:18:36.240679 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:18:36.241817 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.001 seconds
[0m10:18:36.242399 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:18:36.242558 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m10:18:36.242704 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:18:36.242880 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m10:18:36.243628 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:18:36.243793 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:18:36.244321 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:18:36.244757 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:18:36.245347 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:18:36.246004 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:18:36.246138 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:18:36.249133 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:36.249319 [debug] [MainThread]: On master: BEGIN
[0m10:18:36.249427 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:18:36.255521 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:18:36.255707 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:36.255892 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:18:36.257790 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:18:36.258936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5e3850>]}
[0m10:18:36.259128 [debug] [MainThread]: On master: ROLLBACK
[0m10:18:36.259559 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:36.259792 [debug] [MainThread]: On master: BEGIN
[0m10:18:36.260295 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:18:36.260422 [debug] [MainThread]: On master: COMMIT
[0m10:18:36.260545 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:36.260655 [debug] [MainThread]: On master: COMMIT
[0m10:18:36.260959 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:18:36.261194 [debug] [MainThread]: On master: Close
[0m10:18:36.263678 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m10:18:36.263893 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m10:18:36.264192 [info ] [Thread-1 (]: 1 of 17 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m10:18:36.264456 [info ] [Thread-2 (]: 2 of 17 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m10:18:36.264737 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_fxf_data)
[0m10:18:36.264941 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_pdl_data)
[0m10:18:36.265123 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m10:18:36.265282 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m10:18:36.269576 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m10:18:36.271232 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m10:18:36.271674 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m10:18:36.271855 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m10:18:36.288745 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m10:18:36.290529 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m10:18:36.291062 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.291250 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.291459 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m10:18:36.291645 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m10:18:36.291820 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:18:36.292072 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:18:36.299054 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m10:18:36.299290 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m10:18:36.299570 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.299785 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.299957 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m10:18:36.300139 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m10:18:36.328524 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.028 seconds
[0m10:18:36.332669 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.332917 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.033 seconds
[0m10:18:36.333233 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m10:18:36.335127 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.335379 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m10:18:36.335787 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.336007 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.337089 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.338079 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.338238 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m10:18:36.338387 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m10:18:36.338888 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.339077 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:36.346156 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:18:36.346677 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:18:36.346873 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.347034 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.347247 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:18:36.347399 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:18:36.373377 [debug] [Thread-2 (]: SQL status: COMMIT in 0.026 seconds
[0m10:18:36.373591 [debug] [Thread-1 (]: SQL status: COMMIT in 0.026 seconds
[0m10:18:36.377321 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m10:18:36.378371 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m10:18:36.380555 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:18:36.380820 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:18:36.380989 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m10:18:36.381157 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m10:18:36.383041 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:18:36.383204 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:18:36.384204 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m10:18:36.384722 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m10:18:36.385802 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097df950>]}
[0m10:18:36.385981 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6fdf90>]}
[0m10:18:36.386307 [info ] [Thread-1 (]: 1 of 17 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.12s]
[0m10:18:36.386604 [info ] [Thread-2 (]: 2 of 17 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.12s]
[0m10:18:36.386863 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m10:18:36.387068 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m10:18:36.387354 [debug] [Thread-2 (]: Began running node model.dbt_service.stg_fxf_data
[0m10:18:36.387569 [info ] [Thread-2 (]: 3 of 17 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m10:18:36.387801 [debug] [Thread-5 (]: Began running node model.dbt_service.raw_company_profiling
[0m10:18:36.387961 [debug] [Thread-6 (]: Began running node model.dbt_service.raw_data_profiling
[0m10:18:36.388138 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_service.raw_pdl_data, now model.dbt_service.stg_fxf_data)
[0m10:18:36.388287 [debug] [Thread-7 (]: Began running node model.dbt_service.raw_location_profiling
[0m10:18:36.388429 [debug] [Thread-8 (]: Began running node model.dbt_service.stg_pdl_data
[0m10:18:36.388629 [info ] [Thread-5 (]: 4 of 17 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m10:18:36.388843 [info ] [Thread-6 (]: 5 of 17 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m10:18:36.389024 [debug] [Thread-2 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m10:18:36.389228 [info ] [Thread-7 (]: 6 of 17 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m10:18:36.389421 [info ] [Thread-8 (]: 7 of 17 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m10:18:36.389612 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_company_profiling)
[0m10:18:36.389792 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_data_profiling)
[0m10:18:36.391246 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m10:18:36.391473 [debug] [Thread-7 (]: Acquiring new postgres connection 'model.dbt_service.raw_location_profiling'
[0m10:18:36.391666 [debug] [Thread-8 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m10:18:36.391824 [debug] [Thread-5 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m10:18:36.391982 [debug] [Thread-6 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m10:18:36.392249 [debug] [Thread-7 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m10:18:36.392413 [debug] [Thread-8 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m10:18:36.393916 [debug] [Thread-5 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m10:18:36.394103 [debug] [Thread-2 (]: Began executing node model.dbt_service.stg_fxf_data
[0m10:18:36.395535 [debug] [Thread-6 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m10:18:36.397309 [debug] [Thread-7 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m10:18:36.398604 [debug] [Thread-8 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m10:18:36.405803 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m10:18:36.406150 [debug] [Thread-5 (]: Began executing node model.dbt_service.raw_company_profiling
[0m10:18:36.407610 [debug] [Thread-5 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m10:18:36.407778 [debug] [Thread-8 (]: Began executing node model.dbt_service.stg_pdl_data
[0m10:18:36.407927 [debug] [Thread-6 (]: Began executing node model.dbt_service.raw_data_profiling
[0m10:18:36.408073 [debug] [Thread-7 (]: Began executing node model.dbt_service.raw_location_profiling
[0m10:18:36.408288 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:18:36.410259 [debug] [Thread-8 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m10:18:36.411473 [debug] [Thread-6 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m10:18:36.411660 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.412780 [debug] [Thread-7 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m10:18:36.412950 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m10:18:36.413192 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m10:18:36.413397 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:18:36.413562 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:18:36.413716 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.413932 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m10:18:36.414091 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.414233 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m10:18:36.414449 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m10:18:36.414653 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m10:18:36.414793 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m10:18:36.414933 [debug] [Thread-6 (]: Opening a new connection, currently in state closed
[0m10:18:36.415080 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m10:18:36.425495 [debug] [Thread-5 (]: SQL status: BEGIN in 0.012 seconds
[0m10:18:36.425728 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.426044 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:18:36.426349 [debug] [Thread-2 (]: SQL status: BEGIN in 0.013 seconds
[0m10:18:36.426592 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:18:36.426890 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m10:18:36.427237 [debug] [Thread-6 (]: SQL status: BEGIN in 0.012 seconds
[0m10:18:36.427496 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.427723 [debug] [Thread-7 (]: SQL status: BEGIN in 0.013 seconds
[0m10:18:36.428003 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m10:18:36.428233 [debug] [Thread-8 (]: SQL status: BEGIN in 0.013 seconds
[0m10:18:36.428443 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:18:36.428659 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.428914 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:18:36.430286 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:18:36.430613 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m10:18:36.431027 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m10:18:36.431223 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m10:18:36.431806 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.432433 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:18:36.432623 [debug] [Thread-8 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:18:36.432817 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:18:36.434230 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:18:36.434415 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:18:36.434572 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m10:18:36.435164 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.435345 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.435888 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:18:36.437103 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m10:18:36.437317 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:18:36.438567 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:18:36.438740 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:18:36.438904 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m10:18:36.439484 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:18:36.440061 [debug] [Thread-2 (]: On model.dbt_service.stg_fxf_data: Close
[0m10:18:36.440241 [debug] [Thread-8 (]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.440570 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5a8f10>]}
[0m10:18:36.441643 [debug] [Thread-8 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m10:18:36.441992 [info ] [Thread-2 (]: 3 of 17 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m10:18:36.442311 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:18:36.442564 [debug] [Thread-2 (]: Finished running node model.dbt_service.stg_fxf_data
[0m10:18:36.442722 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m10:18:36.443284 [debug] [Thread-8 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:18:36.443887 [debug] [Thread-8 (]: On model.dbt_service.stg_pdl_data: Close
[0m10:18:36.444148 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6f6250>]}
[0m10:18:36.444431 [info ] [Thread-8 (]: 7 of 17 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m10:18:36.444679 [debug] [Thread-8 (]: Finished running node model.dbt_service.stg_pdl_data
[0m10:18:36.445049 [debug] [Thread-10 ]: Began running node model.dbt_service.company_analysis
[0m10:18:36.445212 [debug] [Thread-11 ]: Began running node model.dbt_service.staging_city_state_profiling
[0m10:18:36.445390 [debug] [Thread-12 ]: Began running node model.dbt_service.staging_company_profiling
[0m10:18:36.445707 [debug] [Thread-13 ]: Began running node model.dbt_service.staging_data_profiling
[0m10:18:36.445587 [info ] [Thread-10 ]: 8 of 17 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m10:18:36.445885 [debug] [Thread-14 ]: Began running node model.dbt_service.staging_location_profiling
[0m10:18:36.446028 [debug] [Thread-15 ]: Began running node model.dbt_service.stg_unified_prospects
[0m10:18:36.446215 [info ] [Thread-11 ]: 9 of 17 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m10:18:36.446415 [info ] [Thread-12 ]: 10 of 17 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m10:18:36.446619 [info ] [Thread-13 ]: 11 of 17 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m10:18:36.446841 [debug] [Thread-10 ]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m10:18:36.447025 [info ] [Thread-14 ]: 12 of 17 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m10:18:36.447277 [info ] [Thread-15 ]: 13 of 17 START sql table model public_staging.stg_unified_prospects ............ [RUN]
[0m10:18:36.447587 [debug] [Thread-11 ]: Acquiring new postgres connection 'model.dbt_service.staging_city_state_profiling'
[0m10:18:36.447894 [debug] [Thread-12 ]: Acquiring new postgres connection 'model.dbt_service.staging_company_profiling'
[0m10:18:36.448095 [debug] [Thread-13 ]: Acquiring new postgres connection 'model.dbt_service.staging_data_profiling'
[0m10:18:36.448258 [debug] [Thread-10 ]: Began compiling node model.dbt_service.company_analysis
[0m10:18:36.448698 [debug] [Thread-14 ]: Acquiring new postgres connection 'model.dbt_service.staging_location_profiling'
[0m10:18:36.449211 [debug] [Thread-15 ]: Acquiring new postgres connection 'model.dbt_service.stg_unified_prospects'
[0m10:18:36.449458 [debug] [Thread-11 ]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m10:18:36.449657 [debug] [Thread-12 ]: Began compiling node model.dbt_service.staging_company_profiling
[0m10:18:36.449818 [debug] [Thread-13 ]: Began compiling node model.dbt_service.staging_data_profiling
[0m10:18:36.451576 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m10:18:36.451790 [debug] [Thread-14 ]: Began compiling node model.dbt_service.staging_location_profiling
[0m10:18:36.451990 [debug] [Thread-15 ]: Began compiling node model.dbt_service.stg_unified_prospects
[0m10:18:36.453510 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.455401 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m10:18:36.457771 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m10:18:36.459412 [debug] [Thread-14 ]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m10:18:36.461297 [debug] [Thread-15 ]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m10:18:36.461644 [debug] [Thread-10 ]: Began executing node model.dbt_service.company_analysis
[0m10:18:36.462023 [debug] [Thread-11 ]: Began executing node model.dbt_service.staging_city_state_profiling
[0m10:18:36.462274 [debug] [Thread-12 ]: Began executing node model.dbt_service.staging_company_profiling
[0m10:18:36.462551 [debug] [Thread-13 ]: Began executing node model.dbt_service.staging_data_profiling
[0m10:18:36.464076 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m10:18:36.464272 [debug] [Thread-14 ]: Began executing node model.dbt_service.staging_location_profiling
[0m10:18:36.464433 [debug] [Thread-15 ]: Began executing node model.dbt_service.stg_unified_prospects
[0m10:18:36.465532 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.466723 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m10:18:36.467950 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m10:18:36.469420 [debug] [Thread-14 ]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m10:18:36.470821 [debug] [Thread-15 ]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m10:18:36.471015 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.471362 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.471550 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.471796 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.471967 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: BEGIN
[0m10:18:36.472142 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.472305 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m10:18:36.472469 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.472611 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: BEGIN
[0m10:18:36.472758 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: BEGIN
[0m10:18:36.472900 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m10:18:36.473051 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: BEGIN
[0m10:18:36.473195 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m10:18:36.473335 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m10:18:36.473469 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m10:18:36.473607 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m10:18:36.473822 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m10:18:36.474025 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m10:18:36.487206 [debug] [Thread-11 ]: SQL status: BEGIN in 0.014 seconds
[0m10:18:36.487565 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.487796 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m10:18:36.488695 [debug] [Thread-14 ]: SQL status: BEGIN in 0.015 seconds
[0m10:18:36.488900 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.489158 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m10:18:36.489539 [debug] [Thread-12 ]: SQL status: BEGIN in 0.016 seconds
[0m10:18:36.489735 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.490007 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:18:36.490513 [debug] [Thread-10 ]: SQL status: BEGIN in 0.018 seconds
[0m10:18:36.490659 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.490869 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m10:18:36.491211 [debug] [Thread-15 ]: SQL status: BEGIN in 0.017 seconds
[0m10:18:36.491603 [debug] [Thread-13 ]: SQL status: BEGIN in 0.018 seconds
[0m10:18:36.491951 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.492145 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.492355 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m10:18:36.492655 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m10:18:36.524569 [debug] [Thread-5 (]: SQL status: SELECT 180 in 0.098 seconds
[0m10:18:36.527209 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.527420 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m10:18:36.528400 [debug] [Thread-5 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:36.532767 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.532977 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m10:18:36.533861 [debug] [Thread-5 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:36.534842 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:18:36.535040 [debug] [Thread-6 (]: SQL status: SELECT 2 in 0.106 seconds
[0m10:18:36.535217 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.536443 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.536600 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:18:36.536749 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m10:18:36.537757 [debug] [Thread-5 (]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.537969 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:36.539012 [debug] [Thread-5 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m10:18:36.540155 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.540395 [debug] [Thread-5 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:18:36.540743 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m10:18:36.541155 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m10:18:36.542022 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.542792 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:18:36.542964 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.543102 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:18:36.543258 [debug] [Thread-5 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:18:36.544226 [debug] [Thread-5 (]: On model.dbt_service.raw_company_profiling: Close
[0m10:18:36.544537 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1e6510>]}
[0m10:18:36.544726 [debug] [Thread-6 (]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.545364 [info ] [Thread-5 (]: 4 of 17 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.15s]
[0m10:18:36.547917 [debug] [Thread-6 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m10:18:36.548432 [debug] [Thread-5 (]: Finished running node model.dbt_service.raw_company_profiling
[0m10:18:36.548912 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:18:36.549360 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m10:18:36.551041 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.551644 [debug] [Thread-6 (]: On model.dbt_service.raw_data_profiling: Close
[0m10:18:36.552123 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7459d0>]}
[0m10:18:36.552847 [info ] [Thread-6 (]: 5 of 17 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.16s]
[0m10:18:36.553371 [debug] [Thread-6 (]: Finished running node model.dbt_service.raw_data_profiling
[0m10:18:36.602136 [debug] [Thread-7 (]: SQL status: SELECT 152 in 0.171 seconds
[0m10:18:36.603717 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.603896 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m10:18:36.604555 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:36.605670 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.605823 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m10:18:36.606290 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.606773 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:18:36.606922 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.607058 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:18:36.608109 [debug] [Thread-7 (]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.609125 [debug] [Thread-7 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m10:18:36.609397 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:18:36.609551 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m10:18:36.611074 [debug] [Thread-7 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.611657 [debug] [Thread-7 (]: On model.dbt_service.raw_location_profiling: Close
[0m10:18:36.611917 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a716f50>]}
[0m10:18:36.612180 [info ] [Thread-7 (]: 6 of 17 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.22s]
[0m10:18:36.612516 [debug] [Thread-7 (]: Finished running node model.dbt_service.raw_location_profiling
[0m10:18:36.627000 [debug] [Thread-13 ]: SQL status: SELECT 2 in 0.134 seconds
[0m10:18:36.628537 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.628717 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m10:18:36.629297 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.630399 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.630552 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m10:18:36.630979 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.631483 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:18:36.631634 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.631771 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:18:36.632329 [debug] [Thread-13 ]: SQL status: COMMIT in 0.000 seconds
[0m10:18:36.634089 [debug] [Thread-13 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m10:18:36.634315 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:18:36.634481 [debug] [Thread-11 ]: SQL status: SELECT 76 in 0.146 seconds
[0m10:18:36.634637 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m10:18:36.635846 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.636044 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m10:18:36.636479 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.637517 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.637680 [debug] [Thread-13 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:18:36.637846 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m10:18:36.638376 [debug] [Thread-13 ]: On model.dbt_service.staging_data_profiling: Close
[0m10:18:36.638662 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11413c6d0>]}
[0m10:18:36.638856 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.639522 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:18:36.639065 [info ] [Thread-13 ]: 11 of 17 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.19s]
[0m10:18:36.639724 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.639929 [debug] [Thread-13 ]: Finished running node model.dbt_service.staging_data_profiling
[0m10:18:36.640077 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:18:36.640662 [debug] [Thread-11 ]: SQL status: COMMIT in 0.000 seconds
[0m10:18:36.641600 [debug] [Thread-11 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m10:18:36.641821 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:18:36.641971 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m10:18:36.642878 [debug] [Thread-11 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.643413 [debug] [Thread-11 ]: On model.dbt_service.staging_city_state_profiling: Close
[0m10:18:36.643641 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11413c790>]}
[0m10:18:36.643893 [info ] [Thread-11 ]: 9 of 17 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.20s]
[0m10:18:36.644123 [debug] [Thread-11 ]: Finished running node model.dbt_service.staging_city_state_profiling
[0m10:18:36.671002 [debug] [Thread-12 ]: SQL status: SELECT 180 in 0.181 seconds
[0m10:18:36.672484 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.672665 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m10:18:36.673175 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.674246 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.674398 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m10:18:36.674848 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.675277 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:18:36.675423 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.675557 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:18:36.676163 [debug] [Thread-12 ]: SQL status: COMMIT in 0.000 seconds
[0m10:18:36.677059 [debug] [Thread-12 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m10:18:36.677278 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:18:36.677423 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m10:18:36.678334 [debug] [Thread-12 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.678783 [debug] [Thread-12 ]: On model.dbt_service.staging_company_profiling: Close
[0m10:18:36.679018 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114100e50>]}
[0m10:18:36.679273 [info ] [Thread-12 ]: 10 of 17 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.23s]
[0m10:18:36.679503 [debug] [Thread-12 ]: Finished running node model.dbt_service.staging_company_profiling
[0m10:18:36.746120 [debug] [Thread-10 ]: SQL status: SELECT 91 in 0.255 seconds
[0m10:18:36.747686 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.747879 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m10:18:36.748434 [debug] [Thread-10 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.749520 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.749670 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m10:18:36.750126 [debug] [Thread-10 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.750585 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:18:36.750727 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.750858 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:18:36.751559 [debug] [Thread-10 ]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.752471 [debug] [Thread-10 ]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m10:18:36.752692 [debug] [Thread-10 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:18:36.752843 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m10:18:36.753878 [debug] [Thread-10 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.754440 [debug] [Thread-10 ]: On model.dbt_service.company_analysis: Close
[0m10:18:36.754697 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ec4f90>]}
[0m10:18:36.754967 [info ] [Thread-10 ]: 8 of 17 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.31s]
[0m10:18:36.755203 [debug] [Thread-10 ]: Finished running node model.dbt_service.company_analysis
[0m10:18:36.769557 [debug] [Thread-14 ]: SQL status: SELECT 152 in 0.280 seconds
[0m10:18:36.770920 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.771081 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m10:18:36.771686 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.772742 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.772887 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m10:18:36.773312 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.773743 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:18:36.773883 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.774015 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:18:36.774700 [debug] [Thread-14 ]: SQL status: COMMIT in 0.000 seconds
[0m10:18:36.776377 [debug] [Thread-14 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m10:18:36.776569 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:18:36.776703 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m10:18:36.777613 [debug] [Thread-14 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:36.778052 [debug] [Thread-14 ]: On model.dbt_service.staging_location_profiling: Close
[0m10:18:36.778275 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114106210>]}
[0m10:18:36.778523 [info ] [Thread-14 ]: 12 of 17 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.33s]
[0m10:18:36.778749 [debug] [Thread-14 ]: Finished running node model.dbt_service.staging_location_profiling
[0m10:18:36.856170 [debug] [Thread-15 ]: SQL status: SELECT 100010 in 0.363 seconds
[0m10:18:36.862283 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.862877 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m10:18:36.863830 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.867022 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.867453 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m10:18:36.868094 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:36.869183 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:18:36.869512 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.869798 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:18:36.870975 [debug] [Thread-15 ]: SQL status: COMMIT in 0.001 seconds
[0m10:18:36.873303 [debug] [Thread-15 ]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m10:18:36.873792 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:18:36.874107 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m10:18:36.876474 [debug] [Thread-15 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:18:36.877479 [debug] [Thread-15 ]: On model.dbt_service.stg_unified_prospects: Close
[0m10:18:36.877985 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e4c990>]}
[0m10:18:36.878523 [info ] [Thread-15 ]: 13 of 17 OK created sql table model public_staging.stg_unified_prospects ....... [[32mSELECT 100010[0m in 0.43s]
[0m10:18:36.879086 [debug] [Thread-15 ]: Finished running node model.dbt_service.stg_unified_prospects
[0m10:18:36.879659 [debug] [Thread-17 ]: Began running node model.dbt_service.stg_prospect_matches
[0m10:18:36.880136 [info ] [Thread-17 ]: 14 of 17 START sql incremental model public_staging.stg_prospect_matches ....... [RUN]
[0m10:18:36.880405 [debug] [Thread-17 ]: Acquiring new postgres connection 'model.dbt_service.stg_prospect_matches'
[0m10:18:36.880727 [debug] [Thread-17 ]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:18:36.888347 [debug] [Thread-17 ]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:18:36.889065 [debug] [Thread-17 ]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:18:36.906414 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.906782 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp101836903528"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.5   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m10:18:36.906961 [debug] [Thread-17 ]: Opening a new connection, currently in state init
[0m10:18:36.958515 [debug] [Thread-17 ]: SQL status: SELECT 0 in 0.051 seconds
[0m10:18:36.966931 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.967132 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:18:36.967657 [debug] [Thread-17 ]: SQL status: BEGIN in 0.000 seconds
[0m10:18:36.967827 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.968053 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp101836903528'
        
      order by ordinal_position

  
[0m10:18:36.971085 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.003 seconds
[0m10:18:36.973242 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.973432 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:18:36.974937 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:18:36.981853 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.982166 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp101836903528'
        
      order by ordinal_position

  
[0m10:18:36.983871 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:18:36.987086 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:36.987376 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:18:36.988778 [debug] [Thread-17 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:18:36.993973 [debug] [Thread-17 ]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:18:37.001991 [debug] [Thread-17 ]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:18:37.002520 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:37.002690 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp101836903528" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp101836903528"
    )
  
[0m10:18:37.003567 [debug] [Thread-17 ]: SQL status: INSERT 0 0 in 0.001 seconds
[0m10:18:37.004106 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:18:37.004276 [debug] [Thread-17 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:18:37.004420 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:18:37.005200 [debug] [Thread-17 ]: SQL status: COMMIT in 0.001 seconds
[0m10:18:37.005497 [debug] [Thread-17 ]: On model.dbt_service.stg_prospect_matches: Close
[0m10:18:37.005748 [debug] [Thread-17 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5c5810>]}
[0m10:18:37.006028 [info ] [Thread-17 ]: 14 of 17 OK created sql incremental model public_staging.stg_prospect_matches .. [[32mINSERT 0 0[0m in 0.13s]
[0m10:18:37.006260 [debug] [Thread-17 ]: Finished running node model.dbt_service.stg_prospect_matches
[0m10:18:37.006580 [debug] [Thread-19 ]: Began running node model.dbt_service.prospect_matching_ratio
[0m10:18:37.006754 [debug] [Thread-20 ]: Began running node model.dbt_service.stg_entity_clusters
[0m10:18:37.007009 [info ] [Thread-19 ]: 15 of 17 START sql view model public_marts.prospect_matching_ratio ............. [RUN]
[0m10:18:37.007269 [info ] [Thread-20 ]: 16 of 17 START sql table model public_staging.stg_entity_clusters .............. [RUN]
[0m10:18:37.007526 [debug] [Thread-19 ]: Acquiring new postgres connection 'model.dbt_service.prospect_matching_ratio'
[0m10:18:37.007737 [debug] [Thread-20 ]: Acquiring new postgres connection 'model.dbt_service.stg_entity_clusters'
[0m10:18:37.007894 [debug] [Thread-19 ]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m10:18:37.008042 [debug] [Thread-20 ]: Began compiling node model.dbt_service.stg_entity_clusters
[0m10:18:37.009693 [debug] [Thread-19 ]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.010856 [debug] [Thread-20 ]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m10:18:37.011177 [debug] [Thread-20 ]: Began executing node model.dbt_service.stg_entity_clusters
[0m10:18:37.011328 [debug] [Thread-19 ]: Began executing node model.dbt_service.prospect_matching_ratio
[0m10:18:37.012682 [debug] [Thread-20 ]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m10:18:37.013894 [debug] [Thread-19 ]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.014217 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.014383 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.014542 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m10:18:37.014689 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m10:18:37.014833 [debug] [Thread-20 ]: Opening a new connection, currently in state init
[0m10:18:37.014967 [debug] [Thread-19 ]: Opening a new connection, currently in state init
[0m10:18:37.021017 [debug] [Thread-19 ]: SQL status: BEGIN in 0.006 seconds
[0m10:18:37.021196 [debug] [Thread-20 ]: SQL status: BEGIN in 0.006 seconds
[0m10:18:37.021373 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.021533 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.021751 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m10:18:37.022003 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged
-- Results:

-- 87 duplicate pairs identified from high-confidence matches (score > 0.8)
-- Canonical ID logic: Uses the higher prospect_id as canonical (keeps the "later" record)
-- Merged ID logic: Lower prospect_id will be marked as duplicate
-- Range: Processing prospects from 658 to 9999
-- How it works:

-- Example: Prospect 658 is canonical, Prospect 291 should be merged into it
-- etc.



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m10:18:37.023765 [debug] [Thread-19 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:18:37.025075 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.025278 [debug] [Thread-20 ]: SQL status: SELECT 87 in 0.003 seconds
[0m10:18:37.025562 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m10:18:37.027284 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.027501 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters" rename to "stg_entity_clusters__dbt_backup"
[0m10:18:37.027921 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:37.028067 [debug] [Thread-19 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:18:37.029311 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.029807 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:18:37.029990 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m10:18:37.030165 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.030448 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:18:37.030718 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:37.031307 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:18:37.031475 [debug] [Thread-19 ]: SQL status: COMMIT in 0.001 seconds
[0m10:18:37.031629 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.032685 [debug] [Thread-19 ]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m10:18:37.032841 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:18:37.033091 [debug] [Thread-19 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:18:37.033292 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m10:18:37.033661 [debug] [Thread-19 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:18:37.033799 [debug] [Thread-20 ]: SQL status: COMMIT in 0.001 seconds
[0m10:18:37.034343 [debug] [Thread-19 ]: On model.dbt_service.prospect_matching_ratio: Close
[0m10:18:37.036306 [debug] [Thread-20 ]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m10:18:37.036626 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:18:37.036919 [debug] [Thread-19 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114190ed0>]}
[0m10:18:37.037100 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m10:18:37.037398 [info ] [Thread-19 ]: 15 of 17 OK created sql view model public_marts.prospect_matching_ratio ........ [[32mCREATE VIEW[0m in 0.03s]
[0m10:18:37.037751 [debug] [Thread-19 ]: Finished running node model.dbt_service.prospect_matching_ratio
[0m10:18:37.038511 [debug] [Thread-20 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:18:37.039062 [debug] [Thread-20 ]: On model.dbt_service.stg_entity_clusters: Close
[0m10:18:37.039288 [debug] [Thread-20 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114192790>]}
[0m10:18:37.039549 [info ] [Thread-20 ]: 16 of 17 OK created sql table model public_staging.stg_entity_clusters ......... [[32mSELECT 87[0m in 0.03s]
[0m10:18:37.039787 [debug] [Thread-20 ]: Finished running node model.dbt_service.stg_entity_clusters
[0m10:18:37.040092 [debug] [Thread-22 ]: Began running node model.dbt_service.data_overview
[0m10:18:37.040336 [info ] [Thread-22 ]: 17 of 17 START sql view model public_marts.data_overview ....................... [RUN]
[0m10:18:37.040546 [debug] [Thread-22 ]: Acquiring new postgres connection 'model.dbt_service.data_overview'
[0m10:18:37.040692 [debug] [Thread-22 ]: Began compiling node model.dbt_service.data_overview
[0m10:18:37.042380 [debug] [Thread-22 ]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m10:18:37.042737 [debug] [Thread-22 ]: Began executing node model.dbt_service.data_overview
[0m10:18:37.044061 [debug] [Thread-22 ]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m10:18:37.044411 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:18:37.044641 [debug] [Thread-22 ]: On model.dbt_service.data_overview: BEGIN
[0m10:18:37.044787 [debug] [Thread-22 ]: Opening a new connection, currently in state init
[0m10:18:37.050330 [debug] [Thread-22 ]: SQL status: BEGIN in 0.006 seconds
[0m10:18:37.050524 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:18:37.050714 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m10:18:37.051874 [debug] [Thread-22 ]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:18:37.053110 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:18:37.053263 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m10:18:37.053736 [debug] [Thread-22 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:18:37.054226 [debug] [Thread-22 ]: On model.dbt_service.data_overview: COMMIT
[0m10:18:37.054393 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:18:37.054544 [debug] [Thread-22 ]: On model.dbt_service.data_overview: COMMIT
[0m10:18:37.055146 [debug] [Thread-22 ]: SQL status: COMMIT in 0.000 seconds
[0m10:18:37.056087 [debug] [Thread-22 ]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m10:18:37.056313 [debug] [Thread-22 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:18:37.056463 [debug] [Thread-22 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m10:18:37.056819 [debug] [Thread-22 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:18:37.057377 [debug] [Thread-22 ]: On model.dbt_service.data_overview: Close
[0m10:18:37.057651 [debug] [Thread-22 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e51820c-d414-4154-bf5b-53e60240bf4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141921d0>]}
[0m10:18:37.057924 [info ] [Thread-22 ]: 17 of 17 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.02s]
[0m10:18:37.058150 [debug] [Thread-22 ]: Finished running node model.dbt_service.data_overview
[0m10:18:37.059329 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:37.059572 [debug] [MainThread]: On master: BEGIN
[0m10:18:37.059710 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:18:37.065211 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m10:18:37.065404 [debug] [MainThread]: On master: COMMIT
[0m10:18:37.065545 [debug] [MainThread]: Using postgres connection "master"
[0m10:18:37.065679 [debug] [MainThread]: On master: COMMIT
[0m10:18:37.065961 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:18:37.066135 [debug] [MainThread]: On master: Close
[0m10:18:37.066299 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:18:37.066417 [debug] [MainThread]: Connection 'model.dbt_service.raw_fxf_data' was properly closed.
[0m10:18:37.066535 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m10:18:37.066628 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m10:18:37.066718 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m10:18:37.066820 [debug] [MainThread]: Connection 'model.dbt_service.raw_company_profiling' was properly closed.
[0m10:18:37.066921 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m10:18:37.067022 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m10:18:37.067118 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m10:18:37.067206 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m10:18:37.067293 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m10:18:37.067381 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m10:18:37.067468 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m10:18:37.067560 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m10:18:37.067649 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m10:18:37.067740 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m10:18:37.067824 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m10:18:37.067912 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m10:18:37.067997 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m10:18:37.068166 [info ] [MainThread]: 
[0m10:18:37.068288 [info ] [MainThread]: Finished running 1 incremental model, 12 table models, 4 view models in 0 hours 0 minutes and 0.95 seconds (0.95s).
[0m10:18:37.069102 [debug] [MainThread]: Command end result
[0m10:18:37.080947 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:18:37.081712 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:18:37.084776 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:18:37.085064 [info ] [MainThread]: 
[0m10:18:37.085256 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:18:37.085402 [info ] [MainThread]: 
[0m10:18:37.085546 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=17
[0m10:18:37.088411 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.3154807, "process_in_blocks": "0", "process_kernel_time": 0.230715, "process_mem_max_rss": "140083200", "process_out_blocks": "0", "process_user_time": 1.387342}
[0m10:18:37.088718 [debug] [MainThread]: Command `dbt run` succeeded at 10:18:37.088677 after 1.32 seconds
[0m10:18:37.088899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091be090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046fc250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047caa50>]}
[0m10:18:37.089068 [debug] [MainThread]: Flushing usage events
[0m10:18:37.519563 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:22:21.562332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109529390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10956fd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109578490>]}


============================== 10:22:21.564690 | 83d0b346-08a8-4d7d-901f-151535b6e0cf ==============================
[0m10:22:21.564690 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:22:21.564989 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'empty': 'False', 'partial_parse': 'True', 'no_print': 'None', 'quiet': 'False', 'use_colors': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select setup_functions', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'log_format': 'default', 'static_parser': 'True'}
[0m10:22:21.671649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4b7e90>]}
[0m10:22:21.701427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059f9790>]}
[0m10:22:21.701931 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:22:21.748041 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:22:21.810936 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 1 files changed.
[0m10:22:21.811220 [debug] [MainThread]: Partial parsing: added file: dbt_service://macros/sql_functions/create_merge_function.sql
[0m10:22:21.811354 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/hooks/setup_functions.sql
[0m10:22:21.811488 [debug] [MainThread]: Partial parsing: added file: dbt_service://models/hooks/schema.yml
[0m10:22:21.811627 [debug] [MainThread]: Partial parsing: updated file: dbt_service://macros/merge_entities.sql
[0m10:22:21.994365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10695c610>]}
[0m10:22:22.033439 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:22:22.034426 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:22:22.046052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c1789d0>]}
[0m10:22:22.046331 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:22:22.046502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c153590>]}
[0m10:22:22.047233 [info ] [MainThread]: 
[0m10:22:22.047377 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:22:22.047494 [info ] [MainThread]: 
[0m10:22:22.047697 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:22:22.048115 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:22:22.074736 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:22:22.075000 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:22:22.075140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.104976 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.030 seconds
[0m10:22:22.105612 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:22:22.107913 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m10:22:22.108166 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m10:22:22.108387 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m10:22:22.110917 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:22:22.111086 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m10:22:22.111330 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m10:22:22.111526 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m10:22:22.112268 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:22:22.113028 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:22:22.113155 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:22:22.113853 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:22:22.114550 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:22:22.115284 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:22:22.115428 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:22:22.115676 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:22:22.115869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:22:22.116026 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:22:22.116177 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:22:22.116304 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:22:22.116436 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.116557 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.116753 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.116880 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.117000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:22.130981 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:22:22.131131 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:22:22.131272 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:22:22.131611 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:22:22.131770 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:22:22.131917 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:22:22.133892 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m10:22:22.134593 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:22:22.134764 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m10:22:22.135006 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m10:22:22.135156 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m10:22:22.135299 [debug] [ThreadPool]: SQL status: BEGIN in 0.019 seconds
[0m10:22:22.135452 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m10:22:22.135651 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:22:22.135818 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:22:22.135970 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:22:22.136139 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:22:22.136274 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:22:22.137037 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:22:22.137203 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:22:22.137367 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:22:22.137576 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:22:22.138004 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:22:22.138474 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:22:22.139483 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m10:22:22.139651 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.001 seconds
[0m10:22:22.139818 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m10:22:22.139963 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m10:22:22.140545 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:22:22.141003 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:22:22.141423 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:22:22.141835 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:22:22.142314 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:22:22.142479 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:22:22.142672 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:22:22.142785 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:22:22.147287 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.147513 [debug] [MainThread]: On master: BEGIN
[0m10:22:22.147639 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:22:22.153709 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:22:22.153961 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.154189 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:22:22.156565 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:22:22.157683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c357810>]}
[0m10:22:22.157888 [debug] [MainThread]: On master: ROLLBACK
[0m10:22:22.158492 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.158797 [debug] [MainThread]: On master: BEGIN
[0m10:22:22.159440 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:22:22.159597 [debug] [MainThread]: On master: COMMIT
[0m10:22:22.159742 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.159873 [debug] [MainThread]: On master: COMMIT
[0m10:22:22.160235 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:22:22.160393 [debug] [MainThread]: On master: Close
[0m10:22:22.163241 [debug] [Thread-1 (]: Began running node model.dbt_service.setup_functions
[0m10:22:22.163553 [info ] [Thread-1 (]: 1 of 1 START sql view model public.setup_functions ............................. [RUN]
[0m10:22:22.163770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.setup_functions)
[0m10:22:22.163922 [debug] [Thread-1 (]: Began compiling node model.dbt_service.setup_functions
[0m10:22:22.167419 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.setup_functions"
[0m10:22:22.168239 [debug] [Thread-1 (]: Began executing node model.dbt_service.setup_functions
[0m10:22:22.184580 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.setup_functions"
[0m10:22:22.185320 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.185572 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: BEGIN
[0m10:22:22.185796 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:22:22.191694 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m10:22:22.191953 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.192220 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

  create view "finny_db"."public"."setup_functions__dbt_tmp"
    
    
  as (
    -- Database function setup for deduplication pipeline
-- This model ensures the merge_similar_entities() function exists in the database



select 'Function merge_similar_entities() created' as status
  );
[0m10:22:22.193237 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:22:22.196019 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.196248 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions__dbt_tmp" rename to "setup_functions"
[0m10:22:22.196891 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:22:22.202775 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.203005 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

        CREATE OR REPLACE FUNCTION merge_similar_entities()
  RETURNS VOID
  LANGUAGE plpgsql
  AS $$
  DECLARE
      rec RECORD;
      merge_count INTEGER := 0;
  BEGIN
      FOR rec IN
          SELECT canonical_id, merged_id
          FROM "finny_db"."public_staging"."stg_entity_clusters"
      LOOP
          -- Lock both entities to prevent race conditions
          PERFORM * FROM "finny_db"."public_staging"."stg_unified_prospects"
          WHERE prospect_id IN (rec.merged_id, rec.canonical_id) FOR UPDATE;

          -- Update canonical record fields only if null (fill in missing data)
          UPDATE "finny_db"."public_staging"."stg_unified_prospects" p
          SET
              name = COALESCE(p.name, s.name),
              email = COALESCE(p.email, s.email),
              company = COALESCE(p.company, s.company),
              title = COALESCE(p.title, s.title),
              city = COALESCE(p.city, s.city),
              state = COALESCE(p.state, s.state),
              company_revenue = COALESCE(p.company_revenue, s.company_revenue)
          FROM "finny_db"."public_staging"."stg_unified_prospects" s
          WHERE p.prospect_id = rec.canonical_id
            AND s.prospect_id = rec.merged_id;

          -- Mark merged record
          UPDATE "finny_db"."public_staging"."stg_unified_prospects"
          SET status = 'merged'
          WHERE prospect_id = rec.merged_id;

          merge_count := merge_count + 1;
      END LOOP;

      RAISE NOTICE 'Merged % entities', merge_count;
  END;
  $$;
      
[0m10:22:22.204404 [debug] [Thread-1 (]: SQL status: CREATE FUNCTION in 0.001 seconds
[0m10:22:22.204780 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: COMMIT
[0m10:22:22.204927 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.205054 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: COMMIT
[0m10:22:22.205741 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:22:22.209036 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public"."setup_functions__dbt_backup"
[0m10:22:22.210993 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:22:22.211183 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
drop view if exists "finny_db"."public"."setup_functions__dbt_backup" cascade
[0m10:22:22.211747 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:22:22.213050 [debug] [Thread-1 (]: On model.dbt_service.setup_functions: Close
[0m10:22:22.214090 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83d0b346-08a8-4d7d-901f-151535b6e0cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c826310>]}
[0m10:22:22.214392 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public.setup_functions ........................ [[32mCREATE VIEW[0m in 0.05s]
[0m10:22:22.214658 [debug] [Thread-1 (]: Finished running node model.dbt_service.setup_functions
[0m10:22:22.215746 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.215905 [debug] [MainThread]: On master: BEGIN
[0m10:22:22.216031 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:22:22.221772 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:22:22.222004 [debug] [MainThread]: On master: COMMIT
[0m10:22:22.222170 [debug] [MainThread]: Using postgres connection "master"
[0m10:22:22.222283 [debug] [MainThread]: On master: COMMIT
[0m10:22:22.222634 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:22:22.222748 [debug] [MainThread]: On master: Close
[0m10:22:22.222919 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:22:22.223040 [debug] [MainThread]: Connection 'model.dbt_service.setup_functions' was properly closed.
[0m10:22:22.223137 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m10:22:22.223243 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m10:22:22.223340 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m10:22:22.223432 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m10:22:22.223543 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m10:22:22.223658 [info ] [MainThread]: 
[0m10:22:22.223810 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m10:22:22.224086 [debug] [MainThread]: Command end result
[0m10:22:22.236738 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:22:22.237845 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:22:22.240356 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:22:22.240501 [info ] [MainThread]: 
[0m10:22:22.240662 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:22:22.240773 [info ] [MainThread]: 
[0m10:22:22.240893 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:22:22.243202 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7154127, "process_in_blocks": "0", "process_kernel_time": 0.196724, "process_mem_max_rss": "144162816", "process_out_blocks": "0", "process_user_time": 1.125443}
[0m10:22:22.243562 [debug] [MainThread]: Command `dbt run` succeeded at 10:22:22.243518 after 0.72 seconds
[0m10:22:22.243750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d74350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10956fa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102daf8d0>]}
[0m10:22:22.243922 [debug] [MainThread]: Flushing usage events
[0m10:22:22.535876 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:22:27.954426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad2b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad9b450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad9a210>]}


============================== 10:22:27.956168 | 6482d638-15ee-42f7-bca8-3cbe96bad24d ==============================
[0m10:22:27.956168 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:22:27.956481 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'partial_parse': 'True', 'write_json': 'True', 'invocation_command': 'dbt run-operation merge_entities', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'quiet': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'use_experimental_parser': 'False', 'log_format': 'default', 'printer_width': '80', 'static_parser': 'True', 'use_colors': 'True', 'empty': 'None', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m10:22:28.038141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6482d638-15ee-42f7-bca8-3cbe96bad24d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad9b550>]}
[0m10:22:28.067431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6482d638-15ee-42f7-bca8-3cbe96bad24d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b355ed0>]}
[0m10:22:28.067872 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:22:28.109189 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:22:28.166154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:22:28.166395 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:22:28.189766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6482d638-15ee-42f7-bca8-3cbe96bad24d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2cc290>]}
[0m10:22:28.229918 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:22:28.230810 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:22:28.239459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6482d638-15ee-42f7-bca8-3cbe96bad24d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad9af10>]}
[0m10:22:28.239686 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:22:28.239850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6482d638-15ee-42f7-bca8-3cbe96bad24d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8462d0>]}
[0m10:22:28.240106 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m10:22:28.240261 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:22:28.240377 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m10:22:28.240501 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:22:28.261049 [debug] [MainThread]: SQL status: BEGIN in 0.021 seconds
[0m10:22:28.261240 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:22:28.261373 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:22:28.261491 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:22:28.262089 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:22:28.264493 [info ] [MainThread]: Executing prospect merge operation...
[0m10:22:28.311928 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:22:28.312208 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
      SELECT merge_similar_entities();
    
  
[0m10:22:29.502474 [debug] [MainThread]: SQL status: SELECT 1 in 1.190 seconds
[0m10:22:29.504819 [info ] [MainThread]: Merge operation completed successfully
[0m10:22:29.505575 [debug] [MainThread]: On macro_merge_entities: Close
[0m10:22:29.514062 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:22:29.515836 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.5951241, "process_in_blocks": "0", "process_kernel_time": 0.146638, "process_mem_max_rss": "134037504", "process_out_blocks": "0", "process_user_time": 0.847457}
[0m10:22:29.516281 [debug] [MainThread]: Command `dbt run-operation` succeeded at 10:22:29.516179 after 1.60 seconds
[0m10:22:29.516630 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m10:22:29.516978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10add7f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10500c110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bcce9d0>]}
[0m10:22:29.517355 [debug] [MainThread]: Flushing usage events
[0m10:22:29.763889 [debug] [MainThread]: An error was encountered while trying to flush usage events
==================== 2025-11-09 15:31:40.788409 | 019a693e-9668-7583-b99e-fecbfbe97f77 ====================
==================== 2025-11-09 15:31:54.257871 | 019a693e-cb10-7603-a816-b3f37fa261d0 ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
15:31:54.736051 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [13.9s]
==================== 2025-11-09 15:33:56.270549 | 019a6940-a7a2-7523-ad42-21d37e79c27f ====================
15:41:05.117779 [info ]: 
==================== Execution Summary =====================
Finished 'init' successfully [7m 9s]
[0m10:41:09.163648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d93650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aa7610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a9dfd0>]}


============================== 10:41:09.165958 | b0e0987e-a707-4d97-b619-42da20fdb034 ==============================
[0m10:41:09.165958 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:41:09.166244 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'quiet': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'printer_width': '80', 'target_path': 'None', 'use_experimental_parser': 'False', 'no_print': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'debug': 'False', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m10:41:09.292599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11304aed0>]}
[0m10:41:09.321435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbc750>]}
[0m10:41:09.321978 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:41:09.367541 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:41:09.432380 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:41:09.432684 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m10:41:09.578802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107084b90>]}
[0m10:41:09.617988 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:41:09.619055 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:41:09.630226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a53b10>]}
[0m10:41:09.630445 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:41:09.630601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d305d0>]}
[0m10:41:09.631581 [info ] [MainThread]: 
[0m10:41:09.631731 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:41:09.631848 [info ] [MainThread]: 
[0m10:41:09.632031 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:41:09.633861 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.634062 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.634271 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.638366 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.638661 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.638897 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:41:09.664041 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.664222 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.664382 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.664511 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.664640 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.664764 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.664881 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:41:09.665011 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.665142 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.665272 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.665394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.665518 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.665646 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:41:09.665768 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.665880 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.665993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.666625 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.666742 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:09.717550 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.052 seconds
[0m10:41:09.717738 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.052 seconds
[0m10:41:09.717890 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.051 seconds
[0m10:41:09.718017 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.051 seconds
[0m10:41:09.718116 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.052 seconds
[0m10:41:09.718638 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.718768 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.053 seconds
[0m10:41:09.719258 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.719644 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.720004 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.720365 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.720844 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:41:09.722182 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m10:41:09.722377 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m10:41:09.724783 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:41:09.724977 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m10:41:09.725167 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m10:41:09.725894 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:41:09.726101 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m10:41:09.726281 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m10:41:09.726394 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:41:09.727151 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:41:09.727829 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:41:09.728242 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:41:09.728865 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:41:09.729483 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:41:09.729597 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.729707 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:41:09.729809 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:41:09.729915 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.730020 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:41:09.730123 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:41:09.730307 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.730433 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.730620 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.730760 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:41:09.745567 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:41:09.745839 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:41:09.746017 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:41:09.746874 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:41:09.747009 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:41:09.747168 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:41:09.750965 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m10:41:09.751864 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:41:09.752085 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:41:09.752299 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:41:09.752421 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:41:09.752535 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:41:09.752770 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.005 seconds
[0m10:41:09.752968 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:41:09.753136 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:41:09.753296 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:41:09.753422 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:41:09.753545 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:41:09.754206 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:41:09.754410 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:41:09.754617 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:41:09.755122 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:41:09.755286 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:41:09.755725 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:41:09.757129 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m10:41:09.757300 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m10:41:09.757572 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.002 seconds
[0m10:41:09.757717 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:41:09.758346 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:41:09.758827 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:41:09.759286 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:41:09.759695 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:41:09.760215 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:41:09.760403 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:41:09.760581 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:41:09.760698 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:41:09.767979 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:09.768188 [debug] [MainThread]: On master: BEGIN
[0m10:41:09.768316 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:41:09.775219 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m10:41:09.775527 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:09.775766 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:41:09.777741 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:41:09.779178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114050210>]}
[0m10:41:09.779402 [debug] [MainThread]: On master: ROLLBACK
[0m10:41:09.779878 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:09.780167 [debug] [MainThread]: On master: BEGIN
[0m10:41:09.780913 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:41:09.781073 [debug] [MainThread]: On master: COMMIT
[0m10:41:09.781224 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:09.781341 [debug] [MainThread]: On master: COMMIT
[0m10:41:09.781690 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:41:09.781857 [debug] [MainThread]: On master: Close
[0m10:41:09.784306 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m10:41:09.784503 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m10:41:09.784733 [info ] [Thread-1 (]: 1 of 18 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m10:41:09.784983 [info ] [Thread-2 (]: 2 of 18 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m10:41:09.785211 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.raw_fxf_data)
[0m10:41:09.785391 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_pdl_data)
[0m10:41:09.785573 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m10:41:09.785750 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m10:41:09.788912 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m10:41:09.790048 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m10:41:09.790851 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m10:41:09.791101 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m10:41:09.806275 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m10:41:09.807548 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m10:41:09.808141 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.808446 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.808703 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m10:41:09.808900 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m10:41:09.809103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:41:09.809310 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:41:09.815901 [debug] [Thread-2 (]: SQL status: BEGIN in 0.007 seconds
[0m10:41:09.816121 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m10:41:09.816355 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.816551 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.816731 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m10:41:09.816909 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m10:41:09.846890 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.030 seconds
[0m10:41:09.852012 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.852280 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m10:41:09.853248 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:09.854843 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.855105 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.038 seconds
[0m10:41:09.855282 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m10:41:09.856602 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.856861 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m10:41:09.857147 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:09.863602 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m10:41:09.864091 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:41:09.865339 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.865510 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.865668 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m10:41:09.865819 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:41:09.866346 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:09.866909 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:41:09.867077 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.867222 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:41:09.871097 [debug] [Thread-2 (]: SQL status: COMMIT in 0.005 seconds
[0m10:41:09.871304 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m10:41:09.873954 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m10:41:09.874924 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m10:41:09.876743 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:41:09.876988 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:41:09.877151 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m10:41:09.877308 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m10:41:09.879376 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:41:09.879528 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:41:09.880510 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m10:41:09.880993 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m10:41:09.882496 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146b7110>]}
[0m10:41:09.882646 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e3b850>]}
[0m10:41:09.882978 [info ] [Thread-1 (]: 1 of 18 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.10s]
[0m10:41:09.883252 [info ] [Thread-2 (]: 2 of 18 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.10s]
[0m10:41:09.883525 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m10:41:09.883734 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m10:41:09.884008 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_fxf_data
[0m10:41:09.884286 [info ] [Thread-4 (]: 3 of 18 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m10:41:09.884471 [debug] [Thread-6 (]: Began running node model.dbt_service.raw_company_profiling
[0m10:41:09.884631 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_fxf_data)
[0m10:41:09.884779 [debug] [Thread-7 (]: Began running node model.dbt_service.raw_data_profiling
[0m10:41:09.884917 [debug] [Thread-8 (]: Began running node model.dbt_service.raw_location_profiling
[0m10:41:09.885054 [debug] [Thread-9 (]: Began running node model.dbt_service.stg_pdl_data
[0m10:41:09.885238 [info ] [Thread-6 (]: 4 of 18 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m10:41:09.885405 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m10:41:09.885585 [info ] [Thread-7 (]: 5 of 18 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m10:41:09.885834 [info ] [Thread-8 (]: 6 of 18 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m10:41:09.886059 [info ] [Thread-9 (]: 7 of 18 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m10:41:09.886247 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_company_profiling)
[0m10:41:09.887712 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m10:41:09.887928 [debug] [Thread-7 (]: Acquiring new postgres connection 'model.dbt_service.raw_data_profiling'
[0m10:41:09.888113 [debug] [Thread-8 (]: Acquiring new postgres connection 'model.dbt_service.raw_location_profiling'
[0m10:41:09.888291 [debug] [Thread-9 (]: Acquiring new postgres connection 'model.dbt_service.stg_pdl_data'
[0m10:41:09.888442 [debug] [Thread-6 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m10:41:09.888642 [debug] [Thread-7 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m10:41:09.888796 [debug] [Thread-8 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m10:41:09.888983 [debug] [Thread-9 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m10:41:09.890438 [debug] [Thread-6 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m10:41:09.890615 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_fxf_data
[0m10:41:09.892099 [debug] [Thread-7 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m10:41:09.894277 [debug] [Thread-8 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m10:41:09.896649 [debug] [Thread-9 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m10:41:09.903755 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m10:41:09.904031 [debug] [Thread-6 (]: Began executing node model.dbt_service.raw_company_profiling
[0m10:41:09.904376 [debug] [Thread-7 (]: Began executing node model.dbt_service.raw_data_profiling
[0m10:41:09.904612 [debug] [Thread-9 (]: Began executing node model.dbt_service.stg_pdl_data
[0m10:41:09.905856 [debug] [Thread-6 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m10:41:09.905993 [debug] [Thread-8 (]: Began executing node model.dbt_service.raw_location_profiling
[0m10:41:09.907223 [debug] [Thread-7 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m10:41:09.907408 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:41:09.908646 [debug] [Thread-9 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m10:41:09.909842 [debug] [Thread-8 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m10:41:09.910070 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m10:41:09.910250 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:09.910448 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:09.910644 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m10:41:09.910831 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:41:09.910966 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m10:41:09.911120 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m10:41:09.911272 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:09.911487 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m10:41:09.911627 [debug] [Thread-6 (]: Opening a new connection, currently in state closed
[0m10:41:09.911768 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m10:41:09.911912 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m10:41:09.912047 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m10:41:09.912308 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m10:41:09.921930 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m10:41:09.922136 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:41:09.922353 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m10:41:09.922979 [debug] [Thread-6 (]: SQL status: BEGIN in 0.011 seconds
[0m10:41:09.923192 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:09.923419 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:41:09.923652 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:41:09.925132 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:41:09.925336 [debug] [Thread-7 (]: SQL status: BEGIN in 0.014 seconds
[0m10:41:09.925564 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m10:41:09.925733 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:09.925955 [debug] [Thread-9 (]: SQL status: BEGIN in 0.014 seconds
[0m10:41:09.926161 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m10:41:09.926391 [debug] [Thread-8 (]: SQL status: BEGIN in 0.014 seconds
[0m10:41:09.926528 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:09.926691 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:41:09.926862 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:09.927454 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:41:09.927681 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m10:41:09.928010 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m10:41:09.928325 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:41:09.928534 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:41:09.929170 [debug] [Thread-4 (]: SQL status: COMMIT in 0.000 seconds
[0m10:41:09.930215 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m10:41:09.930395 [debug] [Thread-9 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:41:09.931680 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:41:09.933529 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:41:09.933713 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m10:41:09.933878 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m10:41:09.934450 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:41:09.934634 [debug] [Thread-9 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:09.935234 [debug] [Thread-4 (]: On model.dbt_service.stg_fxf_data: Close
[0m10:41:09.935760 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:41:09.936025 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:41:09.936277 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11304e0d0>]}
[0m10:41:09.936444 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:41:09.936727 [info ] [Thread-4 (]: 3 of 18 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m10:41:09.937134 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_fxf_data
[0m10:41:09.937664 [debug] [Thread-9 (]: SQL status: COMMIT in 0.001 seconds
[0m10:41:09.938707 [debug] [Thread-9 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m10:41:09.938945 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:41:09.939095 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m10:41:09.939622 [debug] [Thread-9 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:41:09.940207 [debug] [Thread-9 (]: On model.dbt_service.stg_pdl_data: Close
[0m10:41:09.940683 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146cb350>]}
[0m10:41:09.941158 [info ] [Thread-9 (]: 7 of 18 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.05s]
[0m10:41:09.941427 [debug] [Thread-9 (]: Finished running node model.dbt_service.stg_pdl_data
[0m10:41:09.941897 [debug] [Thread-11 ]: Began running node model.dbt_service.company_analysis
[0m10:41:09.942108 [debug] [Thread-12 ]: Began running node model.dbt_service.staging_city_state_profiling
[0m10:41:09.942271 [debug] [Thread-13 ]: Began running node model.dbt_service.staging_company_profiling
[0m10:41:09.942449 [debug] [Thread-14 ]: Began running node model.dbt_service.staging_data_profiling
[0m10:41:09.942615 [debug] [Thread-15 ]: Began running node model.dbt_service.staging_location_profiling
[0m10:41:09.942752 [debug] [Thread-16 ]: Began running node model.dbt_service.stg_unified_prospects
[0m10:41:09.942965 [info ] [Thread-11 ]: 8 of 18 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m10:41:09.943209 [info ] [Thread-12 ]: 9 of 18 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m10:41:09.943423 [info ] [Thread-13 ]: 10 of 18 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m10:41:09.943623 [info ] [Thread-14 ]: 11 of 18 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m10:41:09.943827 [info ] [Thread-15 ]: 12 of 18 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m10:41:09.944019 [info ] [Thread-16 ]: 13 of 18 START sql table model public_staging.stg_unified_prospects ............ [RUN]
[0m10:41:09.944261 [debug] [Thread-11 ]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m10:41:09.944463 [debug] [Thread-12 ]: Acquiring new postgres connection 'model.dbt_service.staging_city_state_profiling'
[0m10:41:09.944920 [debug] [Thread-13 ]: Acquiring new postgres connection 'model.dbt_service.staging_company_profiling'
[0m10:41:09.945208 [debug] [Thread-14 ]: Acquiring new postgres connection 'model.dbt_service.staging_data_profiling'
[0m10:41:09.945427 [debug] [Thread-15 ]: Acquiring new postgres connection 'model.dbt_service.staging_location_profiling'
[0m10:41:09.945615 [debug] [Thread-16 ]: Acquiring new postgres connection 'model.dbt_service.stg_unified_prospects'
[0m10:41:09.945781 [debug] [Thread-11 ]: Began compiling node model.dbt_service.company_analysis
[0m10:41:09.945939 [debug] [Thread-12 ]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m10:41:09.946123 [debug] [Thread-13 ]: Began compiling node model.dbt_service.staging_company_profiling
[0m10:41:09.946298 [debug] [Thread-14 ]: Began compiling node model.dbt_service.staging_data_profiling
[0m10:41:09.946458 [debug] [Thread-15 ]: Began compiling node model.dbt_service.staging_location_profiling
[0m10:41:09.946607 [debug] [Thread-16 ]: Began compiling node model.dbt_service.stg_unified_prospects
[0m10:41:09.948283 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m10:41:09.950155 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m10:41:09.951481 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m10:41:09.952810 [debug] [Thread-14 ]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m10:41:09.954147 [debug] [Thread-15 ]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m10:41:09.955575 [debug] [Thread-16 ]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m10:41:09.956519 [debug] [Thread-11 ]: Began executing node model.dbt_service.company_analysis
[0m10:41:09.956719 [debug] [Thread-13 ]: Began executing node model.dbt_service.staging_company_profiling
[0m10:41:09.956863 [debug] [Thread-12 ]: Began executing node model.dbt_service.staging_city_state_profiling
[0m10:41:09.958509 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m10:41:09.959952 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m10:41:09.960119 [debug] [Thread-14 ]: Began executing node model.dbt_service.staging_data_profiling
[0m10:41:09.960269 [debug] [Thread-15 ]: Began executing node model.dbt_service.staging_location_profiling
[0m10:41:09.960404 [debug] [Thread-16 ]: Began executing node model.dbt_service.stg_unified_prospects
[0m10:41:09.962567 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m10:41:09.963938 [debug] [Thread-14 ]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m10:41:09.965303 [debug] [Thread-15 ]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m10:41:09.965492 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:09.965655 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:09.966781 [debug] [Thread-16 ]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m10:41:09.967084 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: BEGIN
[0m10:41:09.967252 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:09.967416 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:09.967593 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: BEGIN
[0m10:41:09.967749 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:09.967931 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m10:41:09.968100 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m10:41:09.968256 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: BEGIN
[0m10:41:09.968408 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:09.968561 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m10:41:09.968719 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: BEGIN
[0m10:41:09.968941 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m10:41:09.969147 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m10:41:09.969330 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m10:41:09.969548 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m10:41:09.969812 [debug] [Thread-16 ]: Opening a new connection, currently in state init
[0m10:41:09.985026 [debug] [Thread-14 ]: SQL status: BEGIN in 0.016 seconds
[0m10:41:09.985280 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:09.985523 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m10:41:09.985939 [debug] [Thread-12 ]: SQL status: BEGIN in 0.017 seconds
[0m10:41:09.986126 [debug] [Thread-13 ]: SQL status: BEGIN in 0.018 seconds
[0m10:41:09.986319 [debug] [Thread-11 ]: SQL status: BEGIN in 0.018 seconds
[0m10:41:09.986512 [debug] [Thread-15 ]: SQL status: BEGIN in 0.017 seconds
[0m10:41:09.986675 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:09.986819 [debug] [Thread-16 ]: SQL status: BEGIN in 0.017 seconds
[0m10:41:09.986975 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:09.987123 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:09.987266 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:09.987486 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m10:41:09.987711 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:09.987958 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:41:09.988278 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m10:41:09.988584 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m10:41:09.988913 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m10:41:10.015849 [debug] [Thread-6 (]: SQL status: SELECT 180 in 0.092 seconds
[0m10:41:10.022557 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:10.023065 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m10:41:10.029064 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m10:41:10.031871 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:10.032325 [debug] [Thread-7 (]: SQL status: SELECT 2 in 0.105 seconds
[0m10:41:10.032681 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m10:41:10.037309 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:10.037863 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m10:41:10.042994 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m10:41:10.044576 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:41:10.045158 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:10.045487 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:41:10.045845 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.008 seconds
[0m10:41:10.047195 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:10.047368 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m10:41:10.050702 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m10:41:10.051878 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:41:10.052074 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:10.052217 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:41:10.053358 [debug] [Thread-6 (]: SQL status: COMMIT in 0.007 seconds
[0m10:41:10.055107 [debug] [Thread-6 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m10:41:10.055387 [debug] [Thread-7 (]: SQL status: COMMIT in 0.003 seconds
[0m10:41:10.056835 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:41:10.060007 [debug] [Thread-7 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m10:41:10.060583 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m10:41:10.061183 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:41:10.062043 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m10:41:10.067678 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.006 seconds
[0m10:41:10.069178 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: Close
[0m10:41:10.069975 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106751d0>]}
[0m10:41:10.070704 [info ] [Thread-6 (]: 4 of 18 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.18s]
[0m10:41:10.071270 [debug] [Thread-6 (]: Finished running node model.dbt_service.raw_company_profiling
[0m10:41:10.073933 [debug] [Thread-7 (]: SQL status: DROP TABLE in 0.011 seconds
[0m10:41:10.075377 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: Close
[0m10:41:10.076176 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114050950>]}
[0m10:41:10.077027 [info ] [Thread-7 (]: 5 of 18 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.19s]
[0m10:41:10.077627 [debug] [Thread-7 (]: Finished running node model.dbt_service.raw_data_profiling
[0m10:41:10.108238 [debug] [Thread-8 (]: SQL status: SELECT 152 in 0.180 seconds
[0m10:41:10.110598 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:10.110781 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m10:41:10.111694 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:10.113045 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:10.113217 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m10:41:10.114056 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:10.114732 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:41:10.114907 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:10.115052 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:41:10.115715 [debug] [Thread-14 ]: SQL status: SELECT 2 in 0.130 seconds
[0m10:41:10.115924 [debug] [Thread-8 (]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.117210 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:10.118308 [debug] [Thread-8 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m10:41:10.118492 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m10:41:10.118782 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:41:10.119003 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m10:41:10.119477 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:10.120724 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:10.120868 [debug] [Thread-8 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:41:10.121082 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m10:41:10.121592 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: Close
[0m10:41:10.121925 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a73650>]}
[0m10:41:10.122342 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:41:10.122216 [info ] [Thread-8 (]: 6 of 18 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.23s]
[0m10:41:10.122893 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:41:10.123185 [debug] [Thread-8 (]: Finished running node model.dbt_service.raw_location_profiling
[0m10:41:10.123389 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:10.123647 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:41:10.124181 [debug] [Thread-14 ]: SQL status: COMMIT in 0.000 seconds
[0m10:41:10.125213 [debug] [Thread-14 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m10:41:10.125443 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:41:10.125599 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m10:41:10.126494 [debug] [Thread-14 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:41:10.127020 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: Close
[0m10:41:10.127270 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166234d0>]}
[0m10:41:10.127520 [info ] [Thread-14 ]: 11 of 18 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.18s]
[0m10:41:10.127749 [debug] [Thread-14 ]: Finished running node model.dbt_service.staging_data_profiling
[0m10:41:10.145020 [debug] [Thread-12 ]: SQL status: SELECT 76 in 0.156 seconds
[0m10:41:10.146678 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:10.146862 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m10:41:10.147427 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.148576 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:10.148738 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m10:41:10.149237 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.149687 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:41:10.149831 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:10.149961 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:41:10.150611 [debug] [Thread-12 ]: SQL status: COMMIT in 0.000 seconds
[0m10:41:10.151572 [debug] [Thread-12 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m10:41:10.151805 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:41:10.151953 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m10:41:10.152824 [debug] [Thread-11 ]: SQL status: SELECT 91 in 0.164 seconds
[0m10:41:10.154036 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:10.154221 [debug] [Thread-12 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:41:10.154397 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m10:41:10.154948 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: Close
[0m10:41:10.155323 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11654fc50>]}
[0m10:41:10.155517 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.155924 [info ] [Thread-12 ]: 9 of 18 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.21s]
[0m10:41:10.157236 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:10.157467 [debug] [Thread-12 ]: Finished running node model.dbt_service.staging_city_state_profiling
[0m10:41:10.157639 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m10:41:10.158165 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.158642 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:41:10.158783 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:10.158913 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:41:10.159394 [debug] [Thread-11 ]: SQL status: COMMIT in 0.000 seconds
[0m10:41:10.161286 [debug] [Thread-11 ]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m10:41:10.161522 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:41:10.161680 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m10:41:10.162600 [debug] [Thread-11 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:41:10.163181 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: Close
[0m10:41:10.163483 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166de090>]}
[0m10:41:10.163799 [info ] [Thread-11 ]: 8 of 18 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.22s]
[0m10:41:10.164027 [debug] [Thread-11 ]: Finished running node model.dbt_service.company_analysis
[0m10:41:10.239277 [debug] [Thread-16 ]: SQL status: SELECT 100010 in 0.247 seconds
[0m10:41:10.241040 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:10.241230 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m10:41:10.241847 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.242930 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:10.243078 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m10:41:10.243518 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.244020 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:41:10.244170 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:10.244307 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:41:10.245387 [debug] [Thread-16 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.246347 [debug] [Thread-16 ]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m10:41:10.246563 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:41:10.246708 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m10:41:10.248754 [debug] [Thread-16 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:41:10.249265 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: Close
[0m10:41:10.249514 [debug] [Thread-16 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11658f690>]}
[0m10:41:10.249781 [info ] [Thread-16 ]: 13 of 18 OK created sql table model public_staging.stg_unified_prospects ....... [[32mSELECT 100010[0m in 0.30s]
[0m10:41:10.250072 [debug] [Thread-16 ]: Finished running node model.dbt_service.stg_unified_prospects
[0m10:41:10.250323 [debug] [Thread-18 ]: Began running node model.dbt_service.stg_prospect_matches
[0m10:41:10.250521 [info ] [Thread-18 ]: 14 of 18 START sql incremental model public_staging.stg_prospect_matches ....... [RUN]
[0m10:41:10.250738 [debug] [Thread-18 ]: Acquiring new postgres connection 'model.dbt_service.stg_prospect_matches'
[0m10:41:10.250886 [debug] [Thread-18 ]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:41:10.253332 [debug] [Thread-18 ]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:41:10.253977 [debug] [Thread-18 ]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:41:10.266829 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.267157 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp104110264405"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 OR  -- Lower threshold
    similarity(a.email, b.email) > 0.4   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m10:41:10.267357 [debug] [Thread-18 ]: Opening a new connection, currently in state init
[0m10:41:10.276114 [debug] [Thread-15 ]: SQL status: SELECT 152 in 0.287 seconds
[0m10:41:10.277541 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:10.277720 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m10:41:10.278227 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.279303 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:10.279454 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m10:41:10.279848 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.280286 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:41:10.280438 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:10.280570 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:41:10.281154 [debug] [Thread-15 ]: SQL status: COMMIT in 0.000 seconds
[0m10:41:10.282058 [debug] [Thread-15 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m10:41:10.282283 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:41:10.282439 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m10:41:10.283547 [debug] [Thread-15 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:41:10.284105 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: Close
[0m10:41:10.284372 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116545810>]}
[0m10:41:10.284641 [info ] [Thread-15 ]: 12 of 18 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.34s]
[0m10:41:10.284873 [debug] [Thread-15 ]: Finished running node model.dbt_service.staging_location_profiling
[0m10:41:10.302111 [debug] [Thread-13 ]: SQL status: SELECT 180 in 0.313 seconds
[0m10:41:10.303636 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:10.303798 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m10:41:10.304386 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.306458 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:10.306609 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m10:41:10.307030 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.307449 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:41:10.307587 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:10.307709 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:41:10.308496 [debug] [Thread-13 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.337147 [debug] [Thread-18 ]: SQL status: SELECT 0 in 0.070 seconds
[0m10:41:10.337826 [debug] [Thread-13 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m10:41:10.343541 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.343819 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:41:10.343982 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:41:10.344172 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m10:41:10.344801 [debug] [Thread-18 ]: SQL status: BEGIN in 0.000 seconds
[0m10:41:10.344952 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.345108 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp104110264405'
        
      order by ordinal_position

  
[0m10:41:10.345583 [debug] [Thread-13 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:41:10.346125 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: Close
[0m10:41:10.346378 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116696590>]}
[0m10:41:10.346636 [info ] [Thread-13 ]: 10 of 18 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.40s]
[0m10:41:10.346864 [debug] [Thread-13 ]: Finished running node model.dbt_service.staging_company_profiling
[0m10:41:10.348063 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.003 seconds
[0m10:41:10.350216 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.350392 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:41:10.351828 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:41:10.357410 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.357645 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp104110264405'
        
      order by ordinal_position

  
[0m10:41:10.359290 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:41:10.360995 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.361184 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:41:10.362936 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.002 seconds
[0m10:41:10.366870 [debug] [Thread-18 ]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:41:10.372406 [debug] [Thread-18 ]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:41:10.372810 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.373195 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp104110264405" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp104110264405"
    )
  
[0m10:41:10.374083 [debug] [Thread-18 ]: SQL status: INSERT 0 0 in 0.001 seconds
[0m10:41:10.374779 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:41:10.374966 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:41:10.375111 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:41:10.376018 [debug] [Thread-18 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.376361 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: Close
[0m10:41:10.376664 [debug] [Thread-18 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145da810>]}
[0m10:41:10.376996 [info ] [Thread-18 ]: 14 of 18 OK created sql incremental model public_staging.stg_prospect_matches .. [[32mINSERT 0 0[0m in 0.13s]
[0m10:41:10.377250 [debug] [Thread-18 ]: Finished running node model.dbt_service.stg_prospect_matches
[0m10:41:10.377635 [debug] [Thread-20 ]: Began running node model.dbt_service.prospect_matching_ratio
[0m10:41:10.377801 [debug] [Thread-21 ]: Began running node model.dbt_service.stg_entity_clusters
[0m10:41:10.378057 [info ] [Thread-20 ]: 15 of 18 START sql view model public_marts.prospect_matching_ratio ............. [RUN]
[0m10:41:10.378301 [info ] [Thread-21 ]: 16 of 18 START sql table model public_staging.stg_entity_clusters .............. [RUN]
[0m10:41:10.378553 [debug] [Thread-20 ]: Acquiring new postgres connection 'model.dbt_service.prospect_matching_ratio'
[0m10:41:10.378798 [debug] [Thread-21 ]: Acquiring new postgres connection 'model.dbt_service.stg_entity_clusters'
[0m10:41:10.378969 [debug] [Thread-20 ]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m10:41:10.379121 [debug] [Thread-21 ]: Began compiling node model.dbt_service.stg_entity_clusters
[0m10:41:10.380896 [debug] [Thread-20 ]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.382075 [debug] [Thread-21 ]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m10:41:10.382732 [debug] [Thread-21 ]: Began executing node model.dbt_service.stg_entity_clusters
[0m10:41:10.382868 [debug] [Thread-20 ]: Began executing node model.dbt_service.prospect_matching_ratio
[0m10:41:10.384257 [debug] [Thread-21 ]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m10:41:10.385526 [debug] [Thread-20 ]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.385926 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.386090 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.386252 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m10:41:10.386404 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m10:41:10.386616 [debug] [Thread-21 ]: Opening a new connection, currently in state init
[0m10:41:10.386768 [debug] [Thread-20 ]: Opening a new connection, currently in state init
[0m10:41:10.393191 [debug] [Thread-20 ]: SQL status: BEGIN in 0.006 seconds
[0m10:41:10.393407 [debug] [Thread-21 ]: SQL status: BEGIN in 0.007 seconds
[0m10:41:10.393605 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.393771 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.394008 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m10:41:10.394262 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged
-- Results:

-- 87 duplicate pairs identified from high-confidence matches (score > 0.8)
-- Canonical ID logic: Uses the higher prospect_id as canonical (keeps the "later" record)
-- Merged ID logic: Lower prospect_id will be marked as duplicate
-- Range: Processing prospects from 658 to 9999
-- How it works:

-- Example: Prospect 658 is canonical, Prospect 291 should be merged into it
-- etc.



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m10:41:10.396166 [debug] [Thread-20 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:41:10.398586 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.400025 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m10:41:10.400372 [debug] [Thread-21 ]: SQL status: SELECT 87 in 0.006 seconds
[0m10:41:10.401645 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.401802 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:41:10.401970 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters" rename to "stg_entity_clusters__dbt_backup"
[0m10:41:10.402469 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:41:10.402656 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.402802 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:41:10.403044 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.405281 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.405491 [debug] [Thread-20 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.412637 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m10:41:10.413781 [debug] [Thread-20 ]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m10:41:10.418950 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:41:10.419158 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.419316 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m10:41:10.419913 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:41:10.420100 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.420349 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:41:10.420524 [debug] [Thread-20 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:41:10.421163 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: Close
[0m10:41:10.421330 [debug] [Thread-21 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.422624 [debug] [Thread-21 ]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m10:41:10.422772 [debug] [Thread-20 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114526f10>]}
[0m10:41:10.423008 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:41:10.423280 [info ] [Thread-20 ]: 15 of 18 OK created sql view model public_marts.prospect_matching_ratio ........ [[32mCREATE VIEW[0m in 0.04s]
[0m10:41:10.423463 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m10:41:10.423683 [debug] [Thread-20 ]: Finished running node model.dbt_service.prospect_matching_ratio
[0m10:41:10.424642 [debug] [Thread-21 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:41:10.425131 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: Close
[0m10:41:10.425357 [debug] [Thread-21 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ff7dd0>]}
[0m10:41:10.425598 [info ] [Thread-21 ]: 16 of 18 OK created sql table model public_staging.stg_entity_clusters ......... [[32mSELECT 87[0m in 0.05s]
[0m10:41:10.425815 [debug] [Thread-21 ]: Finished running node model.dbt_service.stg_entity_clusters
[0m10:41:10.426172 [debug] [Thread-23 ]: Began running node model.dbt_service.data_overview
[0m10:41:10.426362 [debug] [Thread-24 ]: Began running node model.dbt_service.setup_functions
[0m10:41:10.426609 [info ] [Thread-23 ]: 17 of 18 START sql view model public_marts.data_overview ....................... [RUN]
[0m10:41:10.426828 [info ] [Thread-24 ]: 18 of 18 START sql view model public.setup_functions ........................... [RUN]
[0m10:41:10.427055 [debug] [Thread-23 ]: Acquiring new postgres connection 'model.dbt_service.data_overview'
[0m10:41:10.427263 [debug] [Thread-24 ]: Acquiring new postgres connection 'model.dbt_service.setup_functions'
[0m10:41:10.427416 [debug] [Thread-23 ]: Began compiling node model.dbt_service.data_overview
[0m10:41:10.427588 [debug] [Thread-24 ]: Began compiling node model.dbt_service.setup_functions
[0m10:41:10.429287 [debug] [Thread-23 ]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m10:41:10.430481 [debug] [Thread-24 ]: Writing injected SQL for node "model.dbt_service.setup_functions"
[0m10:41:10.431128 [debug] [Thread-24 ]: Began executing node model.dbt_service.setup_functions
[0m10:41:10.431273 [debug] [Thread-23 ]: Began executing node model.dbt_service.data_overview
[0m10:41:10.432794 [debug] [Thread-24 ]: Writing runtime sql for node "model.dbt_service.setup_functions"
[0m10:41:10.434404 [debug] [Thread-23 ]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m10:41:10.434861 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.435043 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:41:10.435195 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: BEGIN
[0m10:41:10.435345 [debug] [Thread-23 ]: On model.dbt_service.data_overview: BEGIN
[0m10:41:10.435486 [debug] [Thread-24 ]: Opening a new connection, currently in state init
[0m10:41:10.435621 [debug] [Thread-23 ]: Opening a new connection, currently in state init
[0m10:41:10.441647 [debug] [Thread-24 ]: SQL status: BEGIN in 0.006 seconds
[0m10:41:10.441861 [debug] [Thread-23 ]: SQL status: BEGIN in 0.006 seconds
[0m10:41:10.442018 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.442185 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:41:10.442343 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

  create view "finny_db"."public"."setup_functions__dbt_tmp"
    
    
  as (
    -- Database function setup for deduplication pipeline
-- This model ensures the merge_similar_entities() function exists in the database



select 'Function merge_similar_entities() created' as status
  );
[0m10:41:10.442544 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m10:41:10.443439 [debug] [Thread-24 ]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:41:10.444897 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.445040 [debug] [Thread-23 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:41:10.445193 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions" rename to "setup_functions__dbt_backup"
[0m10:41:10.447388 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:41:10.447590 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m10:41:10.447866 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.449138 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.449321 [debug] [Thread-23 ]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:41:10.449483 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions__dbt_tmp" rename to "setup_functions"
[0m10:41:10.449989 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m10:41:10.450184 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:41:10.450337 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m10:41:10.450504 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:41:10.452671 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.452847 [debug] [Thread-23 ]: SQL status: COMMIT in 0.002 seconds
[0m10:41:10.453079 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

        CREATE OR REPLACE FUNCTION merge_similar_entities()
  RETURNS VOID
  LANGUAGE plpgsql
  AS $$
  DECLARE
      rec RECORD;
      merge_count INTEGER := 0;
  BEGIN
      FOR rec IN
          SELECT canonical_id, merged_id
          FROM "finny_db"."public_staging"."stg_entity_clusters"
      LOOP
          -- Lock both entities to prevent race conditions
          PERFORM * FROM "finny_db"."public_staging"."stg_unified_prospects"
          WHERE prospect_id IN (rec.merged_id, rec.canonical_id) FOR UPDATE;

          -- Update canonical record fields only if null (fill in missing data)
          UPDATE "finny_db"."public_staging"."stg_unified_prospects" p
          SET
              name = COALESCE(p.name, s.name),
              email = COALESCE(p.email, s.email),
              company = COALESCE(p.company, s.company),
              title = COALESCE(p.title, s.title),
              city = COALESCE(p.city, s.city),
              state = COALESCE(p.state, s.state),
              company_revenue = COALESCE(p.company_revenue, s.company_revenue)
          FROM "finny_db"."public_staging"."stg_unified_prospects" s
          WHERE p.prospect_id = rec.canonical_id
            AND s.prospect_id = rec.merged_id;

          -- Mark merged record
          UPDATE "finny_db"."public_staging"."stg_unified_prospects"
          SET status = 'merged'
          WHERE prospect_id = rec.merged_id;

          merge_count := merge_count + 1;
      END LOOP;

      RAISE NOTICE 'Merged % entities', merge_count;
  END;
  $$;
      
[0m10:41:10.454129 [debug] [Thread-23 ]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m10:41:10.454416 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:41:10.454567 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m10:41:10.454980 [debug] [Thread-23 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:41:10.455521 [debug] [Thread-23 ]: On model.dbt_service.data_overview: Close
[0m10:41:10.455667 [debug] [Thread-24 ]: SQL status: CREATE FUNCTION in 0.001 seconds
[0m10:41:10.456042 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m10:41:10.456290 [debug] [Thread-23 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114533610>]}
[0m10:41:10.456445 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.456722 [info ] [Thread-23 ]: 17 of 18 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.03s]
[0m10:41:10.456915 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m10:41:10.457125 [debug] [Thread-23 ]: Finished running node model.dbt_service.data_overview
[0m10:41:10.458031 [debug] [Thread-24 ]: SQL status: COMMIT in 0.001 seconds
[0m10:41:10.459085 [debug] [Thread-24 ]: Applying DROP to: "finny_db"."public"."setup_functions__dbt_backup"
[0m10:41:10.459308 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:41:10.459445 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
drop view if exists "finny_db"."public"."setup_functions__dbt_backup" cascade
[0m10:41:10.460315 [debug] [Thread-24 ]: SQL status: DROP VIEW in 0.001 seconds
[0m10:41:10.460808 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: Close
[0m10:41:10.461033 [debug] [Thread-24 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0e0987e-a707-4d97-b619-42da20fdb034', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114505b50>]}
[0m10:41:10.461280 [info ] [Thread-24 ]: 18 of 18 OK created sql view model public.setup_functions ...................... [[32mCREATE VIEW[0m in 0.03s]
[0m10:41:10.461499 [debug] [Thread-24 ]: Finished running node model.dbt_service.setup_functions
[0m10:41:10.462590 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:10.462753 [debug] [MainThread]: On master: BEGIN
[0m10:41:10.462879 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:41:10.468625 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:41:10.468801 [debug] [MainThread]: On master: COMMIT
[0m10:41:10.468930 [debug] [MainThread]: Using postgres connection "master"
[0m10:41:10.469035 [debug] [MainThread]: On master: COMMIT
[0m10:41:10.469371 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:41:10.469618 [debug] [MainThread]: On master: Close
[0m10:41:10.469843 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:41:10.469963 [debug] [MainThread]: Connection 'model.dbt_service.raw_fxf_data' was properly closed.
[0m10:41:10.470072 [debug] [MainThread]: Connection 'model.dbt_service.raw_pdl_data' was properly closed.
[0m10:41:10.470193 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m10:41:10.470292 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m10:41:10.470389 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m10:41:10.470498 [debug] [MainThread]: Connection 'model.dbt_service.raw_company_profiling' was properly closed.
[0m10:41:10.470599 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m10:41:10.470704 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m10:41:10.470810 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m10:41:10.470907 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m10:41:10.471006 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m10:41:10.471102 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m10:41:10.471192 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m10:41:10.471281 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m10:41:10.471378 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m10:41:10.471468 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m10:41:10.471555 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m10:41:10.471643 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m10:41:10.471732 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m10:41:10.471819 [debug] [MainThread]: Connection 'model.dbt_service.setup_functions' was properly closed.
[0m10:41:10.472015 [info ] [MainThread]: 
[0m10:41:10.472189 [info ] [MainThread]: Finished running 1 incremental model, 12 table models, 5 view models in 0 hours 0 minutes and 0.84 seconds (0.84s).
[0m10:41:10.473219 [debug] [MainThread]: Command end result
[0m10:41:10.485870 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:41:10.486592 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:41:10.489159 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:41:10.489297 [info ] [MainThread]: 
[0m10:41:10.489610 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:41:10.489780 [info ] [MainThread]: 
[0m10:41:10.489947 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=18
[0m10:41:10.492351 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.3634709, "process_in_blocks": "0", "process_kernel_time": 0.239588, "process_mem_max_rss": "143507456", "process_out_blocks": "0", "process_user_time": 1.584864}
[0m10:41:10.492679 [debug] [MainThread]: Command `dbt run` succeeded at 10:41:10.492643 after 1.36 seconds
[0m10:41:10.492868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dbc290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e8aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104df77d0>]}
[0m10:41:10.493033 [debug] [MainThread]: Flushing usage events
[0m10:41:11.023953 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:41:19.320760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108529590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10856b350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108598e50>]}


============================== 10:41:19.322412 | a527bbe8-d742-4102-a5ed-d9d5924e150e ==============================
[0m10:41:19.322412 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:41:19.322716 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'no_print': 'None', 'printer_width': '80', 'quiet': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'empty': 'None', 'debug': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'log_format': 'default', 'use_colors': 'True', 'warn_error': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'invocation_command': 'dbt run-operation merge_entities', 'indirect_selection': 'eager'}
[0m10:41:19.406048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a527bbe8-d742-4102-a5ed-d9d5924e150e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108568710>]}
[0m10:41:19.434948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a527bbe8-d742-4102-a5ed-d9d5924e150e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049b5d90>]}
[0m10:41:19.435377 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:41:19.476300 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:41:19.532328 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:41:19.532525 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:41:19.555515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a527bbe8-d742-4102-a5ed-d9d5924e150e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10859a690>]}
[0m10:41:19.594487 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:41:19.595324 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:41:19.600769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a527bbe8-d742-4102-a5ed-d9d5924e150e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cf25d0>]}
[0m10:41:19.600984 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:41:19.601144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a527bbe8-d742-4102-a5ed-d9d5924e150e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108daa0d0>]}
[0m10:41:19.601384 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m10:41:19.601532 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:41:19.601647 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m10:41:19.601760 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:41:19.621208 [debug] [MainThread]: SQL status: BEGIN in 0.019 seconds
[0m10:41:19.621453 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:41:19.621585 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:41:19.621712 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:41:19.622498 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m10:41:19.624722 [info ] [MainThread]: Executing prospect merge operation...
[0m10:41:19.670234 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:41:19.670462 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
      SELECT merge_similar_entities();
    
  
[0m10:41:20.935735 [debug] [MainThread]: SQL status: SELECT 1 in 1.265 seconds
[0m10:41:20.937890 [info ] [MainThread]: Merge operation completed successfully
[0m10:41:20.938543 [debug] [MainThread]: On macro_merge_entities: Close
[0m10:41:20.946417 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:41:20.948857 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.6614107, "process_in_blocks": "0", "process_kernel_time": 0.157405, "process_mem_max_rss": "131596288", "process_out_blocks": "0", "process_user_time": 0.838909}
[0m10:41:20.949202 [debug] [MainThread]: Command `dbt run-operation` succeeded at 10:41:20.949154 after 1.66 seconds
[0m10:41:20.949384 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m10:41:20.949747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029b4390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cf2950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a82c10>]}
[0m10:41:20.950136 [debug] [MainThread]: Flushing usage events
[0m10:41:21.191274 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:13.600306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109514490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109582210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10958fd10>]}


============================== 10:46:13.602133 | 86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7 ==============================
[0m10:46:13.602133 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:46:13.602416 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'fail_fast': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'target_path': 'None', 'debug': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'use_colors': 'True', 'empty': 'False', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'quiet': 'False', 'printer_width': '80'}
[0m10:46:13.683705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108243310>]}
[0m10:46:13.712479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b59d90>]}
[0m10:46:13.712891 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:46:13.756378 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:46:13.813929 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:46:13.814338 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m10:46:13.963290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4491d0>]}
[0m10:46:14.002389 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:46:14.003151 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:46:14.009318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4cee10>]}
[0m10:46:14.009549 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:46:14.009711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3ff450>]}
[0m10:46:14.010743 [info ] [MainThread]: 
[0m10:46:14.010902 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:46:14.011017 [info ] [MainThread]: 
[0m10:46:14.011205 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:46:14.013134 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.013375 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.013582 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.017732 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.018104 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.019119 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:46:14.040551 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.040723 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.040875 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.041016 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.041138 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.041249 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.041361 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:46:14.041492 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.041613 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.041732 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.041842 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.041951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.042065 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:46:14.042171 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.042277 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.042378 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.042478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.042708 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:14.076020 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.034 seconds
[0m10:46:14.076635 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.077643 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.035 seconds
[0m10:46:14.078159 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.078782 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.036 seconds
[0m10:46:14.078948 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.037 seconds
[0m10:46:14.079367 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.079756 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.079899 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.038 seconds
[0m10:46:14.080027 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.038 seconds
[0m10:46:14.080571 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.080939 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:46:14.081973 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m10:46:14.082167 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m10:46:14.084498 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:46:14.084699 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m10:46:14.084878 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m10:46:14.085065 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m10:46:14.085250 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m10:46:14.085934 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:46:14.086089 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:46:14.086731 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:46:14.087356 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:46:14.088294 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:46:14.088915 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:46:14.089037 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:46:14.089146 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.089261 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:46:14.089375 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:46:14.089480 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:46:14.089585 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:46:14.089694 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.089873 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.090001 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.090123 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.090247 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:14.096427 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m10:46:14.096586 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:46:14.096746 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:46:14.098226 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.001 seconds
[0m10:46:14.098803 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:46:14.099185 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:46:14.104015 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:46:14.104219 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:46:14.104387 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:46:14.105464 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.105767 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:46:14.105998 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.001 seconds
[0m10:46:14.106188 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.106381 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:46:14.106549 [debug] [ThreadPool]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.106652 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:46:14.107081 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:46:14.107298 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:46:14.107601 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:46:14.107773 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:46:14.107965 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:46:14.108144 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:46:14.108307 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:46:14.108494 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:46:14.108987 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.001 seconds
[0m10:46:14.109506 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:46:14.109888 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:46:14.110055 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m10:46:14.110268 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m10:46:14.110420 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.001 seconds
[0m10:46:14.111635 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:46:14.112251 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:46:14.112658 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:46:14.113119 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:46:14.113325 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:46:14.113461 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:46:14.117010 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.117147 [debug] [MainThread]: On master: BEGIN
[0m10:46:14.117249 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:46:14.123757 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:46:14.124001 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.124198 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:46:14.126315 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:46:14.127919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afd95d0>]}
[0m10:46:14.128212 [debug] [MainThread]: On master: ROLLBACK
[0m10:46:14.128642 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.128772 [debug] [MainThread]: On master: BEGIN
[0m10:46:14.129207 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:46:14.129355 [debug] [MainThread]: On master: COMMIT
[0m10:46:14.129521 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.129640 [debug] [MainThread]: On master: COMMIT
[0m10:46:14.129958 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.130153 [debug] [MainThread]: On master: Close
[0m10:46:14.132311 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m10:46:14.132529 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m10:46:14.132847 [info ] [Thread-1 (]: 1 of 18 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m10:46:14.133163 [info ] [Thread-2 (]: 2 of 18 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m10:46:14.133426 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.raw_fxf_data)
[0m10:46:14.133703 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.raw_pdl_data)
[0m10:46:14.133963 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m10:46:14.134130 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m10:46:14.137310 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m10:46:14.138355 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m10:46:14.138743 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m10:46:14.138939 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m10:46:14.154599 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m10:46:14.155888 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m10:46:14.156426 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.156717 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.156926 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m10:46:14.157123 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m10:46:14.157311 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:46:14.157440 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:14.168170 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m10:46:14.168646 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.168863 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m10:46:14.169118 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m10:46:14.169339 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.169490 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m10:46:14.195565 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.026 seconds
[0m10:46:14.200133 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.200345 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m10:46:14.200952 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.201143 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.031 seconds
[0m10:46:14.202262 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.203448 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.203612 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m10:46:14.203768 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m10:46:14.204271 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.210628 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m10:46:14.210843 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:46:14.211984 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.212149 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.212299 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m10:46:14.212460 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m10:46:14.213083 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.213663 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:46:14.213845 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.213991 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m10:46:14.216919 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m10:46:14.217092 [debug] [Thread-2 (]: SQL status: COMMIT in 0.003 seconds
[0m10:46:14.220032 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m10:46:14.221164 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m10:46:14.223292 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m10:46:14.223580 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m10:46:14.223768 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m10:46:14.223932 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m10:46:14.225701 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:46:14.225870 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:46:14.226913 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m10:46:14.227426 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m10:46:14.228692 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c762a90>]}
[0m10:46:14.228846 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c34e850>]}
[0m10:46:14.229160 [info ] [Thread-2 (]: 2 of 18 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.09s]
[0m10:46:14.229445 [info ] [Thread-1 (]: 1 of 18 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.09s]
[0m10:46:14.229686 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m10:46:14.229910 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m10:46:14.230221 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_pdl_data
[0m10:46:14.230437 [debug] [Thread-6 (]: Began running node model.dbt_service.raw_company_profiling
[0m10:46:14.230743 [debug] [Thread-7 (]: Began running node model.dbt_service.raw_data_profiling
[0m10:46:14.230611 [info ] [Thread-4 (]: 3 of 18 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m10:46:14.230925 [debug] [Thread-8 (]: Began running node model.dbt_service.raw_location_profiling
[0m10:46:14.231095 [debug] [Thread-9 (]: Began running node model.dbt_service.stg_fxf_data
[0m10:46:14.231321 [info ] [Thread-6 (]: 4 of 18 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m10:46:14.231522 [info ] [Thread-7 (]: 5 of 18 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m10:46:14.231717 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_pdl_data)
[0m10:46:14.231900 [info ] [Thread-8 (]: 6 of 18 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m10:46:14.232127 [info ] [Thread-9 (]: 7 of 18 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m10:46:14.232317 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_company_profiling)
[0m10:46:14.232522 [debug] [Thread-7 (]: Acquiring new postgres connection 'model.dbt_service.raw_data_profiling'
[0m10:46:14.232677 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m10:46:14.232856 [debug] [Thread-8 (]: Acquiring new postgres connection 'model.dbt_service.raw_location_profiling'
[0m10:46:14.233055 [debug] [Thread-9 (]: Acquiring new postgres connection 'model.dbt_service.stg_fxf_data'
[0m10:46:14.233212 [debug] [Thread-6 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m10:46:14.233358 [debug] [Thread-7 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m10:46:14.234794 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m10:46:14.234955 [debug] [Thread-8 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m10:46:14.235100 [debug] [Thread-9 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m10:46:14.236511 [debug] [Thread-6 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m10:46:14.237930 [debug] [Thread-7 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m10:46:14.239687 [debug] [Thread-8 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m10:46:14.242351 [debug] [Thread-9 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m10:46:14.242614 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_pdl_data
[0m10:46:14.242890 [debug] [Thread-6 (]: Began executing node model.dbt_service.raw_company_profiling
[0m10:46:14.249663 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m10:46:14.249823 [debug] [Thread-7 (]: Began executing node model.dbt_service.raw_data_profiling
[0m10:46:14.249958 [debug] [Thread-8 (]: Began executing node model.dbt_service.raw_location_profiling
[0m10:46:14.251208 [debug] [Thread-6 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m10:46:14.251407 [debug] [Thread-9 (]: Began executing node model.dbt_service.stg_fxf_data
[0m10:46:14.252495 [debug] [Thread-7 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m10:46:14.253744 [debug] [Thread-8 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m10:46:14.253960 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:46:14.255131 [debug] [Thread-9 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m10:46:14.255289 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.255500 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m10:46:14.255734 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m10:46:14.255892 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.256061 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.256253 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m10:46:14.256420 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:46:14.256565 [debug] [Thread-6 (]: Opening a new connection, currently in state closed
[0m10:46:14.256713 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m10:46:14.256870 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m10:46:14.257094 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m10:46:14.257298 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m10:46:14.257444 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m10:46:14.257583 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m10:46:14.270070 [debug] [Thread-4 (]: SQL status: BEGIN in 0.014 seconds
[0m10:46:14.270271 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:46:14.270448 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m10:46:14.271121 [debug] [Thread-6 (]: SQL status: BEGIN in 0.015 seconds
[0m10:46:14.271328 [debug] [Thread-8 (]: SQL status: BEGIN in 0.014 seconds
[0m10:46:14.271518 [debug] [Thread-9 (]: SQL status: BEGIN in 0.014 seconds
[0m10:46:14.271708 [debug] [Thread-7 (]: SQL status: BEGIN in 0.014 seconds
[0m10:46:14.271878 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.272027 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:46:14.272180 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.272344 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:46:14.272475 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.272669 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:46:14.274027 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:46:14.274292 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m10:46:14.274575 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m10:46:14.274792 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m10:46:14.275010 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m10:46:14.275685 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.276219 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:46:14.276412 [debug] [Thread-9 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:46:14.276709 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:46:14.277823 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:46:14.277975 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m10:46:14.278160 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m10:46:14.278789 [debug] [Thread-9 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.279323 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:46:14.279490 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:46:14.279632 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m10:46:14.281557 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m10:46:14.281745 [debug] [Thread-9 (]: SQL status: COMMIT in 0.002 seconds
[0m10:46:14.283365 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m10:46:14.284447 [debug] [Thread-9 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m10:46:14.285691 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m10:46:14.285924 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m10:46:14.286100 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m10:46:14.286275 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m10:46:14.286856 [debug] [Thread-9 (]: SQL status: DROP VIEW in 0.000 seconds
[0m10:46:14.287025 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.001 seconds
[0m10:46:14.287809 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: Close
[0m10:46:14.288459 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: Close
[0m10:46:14.288804 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d649390>]}
[0m10:46:14.289063 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d058550>]}
[0m10:46:14.289471 [info ] [Thread-9 (]: 7 of 18 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m10:46:14.289785 [info ] [Thread-4 (]: 3 of 18 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m10:46:14.290164 [debug] [Thread-9 (]: Finished running node model.dbt_service.stg_fxf_data
[0m10:46:14.290441 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_pdl_data
[0m10:46:14.290842 [debug] [Thread-11 ]: Began running node model.dbt_service.company_analysis
[0m10:46:14.291038 [debug] [Thread-12 ]: Began running node model.dbt_service.staging_city_state_profiling
[0m10:46:14.291399 [debug] [Thread-13 ]: Began running node model.dbt_service.staging_company_profiling
[0m10:46:14.291587 [debug] [Thread-14 ]: Began running node model.dbt_service.staging_data_profiling
[0m10:46:14.291255 [info ] [Thread-11 ]: 8 of 18 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m10:46:14.291794 [debug] [Thread-15 ]: Began running node model.dbt_service.staging_location_profiling
[0m10:46:14.291971 [debug] [Thread-16 ]: Began running node model.dbt_service.stg_unified_prospects
[0m10:46:14.292195 [info ] [Thread-12 ]: 9 of 18 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m10:46:14.292457 [info ] [Thread-13 ]: 10 of 18 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m10:46:14.292668 [info ] [Thread-14 ]: 11 of 18 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m10:46:14.292979 [debug] [Thread-11 ]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m10:46:14.293228 [info ] [Thread-15 ]: 12 of 18 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m10:46:14.293480 [info ] [Thread-16 ]: 13 of 18 START sql table model public_staging.stg_unified_prospects ............ [RUN]
[0m10:46:14.293717 [debug] [Thread-12 ]: Acquiring new postgres connection 'model.dbt_service.staging_city_state_profiling'
[0m10:46:14.293925 [debug] [Thread-13 ]: Acquiring new postgres connection 'model.dbt_service.staging_company_profiling'
[0m10:46:14.294211 [debug] [Thread-14 ]: Acquiring new postgres connection 'model.dbt_service.staging_data_profiling'
[0m10:46:14.294397 [debug] [Thread-11 ]: Began compiling node model.dbt_service.company_analysis
[0m10:46:14.294630 [debug] [Thread-15 ]: Acquiring new postgres connection 'model.dbt_service.staging_location_profiling'
[0m10:46:14.294850 [debug] [Thread-16 ]: Acquiring new postgres connection 'model.dbt_service.stg_unified_prospects'
[0m10:46:14.295017 [debug] [Thread-12 ]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m10:46:14.295185 [debug] [Thread-13 ]: Began compiling node model.dbt_service.staging_company_profiling
[0m10:46:14.295337 [debug] [Thread-14 ]: Began compiling node model.dbt_service.staging_data_profiling
[0m10:46:14.297191 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m10:46:14.297419 [debug] [Thread-15 ]: Began compiling node model.dbt_service.staging_location_profiling
[0m10:46:14.297571 [debug] [Thread-16 ]: Began compiling node model.dbt_service.stg_unified_prospects
[0m10:46:14.299086 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.300479 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m10:46:14.301774 [debug] [Thread-14 ]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m10:46:14.303445 [debug] [Thread-15 ]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m10:46:14.304918 [debug] [Thread-16 ]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m10:46:14.305217 [debug] [Thread-11 ]: Began executing node model.dbt_service.company_analysis
[0m10:46:14.305483 [debug] [Thread-12 ]: Began executing node model.dbt_service.staging_city_state_profiling
[0m10:46:14.305695 [debug] [Thread-13 ]: Began executing node model.dbt_service.staging_company_profiling
[0m10:46:14.305869 [debug] [Thread-14 ]: Began executing node model.dbt_service.staging_data_profiling
[0m10:46:14.306026 [debug] [Thread-15 ]: Began executing node model.dbt_service.staging_location_profiling
[0m10:46:14.307509 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m10:46:14.308780 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.308931 [debug] [Thread-16 ]: Began executing node model.dbt_service.stg_unified_prospects
[0m10:46:14.311063 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m10:46:14.312323 [debug] [Thread-14 ]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m10:46:14.313518 [debug] [Thread-15 ]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m10:46:14.314891 [debug] [Thread-16 ]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m10:46:14.315124 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.315307 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.315707 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.315881 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.316127 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: BEGIN
[0m10:46:14.316319 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.316496 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m10:46:14.316643 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.316793 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: BEGIN
[0m10:46:14.316949 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: BEGIN
[0m10:46:14.317096 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m10:46:14.317243 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: BEGIN
[0m10:46:14.317379 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m10:46:14.317535 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m10:46:14.317693 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m10:46:14.317852 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m10:46:14.318056 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m10:46:14.318247 [debug] [Thread-16 ]: Opening a new connection, currently in state init
[0m10:46:14.333265 [debug] [Thread-14 ]: SQL status: BEGIN in 0.015 seconds
[0m10:46:14.333516 [debug] [Thread-11 ]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.333717 [debug] [Thread-15 ]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.333876 [debug] [Thread-12 ]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.334000 [debug] [Thread-13 ]: SQL status: BEGIN in 0.016 seconds
[0m10:46:14.334240 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.334445 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.334636 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.334830 [debug] [Thread-16 ]: SQL status: BEGIN in 0.017 seconds
[0m10:46:14.335009 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.335184 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.335422 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m10:46:14.335713 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m10:46:14.336012 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m10:46:14.336248 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.336474 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m10:46:14.336782 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m10:46:14.337171 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m10:46:14.373428 [debug] [Thread-6 (]: SQL status: SELECT 180 in 0.098 seconds
[0m10:46:14.376242 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.376477 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m10:46:14.378140 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.379445 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.379612 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m10:46:14.380900 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.382198 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:46:14.382621 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.382946 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m10:46:14.386769 [debug] [Thread-6 (]: SQL status: COMMIT in 0.003 seconds
[0m10:46:14.389668 [debug] [Thread-6 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m10:46:14.390303 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m10:46:14.390788 [debug] [Thread-7 (]: SQL status: SELECT 2 in 0.115 seconds
[0m10:46:14.391247 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m10:46:14.393335 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.393812 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m10:46:14.394922 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.395350 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.002 seconds
[0m10:46:14.397500 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.398157 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: Close
[0m10:46:14.398409 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m10:46:14.399124 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c79c510>]}
[0m10:46:14.400301 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.399966 [info ] [Thread-6 (]: 4 of 18 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.17s]
[0m10:46:14.401386 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:46:14.401675 [debug] [Thread-6 (]: Finished running node model.dbt_service.raw_company_profiling
[0m10:46:14.401849 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.402083 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m10:46:14.403135 [debug] [Thread-7 (]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.406898 [debug] [Thread-7 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m10:46:14.407872 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m10:46:14.408514 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m10:46:14.410662 [debug] [Thread-7 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.412404 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: Close
[0m10:46:14.413542 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d0ca910>]}
[0m10:46:14.414377 [info ] [Thread-7 (]: 5 of 18 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.18s]
[0m10:46:14.415065 [debug] [Thread-7 (]: Finished running node model.dbt_service.raw_data_profiling
[0m10:46:14.458053 [debug] [Thread-8 (]: SQL status: SELECT 152 in 0.183 seconds
[0m10:46:14.460351 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.460539 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m10:46:14.461164 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.462268 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.462420 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m10:46:14.462970 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.463513 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:46:14.463673 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.463812 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m10:46:14.464582 [debug] [Thread-8 (]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.465549 [debug] [Thread-8 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m10:46:14.465783 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m10:46:14.465940 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m10:46:14.466946 [debug] [Thread-8 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.467527 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: Close
[0m10:46:14.467802 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c3b5590>]}
[0m10:46:14.468073 [info ] [Thread-8 (]: 6 of 18 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.23s]
[0m10:46:14.468305 [debug] [Thread-8 (]: Finished running node model.dbt_service.raw_location_profiling
[0m10:46:14.492320 [debug] [Thread-12 ]: SQL status: SELECT 76 in 0.155 seconds
[0m10:46:14.493868 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.494051 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m10:46:14.494274 [debug] [Thread-11 ]: SQL status: SELECT 91 in 0.157 seconds
[0m10:46:14.495429 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.495590 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.495783 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m10:46:14.496964 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.497165 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m10:46:14.497472 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.497621 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.498766 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.499283 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:46:14.499449 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m10:46:14.499605 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.499807 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m10:46:14.500064 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.500587 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:46:14.500760 [debug] [Thread-12 ]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.500904 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.501821 [debug] [Thread-12 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m10:46:14.501982 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m10:46:14.502210 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m10:46:14.502387 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m10:46:14.502810 [debug] [Thread-11 ]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.503771 [debug] [Thread-11 ]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m10:46:14.503919 [debug] [Thread-12 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.504143 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m10:46:14.504737 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: Close
[0m10:46:14.504979 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m10:46:14.505298 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d091790>]}
[0m10:46:14.505586 [info ] [Thread-12 ]: 9 of 18 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.21s]
[0m10:46:14.505822 [debug] [Thread-12 ]: Finished running node model.dbt_service.staging_city_state_profiling
[0m10:46:14.506036 [debug] [Thread-11 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.506550 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: Close
[0m10:46:14.506757 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f5350>]}
[0m10:46:14.506988 [info ] [Thread-11 ]: 8 of 18 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.21s]
[0m10:46:14.507200 [debug] [Thread-11 ]: Finished running node model.dbt_service.company_analysis
[0m10:46:14.508887 [debug] [Thread-13 ]: SQL status: SELECT 180 in 0.171 seconds
[0m10:46:14.510216 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.510378 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m10:46:14.510852 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.511930 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.512140 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m10:46:14.512655 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.513185 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:46:14.513341 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.513485 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m10:46:14.514057 [debug] [Thread-13 ]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.516275 [debug] [Thread-13 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m10:46:14.516607 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m10:46:14.516776 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m10:46:14.517801 [debug] [Thread-13 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.518325 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: Close
[0m10:46:14.518582 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d733b10>]}
[0m10:46:14.518886 [info ] [Thread-13 ]: 10 of 18 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.22s]
[0m10:46:14.519189 [debug] [Thread-13 ]: Finished running node model.dbt_service.staging_company_profiling
[0m10:46:14.602140 [debug] [Thread-14 ]: SQL status: SELECT 2 in 0.265 seconds
[0m10:46:14.603708 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.603881 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m10:46:14.604481 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.605625 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.605775 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m10:46:14.606210 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.606721 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:46:14.606874 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.607014 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m10:46:14.607941 [debug] [Thread-14 ]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.608870 [debug] [Thread-14 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m10:46:14.609100 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m10:46:14.609263 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m10:46:14.610286 [debug] [Thread-14 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.610784 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: Close
[0m10:46:14.611040 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7751d0>]}
[0m10:46:14.611307 [info ] [Thread-14 ]: 11 of 18 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.32s]
[0m10:46:14.611591 [debug] [Thread-14 ]: Finished running node model.dbt_service.staging_data_profiling
[0m10:46:14.622522 [debug] [Thread-15 ]: SQL status: SELECT 152 in 0.285 seconds
[0m10:46:14.624076 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.624357 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m10:46:14.624994 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.626141 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.626277 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m10:46:14.626681 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.627094 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:46:14.627234 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.627363 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m10:46:14.627961 [debug] [Thread-15 ]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.628859 [debug] [Thread-15 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m10:46:14.629071 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m10:46:14.629216 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m10:46:14.630078 [debug] [Thread-15 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.630528 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: Close
[0m10:46:14.630774 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f5410>]}
[0m10:46:14.631026 [info ] [Thread-15 ]: 12 of 18 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.34s]
[0m10:46:14.631253 [debug] [Thread-15 ]: Finished running node model.dbt_service.staging_location_profiling
[0m10:46:14.681589 [debug] [Thread-16 ]: SQL status: SELECT 100010 in 0.344 seconds
[0m10:46:14.683443 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.683838 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m10:46:14.684598 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.685795 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.685979 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m10:46:14.686415 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.686822 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:46:14.686951 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.687069 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m10:46:14.692469 [debug] [Thread-16 ]: SQL status: COMMIT in 0.005 seconds
[0m10:46:14.693774 [debug] [Thread-16 ]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m10:46:14.694002 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m10:46:14.694143 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m10:46:14.696455 [debug] [Thread-16 ]: SQL status: DROP TABLE in 0.002 seconds
[0m10:46:14.696962 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: Close
[0m10:46:14.697194 [debug] [Thread-16 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d0d6810>]}
[0m10:46:14.697455 [info ] [Thread-16 ]: 13 of 18 OK created sql table model public_staging.stg_unified_prospects ....... [[32mSELECT 100010[0m in 0.40s]
[0m10:46:14.697669 [debug] [Thread-16 ]: Finished running node model.dbt_service.stg_unified_prospects
[0m10:46:14.698358 [debug] [Thread-18 ]: Began running node model.dbt_service.stg_prospect_matches
[0m10:46:14.699039 [info ] [Thread-18 ]: 14 of 18 START sql incremental model public_staging.stg_prospect_matches ....... [RUN]
[0m10:46:14.699419 [debug] [Thread-18 ]: Acquiring new postgres connection 'model.dbt_service.stg_prospect_matches'
[0m10:46:14.699583 [debug] [Thread-18 ]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:46:14.703060 [debug] [Thread-18 ]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:46:14.703534 [debug] [Thread-18 ]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:46:14.756512 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.756840 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp104614753926"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 and  -- Lower threshold
    similarity(a.email, b.email) > 0.4   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m10:46:14.757054 [debug] [Thread-18 ]: Opening a new connection, currently in state init
[0m10:46:14.795971 [debug] [Thread-18 ]: SQL status: SELECT 0 in 0.039 seconds
[0m10:46:14.803187 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.803428 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:46:14.804051 [debug] [Thread-18 ]: SQL status: BEGIN in 0.000 seconds
[0m10:46:14.804261 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.804443 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp104614753926'
        
      order by ordinal_position

  
[0m10:46:14.807856 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.003 seconds
[0m10:46:14.810430 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.810680 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:46:14.812382 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.002 seconds
[0m10:46:14.818362 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.818628 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp104614753926'
        
      order by ordinal_position

  
[0m10:46:14.820355 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.002 seconds
[0m10:46:14.821828 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.822015 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m10:46:14.823615 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m10:46:14.828137 [debug] [Thread-18 ]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:46:14.837557 [debug] [Thread-18 ]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:46:14.838108 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.838281 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp104614753926" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp104614753926"
    )
  
[0m10:46:14.839215 [debug] [Thread-18 ]: SQL status: INSERT 0 0 in 0.001 seconds
[0m10:46:14.839864 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:46:14.840055 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:46:14.840215 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m10:46:14.840897 [debug] [Thread-18 ]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.841223 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: Close
[0m10:46:14.841504 [debug] [Thread-18 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d0fe750>]}
[0m10:46:14.841798 [info ] [Thread-18 ]: 14 of 18 OK created sql incremental model public_staging.stg_prospect_matches .. [[32mINSERT 0 0[0m in 0.14s]
[0m10:46:14.842038 [debug] [Thread-18 ]: Finished running node model.dbt_service.stg_prospect_matches
[0m10:46:14.842419 [debug] [Thread-20 ]: Began running node model.dbt_service.prospect_matching_ratio
[0m10:46:14.842598 [debug] [Thread-21 ]: Began running node model.dbt_service.stg_entity_clusters
[0m10:46:14.842882 [info ] [Thread-20 ]: 15 of 18 START sql view model public_marts.prospect_matching_ratio ............. [RUN]
[0m10:46:14.843124 [info ] [Thread-21 ]: 16 of 18 START sql table model public_staging.stg_entity_clusters .............. [RUN]
[0m10:46:14.843375 [debug] [Thread-20 ]: Acquiring new postgres connection 'model.dbt_service.prospect_matching_ratio'
[0m10:46:14.843574 [debug] [Thread-21 ]: Acquiring new postgres connection 'model.dbt_service.stg_entity_clusters'
[0m10:46:14.843743 [debug] [Thread-20 ]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m10:46:14.843914 [debug] [Thread-21 ]: Began compiling node model.dbt_service.stg_entity_clusters
[0m10:46:14.845834 [debug] [Thread-20 ]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.847264 [debug] [Thread-21 ]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m10:46:14.847671 [debug] [Thread-21 ]: Began executing node model.dbt_service.stg_entity_clusters
[0m10:46:14.847825 [debug] [Thread-20 ]: Began executing node model.dbt_service.prospect_matching_ratio
[0m10:46:14.849287 [debug] [Thread-21 ]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m10:46:14.851703 [debug] [Thread-20 ]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.855355 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.855602 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m10:46:14.855782 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.855939 [debug] [Thread-20 ]: Opening a new connection, currently in state init
[0m10:46:14.856093 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m10:46:14.856330 [debug] [Thread-21 ]: Opening a new connection, currently in state init
[0m10:46:14.865144 [debug] [Thread-21 ]: SQL status: BEGIN in 0.009 seconds
[0m10:46:14.865437 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.865628 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged
-- Results:

-- 87 duplicate pairs identified from high-confidence matches (score > 0.8)
-- Canonical ID logic: Uses the higher prospect_id as canonical (keeps the "later" record)
-- Merged ID logic: Lower prospect_id will be marked as duplicate
-- Range: Processing prospects from 658 to 9999
-- How it works:

-- Example: Prospect 658 is canonical, Prospect 291 should be merged into it
-- etc.



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m10:46:14.867017 [debug] [Thread-20 ]: SQL status: BEGIN in 0.011 seconds
[0m10:46:14.867205 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.867420 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m10:46:14.868289 [debug] [Thread-21 ]: SQL status: SELECT 87 in 0.002 seconds
[0m10:46:14.870047 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.870204 [debug] [Thread-20 ]: SQL status: CREATE VIEW in 0.003 seconds
[0m10:46:14.870373 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters" rename to "stg_entity_clusters__dbt_backup"
[0m10:46:14.871685 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.871908 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m10:46:14.872256 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.873464 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.873659 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:46:14.873874 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m10:46:14.874557 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:46:14.874770 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.874928 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m10:46:14.875147 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.875602 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:46:14.875759 [debug] [Thread-20 ]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.875903 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.877092 [debug] [Thread-20 ]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m10:46:14.877275 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m10:46:14.877553 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m10:46:14.877757 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m10:46:14.878210 [debug] [Thread-20 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:46:14.878391 [debug] [Thread-21 ]: SQL status: COMMIT in 0.001 seconds
[0m10:46:14.879072 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: Close
[0m10:46:14.880238 [debug] [Thread-21 ]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m10:46:14.880615 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m10:46:14.880950 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m10:46:14.881123 [debug] [Thread-20 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d75cf90>]}
[0m10:46:14.881463 [info ] [Thread-20 ]: 15 of 18 OK created sql view model public_marts.prospect_matching_ratio ........ [[32mCREATE VIEW[0m in 0.04s]
[0m10:46:14.881715 [debug] [Thread-20 ]: Finished running node model.dbt_service.prospect_matching_ratio
[0m10:46:14.882465 [debug] [Thread-21 ]: SQL status: DROP TABLE in 0.001 seconds
[0m10:46:14.883079 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: Close
[0m10:46:14.883317 [debug] [Thread-21 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a521e10>]}
[0m10:46:14.883579 [info ] [Thread-21 ]: 16 of 18 OK created sql table model public_staging.stg_entity_clusters ......... [[32mSELECT 87[0m in 0.04s]
[0m10:46:14.883806 [debug] [Thread-21 ]: Finished running node model.dbt_service.stg_entity_clusters
[0m10:46:14.884114 [debug] [Thread-23 ]: Began running node model.dbt_service.data_overview
[0m10:46:14.884300 [debug] [Thread-24 ]: Began running node model.dbt_service.setup_functions
[0m10:46:14.884552 [info ] [Thread-23 ]: 17 of 18 START sql view model public_marts.data_overview ....................... [RUN]
[0m10:46:14.884761 [info ] [Thread-24 ]: 18 of 18 START sql view model public.setup_functions ........................... [RUN]
[0m10:46:14.884994 [debug] [Thread-23 ]: Acquiring new postgres connection 'model.dbt_service.data_overview'
[0m10:46:14.885196 [debug] [Thread-24 ]: Acquiring new postgres connection 'model.dbt_service.setup_functions'
[0m10:46:14.885354 [debug] [Thread-23 ]: Began compiling node model.dbt_service.data_overview
[0m10:46:14.885502 [debug] [Thread-24 ]: Began compiling node model.dbt_service.setup_functions
[0m10:46:14.887304 [debug] [Thread-23 ]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m10:46:14.888474 [debug] [Thread-24 ]: Writing injected SQL for node "model.dbt_service.setup_functions"
[0m10:46:14.888873 [debug] [Thread-23 ]: Began executing node model.dbt_service.data_overview
[0m10:46:14.889081 [debug] [Thread-24 ]: Began executing node model.dbt_service.setup_functions
[0m10:46:14.890778 [debug] [Thread-23 ]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m10:46:14.892196 [debug] [Thread-24 ]: Writing runtime sql for node "model.dbt_service.setup_functions"
[0m10:46:14.892554 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:46:14.892712 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.892860 [debug] [Thread-23 ]: On model.dbt_service.data_overview: BEGIN
[0m10:46:14.893002 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: BEGIN
[0m10:46:14.893130 [debug] [Thread-23 ]: Opening a new connection, currently in state init
[0m10:46:14.893254 [debug] [Thread-24 ]: Opening a new connection, currently in state init
[0m10:46:14.899830 [debug] [Thread-23 ]: SQL status: BEGIN in 0.007 seconds
[0m10:46:14.900016 [debug] [Thread-24 ]: SQL status: BEGIN in 0.007 seconds
[0m10:46:14.900190 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:46:14.900343 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.900521 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m10:46:14.900711 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

  create view "finny_db"."public"."setup_functions__dbt_tmp"
    
    
  as (
    -- Database function setup for deduplication pipeline
-- This model ensures the merge_similar_entities() function exists in the database



select 'Function merge_similar_entities() created' as status
  );
[0m10:46:14.901589 [debug] [Thread-24 ]: SQL status: CREATE VIEW in 0.001 seconds
[0m10:46:14.903065 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.903208 [debug] [Thread-23 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m10:46:14.903364 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions" rename to "setup_functions__dbt_backup"
[0m10:46:14.905537 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:46:14.905732 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m10:46:14.906239 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:46:14.906453 [debug] [Thread-23 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.907689 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.908240 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m10:46:14.908386 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions__dbt_tmp" rename to "setup_functions"
[0m10:46:14.908522 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:46:14.908738 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m10:46:14.909020 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m10:46:14.911377 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.911554 [debug] [Thread-23 ]: SQL status: COMMIT in 0.003 seconds
[0m10:46:14.911805 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

        CREATE OR REPLACE FUNCTION merge_similar_entities()
  RETURNS VOID
  LANGUAGE plpgsql
  AS $$
  DECLARE
      rec RECORD;
      merge_count INTEGER := 0;
  BEGIN
      FOR rec IN
          SELECT canonical_id, merged_id
          FROM "finny_db"."public_staging"."stg_entity_clusters"
      LOOP
          -- Lock both entities to prevent race conditions
          PERFORM * FROM "finny_db"."public_staging"."stg_unified_prospects"
          WHERE prospect_id IN (rec.merged_id, rec.canonical_id) FOR UPDATE;

          -- Update canonical record fields only if null (fill in missing data)
          UPDATE "finny_db"."public_staging"."stg_unified_prospects" p
          SET
              name = COALESCE(p.name, s.name),
              email = COALESCE(p.email, s.email),
              company = COALESCE(p.company, s.company),
              title = COALESCE(p.title, s.title),
              city = COALESCE(p.city, s.city),
              state = COALESCE(p.state, s.state),
              company_revenue = COALESCE(p.company_revenue, s.company_revenue)
          FROM "finny_db"."public_staging"."stg_unified_prospects" s
          WHERE p.prospect_id = rec.canonical_id
            AND s.prospect_id = rec.merged_id;

          -- Mark merged record
          UPDATE "finny_db"."public_staging"."stg_unified_prospects"
          SET status = 'merged'
          WHERE prospect_id = rec.merged_id;

          merge_count := merge_count + 1;
      END LOOP;

      RAISE NOTICE 'Merged % entities', merge_count;
  END;
  $$;
      
[0m10:46:14.913028 [debug] [Thread-23 ]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m10:46:14.913314 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m10:46:14.913465 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m10:46:14.913838 [debug] [Thread-23 ]: SQL status: DROP VIEW in 0.000 seconds
[0m10:46:14.914338 [debug] [Thread-23 ]: On model.dbt_service.data_overview: Close
[0m10:46:14.914502 [debug] [Thread-24 ]: SQL status: CREATE FUNCTION in 0.001 seconds
[0m10:46:14.914864 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m10:46:14.915103 [debug] [Thread-23 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d765650>]}
[0m10:46:14.915281 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.915711 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m10:46:14.915589 [info ] [Thread-23 ]: 17 of 18 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.03s]
[0m10:46:14.916024 [debug] [Thread-23 ]: Finished running node model.dbt_service.data_overview
[0m10:46:14.916321 [debug] [Thread-24 ]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.917316 [debug] [Thread-24 ]: Applying DROP to: "finny_db"."public"."setup_functions__dbt_backup"
[0m10:46:14.917534 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m10:46:14.917670 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
drop view if exists "finny_db"."public"."setup_functions__dbt_backup" cascade
[0m10:46:14.918416 [debug] [Thread-24 ]: SQL status: DROP VIEW in 0.001 seconds
[0m10:46:14.918920 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: Close
[0m10:46:14.919127 [debug] [Thread-24 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86e89c06-3b4a-4be7-87fc-b9cbb0f5c3e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d65ba90>]}
[0m10:46:14.919364 [info ] [Thread-24 ]: 18 of 18 OK created sql view model public.setup_functions ...................... [[32mCREATE VIEW[0m in 0.03s]
[0m10:46:14.919586 [debug] [Thread-24 ]: Finished running node model.dbt_service.setup_functions
[0m10:46:14.920542 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.920665 [debug] [MainThread]: On master: BEGIN
[0m10:46:14.920778 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:46:14.925981 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m10:46:14.926151 [debug] [MainThread]: On master: COMMIT
[0m10:46:14.926275 [debug] [MainThread]: Using postgres connection "master"
[0m10:46:14.926385 [debug] [MainThread]: On master: COMMIT
[0m10:46:14.926660 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:46:14.926777 [debug] [MainThread]: On master: Close
[0m10:46:14.926952 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:46:14.927063 [debug] [MainThread]: Connection 'model.dbt_service.raw_fxf_data' was properly closed.
[0m10:46:14.927171 [debug] [MainThread]: Connection 'model.dbt_service.raw_pdl_data' was properly closed.
[0m10:46:14.927274 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m10:46:14.927373 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m10:46:14.927473 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m10:46:14.927580 [debug] [MainThread]: Connection 'model.dbt_service.raw_company_profiling' was properly closed.
[0m10:46:14.927682 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m10:46:14.927784 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m10:46:14.927884 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m10:46:14.927977 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m10:46:14.928063 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m10:46:14.928152 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m10:46:14.928240 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m10:46:14.928327 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m10:46:14.928415 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m10:46:14.928501 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m10:46:14.928588 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m10:46:14.928689 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m10:46:14.928899 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m10:46:14.929011 [debug] [MainThread]: Connection 'model.dbt_service.setup_functions' was properly closed.
[0m10:46:14.929195 [info ] [MainThread]: 
[0m10:46:14.929328 [info ] [MainThread]: Finished running 1 incremental model, 12 table models, 5 view models in 0 hours 0 minutes and 0.92 seconds (0.92s).
[0m10:46:14.930211 [debug] [MainThread]: Command end result
[0m10:46:14.941924 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:46:14.942644 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:46:14.945175 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:46:14.945299 [info ] [MainThread]: 
[0m10:46:14.945443 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:46:14.945560 [info ] [MainThread]: 
[0m10:46:14.945677 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=18
[0m10:46:14.947508 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.3793962, "process_in_blocks": "0", "process_kernel_time": 0.203883, "process_mem_max_rss": "146685952", "process_out_blocks": "0", "process_user_time": 1.580534}
[0m10:46:14.947761 [debug] [MainThread]: Command `dbt run` succeeded at 10:46:14.947724 after 1.38 seconds
[0m10:46:14.947956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095bbcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10950e750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caead0>]}
[0m10:46:14.948126 [debug] [MainThread]: Flushing usage events
[0m10:46:15.357737 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:29.409176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111eb2fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2f310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2fa90>]}


============================== 10:46:29.410799 | ca24297e-791e-4c54-9655-d20208123b4a ==============================
[0m10:46:29.410799 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:46:29.411090 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'target_path': 'None', 'debug': 'False', 'introspect': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'invocation_command': 'dbt run-operation merge_entities', 'warn_error': 'None', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'log_cache_events': 'False', 'no_print': 'None', 'partial_parse': 'True', 'empty': 'None', 'static_parser': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80'}
[0m10:46:29.492805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca24297e-791e-4c54-9655-d20208123b4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068cbc10>]}
[0m10:46:29.521728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca24297e-791e-4c54-9655-d20208123b4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054f8290>]}
[0m10:46:29.522135 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:46:29.563118 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:46:29.618422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:46:29.618623 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:46:29.641534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca24297e-791e-4c54-9655-d20208123b4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112cece50>]}
[0m10:46:29.680596 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:46:29.681429 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:46:29.686673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca24297e-791e-4c54-9655-d20208123b4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d11050>]}
[0m10:46:29.686894 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:46:29.687050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca24297e-791e-4c54-9655-d20208123b4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138ed790>]}
[0m10:46:29.687298 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m10:46:29.687454 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:46:29.687568 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m10:46:29.687683 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:46:29.709267 [debug] [MainThread]: SQL status: BEGIN in 0.022 seconds
[0m10:46:29.709498 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:46:29.709619 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:46:29.709731 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m10:46:29.710174 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:46:29.712405 [info ] [MainThread]: Executing prospect merge operation...
[0m10:46:29.761048 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m10:46:29.761298 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
      SELECT merge_similar_entities();
    
  
[0m10:46:30.997206 [debug] [MainThread]: SQL status: SELECT 1 in 1.235 seconds
[0m10:46:30.999881 [info ] [MainThread]: Merge operation completed successfully
[0m10:46:31.000590 [debug] [MainThread]: On macro_merge_entities: Close
[0m10:46:31.009321 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:46:31.011456 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.6351805, "process_in_blocks": "0", "process_kernel_time": 0.141213, "process_mem_max_rss": "131334144", "process_out_blocks": "0", "process_user_time": 0.842107}
[0m10:46:31.012255 [debug] [MainThread]: Command `dbt run-operation` succeeded at 10:46:31.012140 after 1.64 seconds
[0m10:46:31.012471 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m10:46:31.012654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1030540d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2fd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f2f710>]}
[0m10:46:31.012992 [debug] [MainThread]: Flushing usage events
[0m10:46:31.246761 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:47:12.755316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109118e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109185950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091193d0>]}


============================== 10:47:12.757159 | 55de2f87-2add-48d1-882d-509607298e01 ==============================
[0m10:47:12.757159 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:47:12.757465 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error': 'None', 'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh', 'debug': 'False', 'target_path': 'None', 'introspect': 'True', 'fail_fast': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'log_format': 'default', 'quiet': 'False', 'printer_width': '80', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service'}
[0m10:47:12.839628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10918f910>]}
[0m10:47:12.868566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106862250>]}
[0m10:47:12.868937 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:47:12.909491 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:47:12.965333 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:47:12.965529 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:47:12.988366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ed6b50>]}
[0m10:47:13.027626 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:47:13.028466 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:47:13.034633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a379790>]}
[0m10:47:13.034855 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:47:13.035013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a33bc10>]}
[0m10:47:13.035724 [info ] [MainThread]: 
[0m10:47:13.035885 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:47:13.036013 [info ] [MainThread]: 
[0m10:47:13.036226 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:47:13.036570 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:47:13.088885 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:47:13.089118 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:47:13.089231 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.110880 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.022 seconds
[0m10:47:13.111428 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:47:13.113651 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m10:47:13.113901 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m10:47:13.116161 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:47:13.116356 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m10:47:13.116826 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m10:47:13.117017 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m10:47:13.117207 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m10:47:13.117933 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:47:13.118090 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:47:13.118772 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:47:13.119418 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:47:13.120051 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:47:13.120697 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:47:13.120821 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:47:13.120937 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:47:13.121049 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:47:13.121157 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:47:13.121261 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:47:13.121371 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:47:13.121509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.121709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.121857 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.121988 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.122130 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:47:13.130877 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m10:47:13.131023 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:47:13.131158 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:47:13.133762 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:47:13.134229 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:47:13.135364 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:47:13.135663 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:47:13.136020 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m10:47:13.136535 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:47:13.136701 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:47:13.136902 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:47:13.137071 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:47:13.137315 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:47:13.137494 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:47:13.137661 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:47:13.137785 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:47:13.137915 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:47:13.138050 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:47:13.138265 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:47:13.138442 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:47:13.138606 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:47:13.139894 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.002 seconds
[0m10:47:13.140490 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:47:13.140671 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:47:13.140916 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m10:47:13.141105 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m10:47:13.141236 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.002 seconds
[0m10:47:13.141906 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:47:13.142069 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:47:13.142763 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:47:13.143324 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:47:13.143866 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:47:13.144699 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:47:13.144825 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:47:13.144940 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:47:13.145496 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:47:13.148818 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:13.149006 [debug] [MainThread]: On master: BEGIN
[0m10:47:13.149117 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:47:13.155178 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m10:47:13.155322 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:13.155497 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:47:13.157550 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:47:13.158972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098b6bd0>]}
[0m10:47:13.159261 [debug] [MainThread]: On master: ROLLBACK
[0m10:47:13.159730 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:13.159865 [debug] [MainThread]: On master: BEGIN
[0m10:47:13.160413 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:47:13.160571 [debug] [MainThread]: On master: COMMIT
[0m10:47:13.160698 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:13.160827 [debug] [MainThread]: On master: COMMIT
[0m10:47:13.161175 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:47:13.161383 [debug] [MainThread]: On master: Close
[0m10:47:13.163253 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m10:47:13.163596 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m10:47:13.163902 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.stg_prospect_matches)
[0m10:47:13.164067 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:47:13.169274 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:47:13.169590 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:47:13.193834 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:47:13.194629 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:47:13.194793 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:47:13.194932 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:47:13.201524 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m10:47:13.201802 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:47:13.201988 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 10000  -- Increase to 10000 records
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 and  -- Lower threshold
    similarity(a.email, b.email) > 0.4   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m10:47:22.459952 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m10:47:22.462422 [debug] [MainThread]: Postgres adapter: Cancelling query 'model.dbt_service.stg_prospect_matches' (3348)
[0m10:47:22.462864 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:22.463104 [debug] [MainThread]: On master: BEGIN
[0m10:47:22.463266 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:47:22.471540 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m10:47:22.471797 [debug] [MainThread]: Using postgres connection "master"
[0m10:47:22.471954 [debug] [MainThread]: On master: select pg_terminate_backend(3348)
[0m10:47:22.472514 [debug] [MainThread]: SQL status: SELECT 1 in 0.000 seconds
[0m10:47:22.472767 [debug] [MainThread]: Postgres adapter: Cancel query 'model.dbt_service.stg_prospect_matches': (True,)
[0m10:47:22.472997 [error] [MainThread]: CANCEL query model.dbt_service.stg_prospect_matches ............................ [[31mCANCEL[0m]
[0m10:47:22.473161 [error] [MainThread]: CANCEL query list_finny_db_public .............................................. [[31mCANCEL[0m]
[0m10:47:22.473284 [error] [MainThread]: CANCEL query list_finny_db_public_raw .......................................... [[31mCANCEL[0m]
[0m10:47:22.473395 [error] [MainThread]: CANCEL query list_finny_db_public_raw_analysis ................................. [[31mCANCEL[0m]
[0m10:47:22.473502 [error] [MainThread]: CANCEL query list_finny_db_public_staging ...................................... [[31mCANCEL[0m]
[0m10:47:22.473607 [error] [MainThread]: CANCEL query list_finny_db_public_staging_analysis ............................. [[31mCANCEL[0m]
[0m10:47:22.473730 [debug] [MainThread]: On master: ROLLBACK
[0m10:47:22.474162 [debug] [MainThread]: On master: Close
[0m10:47:22.474934 [debug] [Thread-1 (]: Postgres adapter: Postgres error: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[0m10:47:22.475163 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: ROLLBACK
[0m10:47:22.476720 [debug] [Thread-1 (]: Failed to rollback 'model.dbt_service.stg_prospect_matches'
[0m10:47:22.476996 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m10:47:22.481814 [debug] [Thread-1 (]: Database Error in model stg_prospect_matches (models/staging/stg_prospect_matches.sql)
  server closed the connection unexpectedly
  	This probably means the server terminated abnormally
  	before or while processing the request.
  compiled code at target/run/dbt_service/models/staging/stg_prospect_matches.sql
[0m10:47:22.482866 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55de2f87-2add-48d1-882d-509607298e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cae3b50>]}
[0m10:47:22.483385 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model public_staging.stg_prospect_matches  [[31mERROR[0m in 9.32s]
[0m10:47:22.483760 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m10:47:22.484022 [info ] [MainThread]: 
[0m10:47:22.484284 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m10:47:22.484429 [info ] [MainThread]: 
[0m10:47:22.484580 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=0
[0m10:47:22.484713 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:47:22.484828 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m10:47:22.484934 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m10:47:22.485038 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m10:47:22.485246 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m10:47:22.485366 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m10:47:22.485473 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m10:47:22.485596 [info ] [MainThread]: 
[0m10:47:22.485729 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 9.45 seconds (9.45s).
[0m10:47:22.485931 [error] [MainThread]: Encountered an error:

[0m10:47:22.489563 [error] [MainThread]: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 373, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/requires.py", line 390, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/cli/main.py", line 587, in run
    results = task.run()
              ^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 599, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 536, in execute_with_hooks
    res = self.execute_nodes()
          ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 416, in execute_nodes
    self.run_queue(pool)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/task/runnable.py", line 343, in run_queue
    self.job_queue.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dbt/graph/queue.py", line 206, in join
    self.inner.join()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", line 327, in wait
    waiter.acquire()
KeyboardInterrupt

[0m10:47:22.493735 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.770888, "process_in_blocks": "0", "process_kernel_time": 0.177233, "process_mem_max_rss": "137150464", "process_out_blocks": "0", "process_user_time": 0.956037}
[0m10:47:22.494212 [debug] [MainThread]: Command `dbt run` failed at 10:47:22.494159 after 9.77 seconds
[0m10:47:22.494657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091bdf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd150d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10472f490>]}
[0m10:47:22.494851 [debug] [MainThread]: Flushing usage events
[0m10:47:22.714867 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:18.841237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f133d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101a79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101a7f90>]}


============================== 10:49:18.843705 | 98e8ebcb-91df-445c-b3f5-6627cd60a46f ==============================
[0m10:49:18.843705 [info ] [MainThread]: Running with dbt=1.10.13
[0m10:49:18.843996 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'use_colors': 'True', 'no_print': 'None', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select stg_prospect_matches --full-refresh', 'cache_selected_only': 'False', 'target_path': 'None', 'quiet': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'indirect_selection': 'eager', 'log_format': 'default'}
[0m10:49:18.937777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cd0f50>]}
[0m10:49:18.967259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046b8a10>]}
[0m10:49:18.968100 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:49:19.013412 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m10:49:19.076643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:49:19.076976 [debug] [MainThread]: Partial parsing: updated file: dbt_service://models/staging/stg_prospect_matches.sql
[0m10:49:19.222643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11003c390>]}
[0m10:49:19.261012 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m10:49:19.261953 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m10:49:19.272619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11185eb50>]}
[0m10:49:19.272839 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m10:49:19.272995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101f9850>]}
[0m10:49:19.273687 [info ] [MainThread]: 
[0m10:49:19.273836 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m10:49:19.273955 [info ] [MainThread]: 
[0m10:49:19.274142 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:49:19.274520 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m10:49:19.295238 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m10:49:19.295433 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m10:49:19.295544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.324567 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.029 seconds
[0m10:49:19.325129 [debug] [ThreadPool]: On list_finny_db: Close
[0m10:49:19.327295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m10:49:19.327546 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m10:49:19.329847 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:49:19.330047 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m10:49:19.330344 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m10:49:19.331189 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:49:19.331362 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m10:49:19.331558 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m10:49:19.331665 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m10:49:19.332374 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:49:19.333021 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:49:19.333142 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m10:49:19.333766 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:49:19.334395 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:49:19.334507 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:49:19.334615 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m10:49:19.334716 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m10:49:19.334816 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.334919 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m10:49:19.335022 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m10:49:19.335230 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.335355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.335572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.335729 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:19.349415 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:49:19.349553 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m10:49:19.349674 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m10:49:19.349800 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m10:49:19.349961 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m10:49:19.350129 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m10:49:19.352602 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m10:49:19.352707 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m10:49:19.352823 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m10:49:19.353724 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.003 seconds
[0m10:49:19.354280 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m10:49:19.354412 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m10:49:19.354823 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m10:49:19.356485 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m10:49:19.356668 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m10:49:19.356816 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m10:49:19.356942 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:49:19.357058 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m10:49:19.357173 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m10:49:19.357407 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m10:49:19.357882 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m10:49:19.358517 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m10:49:19.359051 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m10:49:19.359203 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m10:49:19.359378 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m10:49:19.359547 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m10:49:19.360063 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m10:49:19.361175 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.001 seconds
[0m10:49:19.361354 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m10:49:19.361550 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m10:49:19.362135 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m10:49:19.362588 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m10:49:19.362982 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m10:49:19.363527 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m10:49:19.363751 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m10:49:19.363885 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m10:49:19.368310 [debug] [MainThread]: Using postgres connection "master"
[0m10:49:19.368526 [debug] [MainThread]: On master: BEGIN
[0m10:49:19.368654 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:49:19.376816 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m10:49:19.377066 [debug] [MainThread]: Using postgres connection "master"
[0m10:49:19.377266 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:49:19.379799 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m10:49:19.381216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110df4750>]}
[0m10:49:19.381451 [debug] [MainThread]: On master: ROLLBACK
[0m10:49:19.381888 [debug] [MainThread]: Using postgres connection "master"
[0m10:49:19.382049 [debug] [MainThread]: On master: BEGIN
[0m10:49:19.382529 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:49:19.382657 [debug] [MainThread]: On master: COMMIT
[0m10:49:19.382775 [debug] [MainThread]: Using postgres connection "master"
[0m10:49:19.382886 [debug] [MainThread]: On master: COMMIT
[0m10:49:19.383163 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:49:19.383289 [debug] [MainThread]: On master: Close
[0m10:49:19.385598 [debug] [Thread-1 (]: Began running node model.dbt_service.stg_prospect_matches
[0m10:49:19.386010 [info ] [Thread-1 (]: 1 of 1 START sql incremental model public_staging.stg_prospect_matches ......... [RUN]
[0m10:49:19.386268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.stg_prospect_matches)
[0m10:49:19.386445 [debug] [Thread-1 (]: Began compiling node model.dbt_service.stg_prospect_matches
[0m10:49:19.391529 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m10:49:19.391871 [debug] [Thread-1 (]: Began executing node model.dbt_service.stg_prospect_matches
[0m10:49:19.412284 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m10:49:19.412675 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:49:19.412824 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m10:49:19.412956 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:49:19.419426 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m10:49:19.419696 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m10:49:19.419898 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
  
    

  create  table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 2000000  -- Increase to 10000 records
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 and  -- Lower threshold
    similarity(a.email, b.email) > 0.4   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m13:00:38.067924 [debug] [Thread-1 (]: SQL status: SELECT 113771 in 6956.143 seconds
[0m13:00:38.086209 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m13:00:38.086556 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches" rename to "stg_prospect_matches__dbt_backup"
[0m13:00:38.088297 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:00:38.090190 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m13:00:38.090489 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
alter table "finny_db"."public_staging"."stg_prospect_matches__dbt_tmp" rename to "stg_prospect_matches"
[0m13:00:38.091216 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m13:00:38.102586 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m13:00:38.102874 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m13:00:38.103096 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m13:00:38.104019 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m13:00:38.107573 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_staging"."stg_prospect_matches__dbt_backup"
[0m13:00:38.110272 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m13:00:38.110513 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */
drop table if exists "finny_db"."public_staging"."stg_prospect_matches__dbt_backup" cascade
[0m13:00:38.113054 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.002 seconds
[0m13:00:38.115287 [debug] [Thread-1 (]: On model.dbt_service.stg_prospect_matches: Close
[0m13:00:38.117247 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98e8ebcb-91df-445c-b3f5-6627cd60a46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e6f410>]}
[0m13:00:38.117825 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model public_staging.stg_prospect_matches .... [[32mSELECT 113771[0m in 7878.73s]
[0m13:00:38.118131 [debug] [Thread-1 (]: Finished running node model.dbt_service.stg_prospect_matches
[0m13:00:38.120273 [debug] [MainThread]: Using postgres connection "master"
[0m13:00:38.120472 [debug] [MainThread]: On master: BEGIN
[0m13:00:38.120638 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:00:38.129036 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m13:00:38.129298 [debug] [MainThread]: On master: COMMIT
[0m13:00:38.129464 [debug] [MainThread]: Using postgres connection "master"
[0m13:00:38.129631 [debug] [MainThread]: On master: COMMIT
[0m13:00:38.130071 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:00:38.130407 [debug] [MainThread]: On master: Close
[0m13:00:38.130681 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:00:38.130842 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m13:00:38.130978 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m13:00:38.131111 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m13:00:38.131237 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m13:00:38.131360 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m13:00:38.131486 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m13:00:38.131663 [info ] [MainThread]: 
[0m13:00:38.131844 [info ] [MainThread]: Finished running 1 incremental model in 2 hours 11 minutes and 18.86 seconds (7878.86s).
[0m13:00:38.132238 [debug] [MainThread]: Command end result
[0m13:00:38.149490 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m13:00:38.151912 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m13:00:38.155423 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m13:00:38.155577 [info ] [MainThread]: 
[0m13:00:38.155789 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:00:38.155936 [info ] [MainThread]: 
[0m13:00:38.156105 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m13:00:38.158676 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6956.849, "process_in_blocks": "0", "process_kernel_time": 0.198708, "process_mem_max_rss": "143360000", "process_out_blocks": "0", "process_user_time": 1.1051}
[0m13:00:38.159309 [debug] [MainThread]: Command `dbt run` succeeded at 13:00:38.159267 after 6956.85 seconds
[0m13:00:38.159619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026af490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026f3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102786bd0>]}
[0m13:00:38.159805 [debug] [MainThread]: Flushing usage events
[0m13:00:38.456552 [debug] [MainThread]: An error was encountered while trying to flush usage events
==================== 2025-11-09 23:39:38.839265 | 019a6afd-55d4-7db3-94b4-54465ce845a1 ====================
dbt-fusion 2.0.0-preview.63
   Loading profiles.yml
23:39:39.133854 [warn ]: dbt0255: Cannot register databases or schemas in the remote. Adapter '[33mpostgres[0m' does not support metadata operations.
    Failed [  0.00s] model public_raw.raw_pdl_data (table)
23:39:39.157856 [error]: dbt1000: Failed to receive render result for model.dbt_service.raw_pdl_data
    Failed [  0.00s] model public_raw.raw_fxf_data (table)
23:39:39.157976 [error]: dbt1000: Failed to receive render result for model.dbt_service.raw_fxf_data
   Skipped [-------] model public_raw_analysis.raw_company_profiling (table)
   Skipped [-------] model public_raw_analysis.raw_data_profiling (table)
   Skipped [-------] model public_raw_analysis.raw_location_profiling (table)
   Skipped [-------] model public_staging.stg_pdl_data (view)
   Skipped [-------] model public_staging_analysis.company_analysis (table)
   Skipped [-------] model public_staging_analysis.staging_company_profiling (table)
   Skipped [-------] model public_staging.stg_unified_prospects (table)
   Skipped [-------] model public_staging_analysis.staging_data_profiling (table)
   Skipped [-------] model public_staging_analysis.staging_location_profiling (table)
   Skipped [-------] model public_staging_analysis.staging_city_state_profiling (table)
   Skipped [-------] model public_marts.data_overview (view)
   Skipped [-------] model public_marts.prospect_matching_ratio (view)
   Skipped [-------] model public_staging.stg_prospect_matches (incremental)
   Skipped [-------] model public.setup_functions (view)
   Skipped [-------] model public_staging.stg_entity_clusters (table)
   Skipped [-------] model public_staging.stg_fxf_data (view)
   Skipped [-------] model public_raw_analysis.raw_company_profiling (table)
   Skipped [-------] model public_raw_analysis.raw_data_profiling (table)
   Skipped [-------] model public_raw_analysis.raw_location_profiling (table)
   Skipped [-------] model public_staging_analysis.company_analysis (table)
   Skipped [-------] model public_staging_analysis.staging_company_profiling (table)
   Skipped [-------] model public_staging.stg_unified_prospects (table)
   Skipped [-------] model public_staging_analysis.staging_data_profiling (table)
   Skipped [-------] model public_staging_analysis.staging_location_profiling (table)
   Skipped [-------] model public_staging_analysis.staging_city_state_profiling (table)
   Skipped [-------] model public_marts.data_overview (view)
   Skipped [-------] model public_marts.prospect_matching_ratio (view)
   Skipped [-------] model public_staging.stg_prospect_matches (incremental)
   Skipped [-------] model public.setup_functions (view)
   Skipped [-------] model public_staging.stg_entity_clusters (table)
[0m18:39:56.621729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107119590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10718f6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10718fdd0>]}


============================== 18:39:56.624038 | 5d28dea2-84cb-418e-a18c-f882b82cc17d ==============================
[0m18:39:56.624038 [info ] [MainThread]: Running with dbt=1.10.13
[0m18:39:56.624315 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'empty': 'False', 'log_format': 'default', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'use_colors': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'quiet': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'printer_width': '80', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'warn_error': 'None'}
[0m18:39:56.747331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072a2410>]}
[0m18:39:56.775980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a733d0>]}
[0m18:39:56.776590 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m18:39:56.849292 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m18:39:56.912935 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:39:56.913120 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:39:56.935436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f155d0>]}
[0m18:39:56.975113 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:39:56.976669 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:39:56.988034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10831fd90>]}
[0m18:39:56.988250 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m18:39:56.988409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108233f10>]}
[0m18:39:56.989375 [info ] [MainThread]: 
[0m18:39:56.989529 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m18:39:56.989645 [info ] [MainThread]: 
[0m18:39:56.989852 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m18:39:56.991683 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:56.991885 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:56.992108 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:56.992368 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:56.996991 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:56.997942 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:39:57.044034 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.044206 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.044368 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.044508 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.044659 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.044781 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.044891 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:39:57.045018 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.045155 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.045296 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.045432 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.045555 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.045696 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:39:57.045828 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.045943 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.046063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.046690 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.046808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:57.098847 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.052 seconds
[0m18:39:57.099565 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.100467 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.055 seconds
[0m18:39:57.100618 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.054 seconds
[0m18:39:57.100782 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.055 seconds
[0m18:39:57.100909 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.055 seconds
[0m18:39:57.101342 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.101734 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.101867 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.056 seconds
[0m18:39:57.102226 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.102615 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.103360 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:39:57.104953 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_marts)
[0m18:39:57.105345 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw)
[0m18:39:57.107940 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:39:57.108173 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m18:39:57.108432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging_analysis)
[0m18:39:57.109174 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:39:57.109351 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m18:39:57.109543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public)
[0m18:39:57.109659 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m18:39:57.110355 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:39:57.111029 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:39:57.111197 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m18:39:57.111807 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:39:57.112452 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:39:57.112591 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.112702 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m18:39:57.112807 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m18:39:57.112908 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.113011 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m18:39:57.113115 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m18:39:57.113308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.113430 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.113617 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.113744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:57.127898 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m18:39:57.128133 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:39:57.128304 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m18:39:57.128526 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m18:39:57.128664 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:39:57.128819 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m18:39:57.131593 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m18:39:57.131781 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:39:57.132034 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m18:39:57.133878 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m18:39:57.134133 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.005 seconds
[0m18:39:57.134306 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.002 seconds
[0m18:39:57.134459 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m18:39:57.134603 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:39:57.134715 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.006 seconds
[0m18:39:57.134843 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m18:39:57.135441 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m18:39:57.135870 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m18:39:57.135997 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:39:57.136139 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m18:39:57.136565 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m18:39:57.136692 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:39:57.136881 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m18:39:57.137156 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m18:39:57.137321 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m18:39:57.137502 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m18:39:57.137688 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m18:39:57.139305 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.002 seconds
[0m18:39:57.139932 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m18:39:57.140128 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.002 seconds
[0m18:39:57.140741 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m18:39:57.141044 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m18:39:57.141530 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m18:39:57.141656 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m18:39:57.141777 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m18:39:57.142752 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m18:39:57.148777 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:57.149843 [debug] [MainThread]: On master: BEGIN
[0m18:39:57.151100 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:39:57.158976 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m18:39:57.159248 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:57.159470 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m18:39:57.162194 [debug] [MainThread]: SQL status: SELECT 6 in 0.002 seconds
[0m18:39:57.163336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106787010>]}
[0m18:39:57.163579 [debug] [MainThread]: On master: ROLLBACK
[0m18:39:57.164011 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:57.164133 [debug] [MainThread]: On master: BEGIN
[0m18:39:57.164732 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m18:39:57.164953 [debug] [MainThread]: On master: COMMIT
[0m18:39:57.165113 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:57.165241 [debug] [MainThread]: On master: COMMIT
[0m18:39:57.165618 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:39:57.165922 [debug] [MainThread]: On master: Close
[0m18:39:57.170017 [debug] [Thread-1 (]: Began running node model.dbt_service.raw_fxf_data
[0m18:39:57.170211 [debug] [Thread-2 (]: Began running node model.dbt_service.raw_pdl_data
[0m18:39:57.170506 [info ] [Thread-1 (]: 1 of 18 START sql table model public_raw.raw_fxf_data .......................... [RUN]
[0m18:39:57.170787 [info ] [Thread-2 (]: 2 of 18 START sql table model public_raw.raw_pdl_data .......................... [RUN]
[0m18:39:57.171019 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_marts, now model.dbt_service.raw_fxf_data)
[0m18:39:57.171212 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw, now model.dbt_service.raw_pdl_data)
[0m18:39:57.171381 [debug] [Thread-1 (]: Began compiling node model.dbt_service.raw_fxf_data
[0m18:39:57.171546 [debug] [Thread-2 (]: Began compiling node model.dbt_service.raw_pdl_data
[0m18:39:57.174660 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.raw_fxf_data"
[0m18:39:57.176348 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_service.raw_pdl_data"
[0m18:39:57.177136 [debug] [Thread-2 (]: Began executing node model.dbt_service.raw_pdl_data
[0m18:39:57.177329 [debug] [Thread-1 (]: Began executing node model.dbt_service.raw_fxf_data
[0m18:39:57.192257 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_service.raw_pdl_data"
[0m18:39:57.193528 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.raw_fxf_data"
[0m18:39:57.194182 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.194506 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.194701 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: BEGIN
[0m18:39:57.194887 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: BEGIN
[0m18:39:57.195048 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:39:57.195197 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:39:57.203694 [debug] [Thread-2 (]: SQL status: BEGIN in 0.008 seconds
[0m18:39:57.203943 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m18:39:57.204188 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.204378 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.204577 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw PDL data from seed  
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."pdl_data"
  );
  
[0m18:39:57.204764 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */

  
    

  create  table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp"
  
  
    as
  
  (
    -- Raw FXF data from seed
-- This model creates a table in the raw schema from our CSV data



select * from "finny_db"."public"."fxf_data"
  );
  
[0m18:39:57.262972 [debug] [Thread-1 (]: SQL status: SELECT 50005 in 0.058 seconds
[0m18:39:57.266985 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.267262 [debug] [Thread-2 (]: SQL status: SELECT 50005 in 0.062 seconds
[0m18:39:57.267559 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data" rename to "raw_fxf_data__dbt_backup"
[0m18:39:57.269771 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.270095 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data" rename to "raw_pdl_data__dbt_backup"
[0m18:39:57.270513 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.270710 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.273175 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.274466 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.274659 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
alter table "finny_db"."public_raw"."raw_fxf_data__dbt_tmp" rename to "raw_fxf_data"
[0m18:39:57.274835 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
alter table "finny_db"."public_raw"."raw_pdl_data__dbt_tmp" rename to "raw_pdl_data"
[0m18:39:57.275530 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.275699 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.282568 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m18:39:57.283154 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m18:39:57.283309 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.283468 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.283597 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: COMMIT
[0m18:39:57.283758 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: COMMIT
[0m18:39:57.291906 [debug] [Thread-1 (]: SQL status: COMMIT in 0.008 seconds
[0m18:39:57.292117 [debug] [Thread-2 (]: SQL status: COMMIT in 0.008 seconds
[0m18:39:57.295192 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_raw"."raw_fxf_data__dbt_backup"
[0m18:39:57.296168 [debug] [Thread-2 (]: Applying DROP to: "finny_db"."public_raw"."raw_pdl_data__dbt_backup"
[0m18:39:57.298263 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.raw_fxf_data"
[0m18:39:57.298483 [debug] [Thread-2 (]: Using postgres connection "model.dbt_service.raw_pdl_data"
[0m18:39:57.298683 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_fxf_data"} */
drop table if exists "finny_db"."public_raw"."raw_fxf_data__dbt_backup" cascade
[0m18:39:57.298867 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_pdl_data"} */
drop table if exists "finny_db"."public_raw"."raw_pdl_data__dbt_backup" cascade
[0m18:39:57.301671 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.003 seconds
[0m18:39:57.301820 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m18:39:57.302824 [debug] [Thread-2 (]: On model.dbt_service.raw_pdl_data: Close
[0m18:39:57.303346 [debug] [Thread-1 (]: On model.dbt_service.raw_fxf_data: Close
[0m18:39:57.304865 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091c73d0>]}
[0m18:39:57.305031 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091a2c90>]}
[0m18:39:57.305391 [info ] [Thread-2 (]: 2 of 18 OK created sql table model public_raw.raw_pdl_data ..................... [[32mSELECT 50005[0m in 0.13s]
[0m18:39:57.305694 [info ] [Thread-1 (]: 1 of 18 OK created sql table model public_raw.raw_fxf_data ..................... [[32mSELECT 50005[0m in 0.13s]
[0m18:39:57.305953 [debug] [Thread-2 (]: Finished running node model.dbt_service.raw_pdl_data
[0m18:39:57.306186 [debug] [Thread-1 (]: Finished running node model.dbt_service.raw_fxf_data
[0m18:39:57.306506 [debug] [Thread-4 (]: Began running node model.dbt_service.stg_pdl_data
[0m18:39:57.306878 [debug] [Thread-6 (]: Began running node model.dbt_service.raw_company_profiling
[0m18:39:57.306745 [info ] [Thread-4 (]: 3 of 18 START sql view model public_staging.stg_pdl_data ....................... [RUN]
[0m18:39:57.307079 [debug] [Thread-7 (]: Began running node model.dbt_service.raw_data_profiling
[0m18:39:57.307259 [debug] [Thread-8 (]: Began running node model.dbt_service.raw_location_profiling
[0m18:39:57.307419 [debug] [Thread-9 (]: Began running node model.dbt_service.stg_fxf_data
[0m18:39:57.307617 [info ] [Thread-6 (]: 4 of 18 START sql table model public_raw_analysis.raw_company_profiling ........ [RUN]
[0m18:39:57.307844 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging_analysis, now model.dbt_service.stg_pdl_data)
[0m18:39:57.308072 [info ] [Thread-7 (]: 5 of 18 START sql table model public_raw_analysis.raw_data_profiling ........... [RUN]
[0m18:39:57.308296 [info ] [Thread-8 (]: 6 of 18 START sql table model public_raw_analysis.raw_location_profiling ....... [RUN]
[0m18:39:57.308505 [info ] [Thread-9 (]: 7 of 18 START sql view model public_staging.stg_fxf_data ....................... [RUN]
[0m18:39:57.308693 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_finny_db_public, now model.dbt_service.raw_company_profiling)
[0m18:39:57.308862 [debug] [Thread-4 (]: Began compiling node model.dbt_service.stg_pdl_data
[0m18:39:57.309072 [debug] [Thread-7 (]: Acquiring new postgres connection 'model.dbt_service.raw_data_profiling'
[0m18:39:57.309272 [debug] [Thread-8 (]: Acquiring new postgres connection 'model.dbt_service.raw_location_profiling'
[0m18:39:57.309462 [debug] [Thread-9 (]: Acquiring new postgres connection 'model.dbt_service.stg_fxf_data'
[0m18:39:57.309626 [debug] [Thread-6 (]: Began compiling node model.dbt_service.raw_company_profiling
[0m18:39:57.311142 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_service.stg_pdl_data"
[0m18:39:57.311337 [debug] [Thread-7 (]: Began compiling node model.dbt_service.raw_data_profiling
[0m18:39:57.311502 [debug] [Thread-8 (]: Began compiling node model.dbt_service.raw_location_profiling
[0m18:39:57.311671 [debug] [Thread-9 (]: Began compiling node model.dbt_service.stg_fxf_data
[0m18:39:57.313120 [debug] [Thread-6 (]: Writing injected SQL for node "model.dbt_service.raw_company_profiling"
[0m18:39:57.314595 [debug] [Thread-7 (]: Writing injected SQL for node "model.dbt_service.raw_data_profiling"
[0m18:39:57.316420 [debug] [Thread-8 (]: Writing injected SQL for node "model.dbt_service.raw_location_profiling"
[0m18:39:57.317766 [debug] [Thread-9 (]: Writing injected SQL for node "model.dbt_service.stg_fxf_data"
[0m18:39:57.317970 [debug] [Thread-4 (]: Began executing node model.dbt_service.stg_pdl_data
[0m18:39:57.326877 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_service.stg_pdl_data"
[0m18:39:57.327469 [debug] [Thread-8 (]: Began executing node model.dbt_service.raw_location_profiling
[0m18:39:57.327713 [debug] [Thread-9 (]: Began executing node model.dbt_service.stg_fxf_data
[0m18:39:57.327962 [debug] [Thread-7 (]: Began executing node model.dbt_service.raw_data_profiling
[0m18:39:57.328238 [debug] [Thread-6 (]: Began executing node model.dbt_service.raw_company_profiling
[0m18:39:57.329819 [debug] [Thread-8 (]: Writing runtime sql for node "model.dbt_service.raw_location_profiling"
[0m18:39:57.331685 [debug] [Thread-9 (]: Writing runtime sql for node "model.dbt_service.stg_fxf_data"
[0m18:39:57.331976 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m18:39:57.333597 [debug] [Thread-7 (]: Writing runtime sql for node "model.dbt_service.raw_data_profiling"
[0m18:39:57.335131 [debug] [Thread-6 (]: Writing runtime sql for node "model.dbt_service.raw_company_profiling"
[0m18:39:57.335458 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: BEGIN
[0m18:39:57.335826 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.336024 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m18:39:57.336251 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:39:57.336450 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.336661 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: BEGIN
[0m18:39:57.336825 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.337016 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: BEGIN
[0m18:39:57.337286 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: BEGIN
[0m18:39:57.337441 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m18:39:57.337598 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: BEGIN
[0m18:39:57.337743 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m18:39:57.337896 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:39:57.338105 [debug] [Thread-6 (]: Opening a new connection, currently in state closed
[0m18:39:57.351944 [debug] [Thread-6 (]: SQL status: BEGIN in 0.014 seconds
[0m18:39:57.352171 [debug] [Thread-8 (]: SQL status: BEGIN in 0.015 seconds
[0m18:39:57.352392 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.352626 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.352897 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for raw data
-- This model analyzes company distribution patterns in the raw data layer



with fxf_company_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct
    from "finny_db"."public_raw"."raw_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m18:39:57.353213 [debug] [Thread-9 (]: SQL status: BEGIN in 0.015 seconds
[0m18:39:57.353570 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for raw data with ISO code standardization via seed
-- This model analyzes geographic distribution patterns in the raw data



with fxf_location_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_fxf_data"
    where location is not null and trim(location) != ''
    group by location
),

fxf_location_standardized as (
    select 
        flp.*,
        coalesce(sim.iso_code, flp.state_region_raw) as state_region
    from fxf_location_profile flp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(flp.state_region_raw) = upper(sim.state_name)
),

pdl_location_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        location,
        -- Extract state/region (usually the last part after comma, or the whole string)
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))  -- Get the last part after comma
            else trim(location)
        end as state_region_raw,
        -- Extract city (usually the first part before comma, if comma exists)
        case 
            when location like '%,%' then trim(split_part(location, ',', 1))  -- Get the first part before comma
            else null
        end as city,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue
    from "finny_db"."public_raw"."raw_pdl_data"
    where location is not null and trim(location) != ''
    group by location
),

pdl_location_standardized as (
    select 
        plp.*,
        coalesce(sim.iso_code, plp.state_region_raw) as state_region
    from pdl_location_profile plp
    left join "finny_db"."public"."state_iso_mapping" sim
        on upper(plp.state_region_raw) = upper(sim.state_name)
),

combined_location_profile as (
    select
        'combined_raw' as source_table,
        'raw' as data_layer,
        location,
        state_region,
        city,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        null::text as state_region_raw
    from (
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from fxf_location_standardized
        union all
        select 
            source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
            company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue
        from pdl_location_standardized
    ) combined
    group by location, state_region, city
)

select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from fxf_location_standardized
union all
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from pdl_location_standardized
union all  
select 
    source_table, data_layer, location, state_region, city, contact_count, unique_contacts,
    company_count, title_count, contacts_with_revenue, avg_revenue, min_revenue, max_revenue, state_region_raw
from combined_location_profile
order by contact_count desc
  );
  
[0m18:39:57.353945 [debug] [Thread-4 (]: SQL status: BEGIN in 0.018 seconds
[0m18:39:57.354140 [debug] [Thread-7 (]: SQL status: BEGIN in 0.016 seconds
[0m18:39:57.354354 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m18:39:57.354568 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m18:39:57.354722 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.354912 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */

  create view "finny_db"."public_staging"."stg_fxf_data__dbt_tmp"
    
    
  as (
    -- Staging model for FXF data
-- This model cleans and normalizes the raw FXF data with location parsing



select
    fxf_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_fxf_data" fxf
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when fxf.location like '%,%' then trim(split_part(fxf.location, ',', -1))
            else trim(fxf.location)
        end
    ) = upper(sim.state_name)
where fxf_id is not null
  );
[0m18:39:57.355175 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */

  create view "finny_db"."public_staging"."stg_pdl_data__dbt_tmp"
    
    
  as (
    -- Staging model for PDL data
-- This model cleans and normalizes the raw PDL data with location parsing



select
    pdl_id,
    name,
    email,
    company,
    company_revenue,
    title,
    location,
    -- Extract city (usually the first part before comma, if comma exists)
    case 
        when location like '%,%' then trim(split_part(location, ',', 1))
        else null
    end as city,
    -- Extract and standardize state using ISO mapping
    coalesce(
        sim.iso_code, 
        case 
            when location like '%,%' then trim(split_part(location, ',', -1))
            else trim(location)
        end
    ) as state
from "finny_db"."public_raw"."raw_pdl_data" pdl
left join "finny_db"."public"."state_iso_mapping" sim
    on upper(
        case 
            when pdl.location like '%,%' then trim(split_part(pdl.location, ',', -1))
            else trim(pdl.location)
        end
    ) = upper(sim.state_name)
where pdl_id is not null
  );
[0m18:39:57.355435 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */

  
    

  create  table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Raw data profiling analysis
-- This model analyzes data quality patterns in the raw data layer



with fxf_raw_profile as (
    select
        'fxf_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_fxf_data"
),

pdl_raw_profile as (
    select
        'pdl_data' as source_table,
        'raw' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles
    from "finny_db"."public_raw"."raw_pdl_data"
)

select * from fxf_raw_profile
union all
select * from pdl_raw_profile
  );
  
[0m18:39:57.357409 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m18:39:57.357684 [debug] [Thread-9 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m18:39:57.358914 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m18:39:57.360076 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m18:39:57.360279 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
alter table "finny_db"."public_staging"."stg_pdl_data__dbt_tmp" rename to "stg_pdl_data"
[0m18:39:57.360481 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
alter table "finny_db"."public_staging"."stg_fxf_data__dbt_tmp" rename to "stg_fxf_data"
[0m18:39:57.361375 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.361607 [debug] [Thread-9 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.362272 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m18:39:57.362941 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m18:39:57.363283 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m18:39:57.363530 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m18:39:57.363764 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: COMMIT
[0m18:39:57.363980 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: COMMIT
[0m18:39:57.364900 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.365947 [debug] [Thread-4 (]: Applying DROP to: "finny_db"."public_staging"."stg_pdl_data__dbt_backup"
[0m18:39:57.366125 [debug] [Thread-9 (]: SQL status: COMMIT in 0.002 seconds
[0m18:39:57.367453 [debug] [Thread-4 (]: Using postgres connection "model.dbt_service.stg_pdl_data"
[0m18:39:57.368940 [debug] [Thread-9 (]: Applying DROP to: "finny_db"."public_staging"."stg_fxf_data__dbt_backup"
[0m18:39:57.369198 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_pdl_data"} */
drop view if exists "finny_db"."public_staging"."stg_pdl_data__dbt_backup" cascade
[0m18:39:57.369559 [debug] [Thread-9 (]: Using postgres connection "model.dbt_service.stg_fxf_data"
[0m18:39:57.369778 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_fxf_data"} */
drop view if exists "finny_db"."public_staging"."stg_fxf_data__dbt_backup" cascade
[0m18:39:57.370128 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m18:39:57.370767 [debug] [Thread-4 (]: On model.dbt_service.stg_pdl_data: Close
[0m18:39:57.370956 [debug] [Thread-9 (]: SQL status: DROP VIEW in 0.001 seconds
[0m18:39:57.371276 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10828e3d0>]}
[0m18:39:57.371895 [debug] [Thread-9 (]: On model.dbt_service.stg_fxf_data: Close
[0m18:39:57.372233 [info ] [Thread-4 (]: 3 of 18 OK created sql view model public_staging.stg_pdl_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m18:39:57.372554 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092b4810>]}
[0m18:39:57.372833 [debug] [Thread-4 (]: Finished running node model.dbt_service.stg_pdl_data
[0m18:39:57.373143 [info ] [Thread-9 (]: 7 of 18 OK created sql view model public_staging.stg_fxf_data .................. [[32mCREATE VIEW[0m in 0.06s]
[0m18:39:57.373506 [debug] [Thread-9 (]: Finished running node model.dbt_service.stg_fxf_data
[0m18:39:57.373850 [debug] [Thread-11 ]: Began running node model.dbt_service.company_analysis
[0m18:39:57.374044 [debug] [Thread-12 ]: Began running node model.dbt_service.staging_city_state_profiling
[0m18:39:57.374231 [debug] [Thread-13 ]: Began running node model.dbt_service.staging_company_profiling
[0m18:39:57.374410 [debug] [Thread-14 ]: Began running node model.dbt_service.staging_data_profiling
[0m18:39:57.374565 [debug] [Thread-15 ]: Began running node model.dbt_service.staging_location_profiling
[0m18:39:57.374895 [debug] [Thread-16 ]: Began running node model.dbt_service.stg_unified_prospects
[0m18:39:57.374763 [info ] [Thread-11 ]: 8 of 18 START sql table model public_staging_analysis.company_analysis ......... [RUN]
[0m18:39:57.375177 [info ] [Thread-12 ]: 9 of 18 START sql table model public_staging_analysis.staging_city_state_profiling  [RUN]
[0m18:39:57.375405 [info ] [Thread-13 ]: 10 of 18 START sql table model public_staging_analysis.staging_company_profiling  [RUN]
[0m18:39:57.375635 [info ] [Thread-14 ]: 11 of 18 START sql table model public_staging_analysis.staging_data_profiling .. [RUN]
[0m18:39:57.375864 [info ] [Thread-15 ]: 12 of 18 START sql table model public_staging_analysis.staging_location_profiling  [RUN]
[0m18:39:57.376092 [info ] [Thread-16 ]: 13 of 18 START sql table model public_staging.stg_unified_prospects ............ [RUN]
[0m18:39:57.376348 [debug] [Thread-11 ]: Acquiring new postgres connection 'model.dbt_service.company_analysis'
[0m18:39:57.376564 [debug] [Thread-12 ]: Acquiring new postgres connection 'model.dbt_service.staging_city_state_profiling'
[0m18:39:57.376900 [debug] [Thread-13 ]: Acquiring new postgres connection 'model.dbt_service.staging_company_profiling'
[0m18:39:57.377137 [debug] [Thread-14 ]: Acquiring new postgres connection 'model.dbt_service.staging_data_profiling'
[0m18:39:57.377339 [debug] [Thread-15 ]: Acquiring new postgres connection 'model.dbt_service.staging_location_profiling'
[0m18:39:57.377546 [debug] [Thread-16 ]: Acquiring new postgres connection 'model.dbt_service.stg_unified_prospects'
[0m18:39:57.377720 [debug] [Thread-11 ]: Began compiling node model.dbt_service.company_analysis
[0m18:39:57.377886 [debug] [Thread-12 ]: Began compiling node model.dbt_service.staging_city_state_profiling
[0m18:39:57.378051 [debug] [Thread-13 ]: Began compiling node model.dbt_service.staging_company_profiling
[0m18:39:57.378223 [debug] [Thread-14 ]: Began compiling node model.dbt_service.staging_data_profiling
[0m18:39:57.378390 [debug] [Thread-15 ]: Began compiling node model.dbt_service.staging_location_profiling
[0m18:39:57.378535 [debug] [Thread-16 ]: Began compiling node model.dbt_service.stg_unified_prospects
[0m18:39:57.381062 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbt_service.company_analysis"
[0m18:39:57.383325 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.384955 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbt_service.staging_company_profiling"
[0m18:39:57.386326 [debug] [Thread-14 ]: Writing injected SQL for node "model.dbt_service.staging_data_profiling"
[0m18:39:57.387674 [debug] [Thread-15 ]: Writing injected SQL for node "model.dbt_service.staging_location_profiling"
[0m18:39:57.388990 [debug] [Thread-16 ]: Writing injected SQL for node "model.dbt_service.stg_unified_prospects"
[0m18:39:57.390478 [debug] [Thread-11 ]: Began executing node model.dbt_service.company_analysis
[0m18:39:57.390800 [debug] [Thread-12 ]: Began executing node model.dbt_service.staging_city_state_profiling
[0m18:39:57.392399 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbt_service.company_analysis"
[0m18:39:57.392573 [debug] [Thread-13 ]: Began executing node model.dbt_service.staging_company_profiling
[0m18:39:57.392718 [debug] [Thread-16 ]: Began executing node model.dbt_service.stg_unified_prospects
[0m18:39:57.392856 [debug] [Thread-15 ]: Began executing node model.dbt_service.staging_location_profiling
[0m18:39:57.394067 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.394267 [debug] [Thread-14 ]: Began executing node model.dbt_service.staging_data_profiling
[0m18:39:57.395564 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbt_service.staging_company_profiling"
[0m18:39:57.396815 [debug] [Thread-16 ]: Writing runtime sql for node "model.dbt_service.stg_unified_prospects"
[0m18:39:57.397002 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.398902 [debug] [Thread-15 ]: Writing runtime sql for node "model.dbt_service.staging_location_profiling"
[0m18:39:57.400244 [debug] [Thread-14 ]: Writing runtime sql for node "model.dbt_service.staging_data_profiling"
[0m18:39:57.400612 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.400814 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: BEGIN
[0m18:39:57.401048 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.401204 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.401429 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: BEGIN
[0m18:39:57.401644 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m18:39:57.401878 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.402067 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.402223 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: BEGIN
[0m18:39:57.402384 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: BEGIN
[0m18:39:57.402534 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m18:39:57.402799 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: BEGIN
[0m18:39:57.403000 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: BEGIN
[0m18:39:57.403173 [debug] [Thread-16 ]: Opening a new connection, currently in state init
[0m18:39:57.403352 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m18:39:57.403602 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m18:39:57.403783 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m18:39:57.418330 [debug] [Thread-11 ]: SQL status: BEGIN in 0.017 seconds
[0m18:39:57.418560 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.418778 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */

  
    

  create  table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp"
  
  
    as
  
  (
    -- Combined company analysis from both FXF and PDL data sources with structured location data



with fxf_companies as (
    select
        'fxf' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null
    group by company
),

pdl_companies as (
    select
        'pdl' as data_source,
        company,
        count(*) as employee_count,
        count(distinct title) as unique_titles,
        avg(company_revenue) as avg_revenue,
        array_agg(distinct location) as locations,
        array_agg(distinct city) filter (where city is not null) as cities,
        array_agg(distinct state) filter (where state is not null) as states,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null
    group by company
),

combined_companies as (
    select * from fxf_companies
    union all
    select * from pdl_companies
)

select
    company,
    array_agg(distinct data_source) as data_sources,
    sum(employee_count) as total_employees,
    sum(unique_titles) as total_unique_titles,
    avg(avg_revenue) as average_revenue,
    sum(unique_cities) as total_unique_cities,
    sum(unique_states) as total_unique_states,
    array_agg(distinct location_item) filter (where location_item is not null) as all_locations,
    array_agg(distinct city_item) filter (where city_item is not null) as all_cities,
    array_agg(distinct state_item) filter (where state_item is not null) as all_states
from (
    select 
        company, data_source, employee_count, unique_titles, avg_revenue, unique_cities, unique_states,
        unnest(locations) as location_item,
        unnest(cities) as city_item,
        unnest(states) as state_item
    from combined_companies
) expanded
group by company
order by total_employees desc
  );
  
[0m18:39:57.419081 [debug] [Thread-12 ]: SQL status: BEGIN in 0.017 seconds
[0m18:39:57.419411 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.419632 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp"
  
  
    as
  
  (
    -- City and State profiling analysis for staging data
-- This model analyzes geographic distribution patterns using parsed city and state fields



with fxf_city_state_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by city, state, location
),

pdl_city_state_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        city,
        state,
        location,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by city, state, location
)

select * from fxf_city_state_profile
union all
select * from pdl_city_state_profile
order by contact_count desc
  );
  
[0m18:39:57.421850 [debug] [Thread-16 ]: SQL status: BEGIN in 0.019 seconds
[0m18:39:57.422064 [debug] [Thread-13 ]: SQL status: BEGIN in 0.019 seconds
[0m18:39:57.422257 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.422476 [debug] [Thread-15 ]: SQL status: BEGIN in 0.019 seconds
[0m18:39:57.422660 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.422804 [debug] [Thread-14 ]: SQL status: BEGIN in 0.019 seconds
[0m18:39:57.423033 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */

  
    

  create  table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp"
  
  
    as
  
  (
    -- Unified prospect records from both FXF and PDL sources
-- This model combines both datasets into a single standardized format



with fxf_prospects as (
    select
        'fxf' as data_source,
        fxf_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as pdl_id
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_prospects as (
    select
        'pdl' as data_source,
        pdl_id as source_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        -- Add source-specific fields as nulls for consistency
        null as fxf_id
    from "finny_db"."public_staging"."stg_pdl_data"
),

unified_prospects as (
    select
        row_number() over (order by data_source, source_id) as prospect_id,
        data_source,
        source_id,
        case when data_source = 'fxf' then source_id else null end as fxf_id,
        case when data_source = 'pdl' then source_id else null end as pdl_id,
        name,
        email,
        company,
        title,
        location,
        city,
        state,
        company_revenue,
        'unidentified' as status  -- Status for prospects not yet processed for matching
    from (
        select * from fxf_prospects
        union all
        select * from pdl_prospects
    ) combined
)

select * from unified_prospects
order by prospect_id
  );
  
[0m18:39:57.423276 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.423690 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Company profiling analysis for staging data with structured location data
-- This model analyzes company distribution patterns in the cleaned staging data



with fxf_company_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct fxf_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_fxf_data"
    where company is not null and trim(company) != ''
    group by company
),

pdl_company_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        company,
        count(*) as employee_count,
        count(distinct pdl_id) as unique_employees,
        count(distinct title) as unique_titles,
        count(distinct location) as office_locations,
        count(distinct city) as unique_cities,
        count(distinct state) as unique_states,
        count(company_revenue) as employees_with_revenue_info,
        count(*) - count(company_revenue) as employees_missing_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Calculate data completeness and quality percentages
        round(100.0 * count(name) / count(*), 2) as name_completeness_pct,
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(title) / count(*), 2) as title_completeness_pct,
        round(100.0 * count(location) / count(*), 2) as location_completeness_pct,
        round(100.0 * count(city) / count(*), 2) as city_completeness_pct,
        round(100.0 * count(state) / count(*), 2) as state_completeness_pct,
        -- Staging-specific quality metrics
        count(*) filter (where email is not null and email like '%@%') as valid_email_count,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct,
        count(distinct case when email like '%@%' then split_part(email, '@', 2) end) as email_domain_count
    from "finny_db"."public_staging"."stg_pdl_data"
    where company is not null and trim(company) != ''
    group by company
)

select * from fxf_company_profile
union all
select * from pdl_company_profile
order by employee_count desc
  );
  
[0m18:39:57.423965 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.424247 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Location profiling analysis for staging data using structured city and state fields
-- This model analyzes geographic distribution patterns in the cleaned staging data



with fxf_location_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct fxf_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_fxf_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

pdl_location_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        count(*) as contact_count,
        count(distinct pdl_id) as unique_contacts,
        count(distinct company) as company_count,
        count(distinct title) as title_count,
        count(company_revenue) as contacts_with_revenue,
        avg(company_revenue) as avg_revenue,
        min(company_revenue) as min_revenue,
        max(company_revenue) as max_revenue,
        -- Staging-specific quality metrics
        count(*) filter (where name is not null and trim(name) != '') as contacts_with_name,
        count(*) filter (where email is not null and email like '%@%') as contacts_with_valid_email,
        round(100.0 * count(*) filter (where email is not null and email like '%@%') / count(*), 2) as email_validity_pct
    from "finny_db"."public_staging"."stg_pdl_data"
    where location is not null and trim(location) != ''
    group by location, city, state
),

combined_location_profile as (
    select
        'combined_staging' as source_table,
        'staging' as data_layer,
        location,
        city,
        state,
        sum(contact_count) as contact_count,
        sum(unique_contacts) as unique_contacts,
        sum(company_count) as company_count,
        sum(title_count) as title_count,
        sum(contacts_with_revenue) as contacts_with_revenue,
        avg(avg_revenue) as avg_revenue,
        min(min_revenue) as min_revenue,
        max(max_revenue) as max_revenue,
        sum(contacts_with_name) as contacts_with_name,
        sum(contacts_with_valid_email) as contacts_with_valid_email,
        round(100.0 * sum(contacts_with_valid_email) / sum(contact_count), 2) as email_validity_pct
    from (
        select * from fxf_location_profile
        union all
        select * from pdl_location_profile
    ) combined
    group by location, city, state
)

select * from fxf_location_profile
union all
select * from pdl_location_profile
union all  
select * from combined_location_profile
order by contact_count desc
  );
  
[0m18:39:57.425132 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */

  
    

  create  table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp"
  
  
    as
  
  (
    -- Staging data profiling analysis
-- This model analyzes data quality patterns in the staging data layer



with fxf_staging_profile as (
    select
        'stg_fxf_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct fxf_id) as unique_ids,
        count(*) - count(distinct fxf_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_fxf_data"
),

pdl_staging_profile as (
    select
        'stg_pdl_data' as source_table,
        'staging' as data_layer,
        count(*) as total_records,
        count(distinct pdl_id) as unique_ids,
        count(*) - count(distinct pdl_id) as duplicate_ids,
        count(name) as non_null_names,
        count(*) - count(name) as null_names,
        count(email) as non_null_emails,
        count(*) - count(email) as null_emails,
        count(company) as non_null_companies,
        count(*) - count(company) as null_companies,
        count(company_revenue) as non_null_revenues,
        count(*) - count(company_revenue) as null_revenues,
        count(title) as non_null_titles,
        count(*) - count(title) as null_titles,
        count(location) as non_null_locations,
        count(*) - count(location) as null_locations,
        count(distinct company) as unique_companies,
        count(distinct location) as unique_locations,
        count(distinct title) as unique_titles,
        -- Staging-specific metrics
        round(100.0 * count(email) / count(*), 2) as email_completeness_pct,
        round(100.0 * count(company_revenue) / count(*), 2) as revenue_completeness_pct,
        round(avg(company_revenue)::numeric, 0) as avg_company_revenue,
        max(company_revenue) as max_company_revenue,
        min(company_revenue) as min_company_revenue
    from "finny_db"."public_staging"."stg_pdl_data"
)

select * from fxf_staging_profile
union all
select * from pdl_staging_profile
order by total_records desc
  );
  
[0m18:39:57.458381 [debug] [Thread-7 (]: SQL status: SELECT 2 in 0.102 seconds
[0m18:39:57.462671 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.463222 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling" rename to "raw_data_profiling__dbt_backup"
[0m18:39:57.464332 [debug] [Thread-6 (]: SQL status: SELECT 180 in 0.110 seconds
[0m18:39:57.465986 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.466396 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m18:39:57.466572 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling" rename to "raw_company_profiling__dbt_backup"
[0m18:39:57.470013 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.470771 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_tmp" rename to "raw_data_profiling"
[0m18:39:57.471466 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.471827 [debug] [Thread-7 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.473401 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.473965 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m18:39:57.474139 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_tmp" rename to "raw_company_profiling"
[0m18:39:57.474300 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.474779 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: COMMIT
[0m18:39:57.476580 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m18:39:57.478341 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m18:39:57.478865 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.479285 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: COMMIT
[0m18:39:57.480620 [debug] [Thread-7 (]: SQL status: COMMIT in 0.005 seconds
[0m18:39:57.483018 [debug] [Thread-7 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup"
[0m18:39:57.483346 [debug] [Thread-7 (]: Using postgres connection "model.dbt_service.raw_data_profiling"
[0m18:39:57.483515 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_data_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_data_profiling__dbt_backup" cascade
[0m18:39:57.484529 [debug] [Thread-6 (]: SQL status: COMMIT in 0.005 seconds
[0m18:39:57.487990 [debug] [Thread-6 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup"
[0m18:39:57.488468 [debug] [Thread-7 (]: SQL status: DROP TABLE in 0.005 seconds
[0m18:39:57.489080 [debug] [Thread-6 (]: Using postgres connection "model.dbt_service.raw_company_profiling"
[0m18:39:57.490564 [debug] [Thread-7 (]: On model.dbt_service.raw_data_profiling: Close
[0m18:39:57.491023 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_company_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_company_profiling__dbt_backup" cascade
[0m18:39:57.491936 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc2b290>]}
[0m18:39:57.492845 [info ] [Thread-7 (]: 5 of 18 OK created sql table model public_raw_analysis.raw_data_profiling ...... [[32mSELECT 2[0m in 0.18s]
[0m18:39:57.493462 [debug] [Thread-7 (]: Finished running node model.dbt_service.raw_data_profiling
[0m18:39:57.495767 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.004 seconds
[0m18:39:57.497282 [debug] [Thread-6 (]: On model.dbt_service.raw_company_profiling: Close
[0m18:39:57.497956 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108330cd0>]}
[0m18:39:57.498539 [info ] [Thread-6 (]: 4 of 18 OK created sql table model public_raw_analysis.raw_company_profiling ... [[32mSELECT 180[0m in 0.19s]
[0m18:39:57.499025 [debug] [Thread-6 (]: Finished running node model.dbt_service.raw_company_profiling
[0m18:39:57.562528 [debug] [Thread-8 (]: SQL status: SELECT 152 in 0.208 seconds
[0m18:39:57.564169 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.564349 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling" rename to "raw_location_profiling__dbt_backup"
[0m18:39:57.565004 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.566380 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.566569 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
alter table "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_tmp" rename to "raw_location_profiling"
[0m18:39:57.567091 [debug] [Thread-8 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.567586 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m18:39:57.567738 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.567873 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: COMMIT
[0m18:39:57.568630 [debug] [Thread-8 (]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.569629 [debug] [Thread-8 (]: Applying DROP to: "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup"
[0m18:39:57.569861 [debug] [Thread-8 (]: Using postgres connection "model.dbt_service.raw_location_profiling"
[0m18:39:57.570011 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.raw_location_profiling"} */
drop table if exists "finny_db"."public_raw_analysis"."raw_location_profiling__dbt_backup" cascade
[0m18:39:57.571050 [debug] [Thread-8 (]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.571620 [debug] [Thread-8 (]: On model.dbt_service.raw_location_profiling: Close
[0m18:39:57.571886 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109286290>]}
[0m18:39:57.572159 [info ] [Thread-8 (]: 6 of 18 OK created sql table model public_raw_analysis.raw_location_profiling .. [[32mSELECT 152[0m in 0.26s]
[0m18:39:57.572395 [debug] [Thread-8 (]: Finished running node model.dbt_service.raw_location_profiling
[0m18:39:57.580896 [debug] [Thread-12 ]: SQL status: SELECT 76 in 0.161 seconds
[0m18:39:57.583291 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.583502 [debug] [Thread-11 ]: SQL status: SELECT 91 in 0.164 seconds
[0m18:39:57.583746 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling" rename to "staging_city_state_profiling__dbt_backup"
[0m18:39:57.585129 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.585300 [debug] [Thread-14 ]: SQL status: SELECT 2 in 0.160 seconds
[0m18:39:57.585487 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis" rename to "company_analysis__dbt_backup"
[0m18:39:57.586642 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.586840 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.587038 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling" rename to "staging_data_profiling__dbt_backup"
[0m18:39:57.587212 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.588233 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.589368 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.589543 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_tmp" rename to "staging_city_state_profiling"
[0m18:39:57.589725 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.589889 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
alter table "finny_db"."public_staging_analysis"."company_analysis__dbt_tmp" rename to "company_analysis"
[0m18:39:57.591090 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.591318 [debug] [Thread-12 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.591530 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_tmp" rename to "staging_data_profiling"
[0m18:39:57.592043 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m18:39:57.592236 [debug] [Thread-11 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:57.592672 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.592855 [debug] [Thread-14 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.593404 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m18:39:57.593565 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: COMMIT
[0m18:39:57.594003 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m18:39:57.594154 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.594342 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.594511 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: COMMIT
[0m18:39:57.594665 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: COMMIT
[0m18:39:57.594919 [debug] [Thread-12 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.595983 [debug] [Thread-12 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup"
[0m18:39:57.596155 [debug] [Thread-11 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.596299 [debug] [Thread-14 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.596521 [debug] [Thread-12 ]: Using postgres connection "model.dbt_service.staging_city_state_profiling"
[0m18:39:57.597448 [debug] [Thread-11 ]: Applying DROP to: "finny_db"."public_staging_analysis"."company_analysis__dbt_backup"
[0m18:39:57.598282 [debug] [Thread-14 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup"
[0m18:39:57.598440 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_city_state_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_city_state_profiling__dbt_backup" cascade
[0m18:39:57.598665 [debug] [Thread-11 ]: Using postgres connection "model.dbt_service.company_analysis"
[0m18:39:57.598967 [debug] [Thread-14 ]: Using postgres connection "model.dbt_service.staging_data_profiling"
[0m18:39:57.599182 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.company_analysis"} */
drop table if exists "finny_db"."public_staging_analysis"."company_analysis__dbt_backup" cascade
[0m18:39:57.599344 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_data_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_data_profiling__dbt_backup" cascade
[0m18:39:57.600103 [debug] [Thread-12 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.600607 [debug] [Thread-12 ]: On model.dbt_service.staging_city_state_profiling: Close
[0m18:39:57.600753 [debug] [Thread-11 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.600906 [debug] [Thread-14 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.601361 [debug] [Thread-11 ]: On model.dbt_service.company_analysis: Close
[0m18:39:57.601595 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092ee350>]}
[0m18:39:57.602159 [debug] [Thread-14 ]: On model.dbt_service.staging_data_profiling: Close
[0m18:39:57.602447 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bcae050>]}
[0m18:39:57.602677 [info ] [Thread-12 ]: 9 of 18 OK created sql table model public_staging_analysis.staging_city_state_profiling  [[32mSELECT 76[0m in 0.23s]
[0m18:39:57.602990 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092b7710>]}
[0m18:39:57.603278 [info ] [Thread-11 ]: 8 of 18 OK created sql table model public_staging_analysis.company_analysis .... [[32mSELECT 91[0m in 0.23s]
[0m18:39:57.603537 [debug] [Thread-12 ]: Finished running node model.dbt_service.staging_city_state_profiling
[0m18:39:57.603786 [info ] [Thread-14 ]: 11 of 18 OK created sql table model public_staging_analysis.staging_data_profiling  [[32mSELECT 2[0m in 0.23s]
[0m18:39:57.604020 [debug] [Thread-11 ]: Finished running node model.dbt_service.company_analysis
[0m18:39:57.604316 [debug] [Thread-14 ]: Finished running node model.dbt_service.staging_data_profiling
[0m18:39:57.696058 [debug] [Thread-16 ]: SQL status: SELECT 100010 in 0.272 seconds
[0m18:39:57.697490 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.697669 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects" rename to "stg_unified_prospects__dbt_backup"
[0m18:39:57.698311 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.699336 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.699488 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
alter table "finny_db"."public_staging"."stg_unified_prospects__dbt_tmp" rename to "stg_unified_prospects"
[0m18:39:57.700955 [debug] [Thread-16 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.701513 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m18:39:57.701664 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.701799 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: COMMIT
[0m18:39:57.710970 [debug] [Thread-16 ]: SQL status: COMMIT in 0.009 seconds
[0m18:39:57.712080 [debug] [Thread-16 ]: Applying DROP to: "finny_db"."public_staging"."stg_unified_prospects__dbt_backup"
[0m18:39:57.712319 [debug] [Thread-16 ]: Using postgres connection "model.dbt_service.stg_unified_prospects"
[0m18:39:57.712468 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_unified_prospects"} */
drop table if exists "finny_db"."public_staging"."stg_unified_prospects__dbt_backup" cascade
[0m18:39:57.715830 [debug] [Thread-16 ]: SQL status: DROP TABLE in 0.003 seconds
[0m18:39:57.716382 [debug] [Thread-16 ]: On model.dbt_service.stg_unified_prospects: Close
[0m18:39:57.716632 [debug] [Thread-16 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd50310>]}
[0m18:39:57.716893 [info ] [Thread-16 ]: 13 of 18 OK created sql table model public_staging.stg_unified_prospects ....... [[32mSELECT 100010[0m in 0.34s]
[0m18:39:57.717119 [debug] [Thread-16 ]: Finished running node model.dbt_service.stg_unified_prospects
[0m18:39:57.717363 [debug] [Thread-18 ]: Began running node model.dbt_service.stg_prospect_matches
[0m18:39:57.717547 [info ] [Thread-18 ]: 14 of 18 START sql incremental model public_staging.stg_prospect_matches ....... [RUN]
[0m18:39:57.717750 [debug] [Thread-18 ]: Acquiring new postgres connection 'model.dbt_service.stg_prospect_matches'
[0m18:39:57.717897 [debug] [Thread-18 ]: Began compiling node model.dbt_service.stg_prospect_matches
[0m18:39:57.721603 [debug] [Thread-18 ]: Writing injected SQL for node "model.dbt_service.stg_prospect_matches"
[0m18:39:57.722246 [debug] [Thread-18 ]: Began executing node model.dbt_service.stg_prospect_matches
[0m18:39:57.734402 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.734641 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

    
  
    

  create temporary table "stg_prospect_matches__dbt_tmp183957732187"
  
  
    as
  
  (
    -- Prospect similarity matching for deduplication
-- This model identifies potential duplicate prospects based on name, email, and company similarity



WITH base AS (
  SELECT *
  FROM "finny_db"."public_staging"."stg_unified_prospects"
  WHERE status IS DISTINCT FROM 'merged'
    AND prospect_id <= 2000000  -- Increase to 10000 records
  
    -- Only process new prospects in incremental runs
    AND prospect_id > (SELECT COALESCE(MAX(source_id), 0) FROM "finny_db"."public_staging"."stg_prospect_matches")
  
),

pairs AS (
  SELECT 
    a.prospect_id AS source_id,
    b.prospect_id AS target_id,
    similarity(a.name, b.name) AS name_sim,
    similarity(a.email, b.email) AS email_sim,
    similarity(a.company, b.company) AS company_sim,
    (
      0.5 * similarity(a.name, b.name) +
      0.4 * similarity(a.email, b.email) +
      0.1 * similarity(a.company, b.company)
    ) AS total_score
  FROM base a
  JOIN base b
    ON a.prospect_id < b.prospect_id
  WHERE (
    similarity(a.name, b.name) > 0.5 and  -- Lower threshold
    similarity(a.email, b.email) > 0.4   -- Lower threshold
  )
)

SELECT 
  source_id,
  target_id,
  name_sim,
  email_sim,
  company_sim,
  total_score
FROM pairs
WHERE total_score > 0.6  -- Lower threshold for final results
  );
  
  
[0m18:39:57.734811 [debug] [Thread-18 ]: Opening a new connection, currently in state init
[0m18:39:57.743783 [debug] [Thread-15 ]: SQL status: SELECT 152 in 0.318 seconds
[0m18:39:57.745236 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.745388 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling" rename to "staging_location_profiling__dbt_backup"
[0m18:39:57.745913 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.746942 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.747100 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_tmp" rename to "staging_location_profiling"
[0m18:39:57.747607 [debug] [Thread-15 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.748156 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m18:39:57.748333 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.748476 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: COMMIT
[0m18:39:57.749137 [debug] [Thread-15 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.750168 [debug] [Thread-15 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup"
[0m18:39:57.750536 [debug] [Thread-15 ]: Using postgres connection "model.dbt_service.staging_location_profiling"
[0m18:39:57.750753 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_location_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_location_profiling__dbt_backup" cascade
[0m18:39:57.752038 [debug] [Thread-15 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.752677 [debug] [Thread-15 ]: On model.dbt_service.staging_location_profiling: Close
[0m18:39:57.752964 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091921d0>]}
[0m18:39:57.753237 [info ] [Thread-15 ]: 12 of 18 OK created sql table model public_staging_analysis.staging_location_profiling  [[32mSELECT 152[0m in 0.38s]
[0m18:39:57.753482 [debug] [Thread-15 ]: Finished running node model.dbt_service.staging_location_profiling
[0m18:39:57.770500 [debug] [Thread-13 ]: SQL status: SELECT 180 in 0.346 seconds
[0m18:39:57.772108 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.772303 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling" rename to "staging_company_profiling__dbt_backup"
[0m18:39:57.772920 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.773931 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.774078 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
alter table "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_tmp" rename to "staging_company_profiling"
[0m18:39:57.774541 [debug] [Thread-13 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.774973 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m18:39:57.775107 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.775238 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: COMMIT
[0m18:39:57.775895 [debug] [Thread-13 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.776752 [debug] [Thread-13 ]: Applying DROP to: "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup"
[0m18:39:57.776958 [debug] [Thread-13 ]: Using postgres connection "model.dbt_service.staging_company_profiling"
[0m18:39:57.777089 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.staging_company_profiling"} */
drop table if exists "finny_db"."public_staging_analysis"."staging_company_profiling__dbt_backup" cascade
[0m18:39:57.778338 [debug] [Thread-13 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.778773 [debug] [Thread-13 ]: On model.dbt_service.staging_company_profiling: Close
[0m18:39:57.779049 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092cb3d0>]}
[0m18:39:57.779298 [info ] [Thread-13 ]: 10 of 18 OK created sql table model public_staging_analysis.staging_company_profiling  [[32mSELECT 180[0m in 0.40s]
[0m18:39:57.779522 [debug] [Thread-13 ]: Finished running node model.dbt_service.staging_company_profiling
[0m18:39:57.889352 [debug] [Thread-18 ]: SQL status: SELECT 0 in 0.154 seconds
[0m18:39:57.905582 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.905961 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: BEGIN
[0m18:39:57.906585 [debug] [Thread-18 ]: SQL status: BEGIN in 0.000 seconds
[0m18:39:57.906789 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.906959 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp183957732187'
        
      order by ordinal_position

  
[0m18:39:57.910237 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.003 seconds
[0m18:39:57.912438 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.912627 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m18:39:57.913973 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m18:39:57.919454 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.919682 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches__dbt_tmp183957732187'
        
      order by ordinal_position

  
[0m18:39:57.921237 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m18:39:57.922650 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.922856 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "finny_db".INFORMATION_SCHEMA.columns
      where table_name = 'stg_prospect_matches'
        
        and table_schema = 'public_staging'
        
      order by ordinal_position

  
[0m18:39:57.924282 [debug] [Thread-18 ]: SQL status: SELECT 6 in 0.001 seconds
[0m18:39:57.931115 [debug] [Thread-18 ]: 
    In "finny_db"."public_staging"."stg_prospect_matches":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m18:39:57.940413 [debug] [Thread-18 ]: Writing runtime sql for node "model.dbt_service.stg_prospect_matches"
[0m18:39:57.940942 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.941112 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_prospect_matches"} */

      
        delete from "finny_db"."public_staging"."stg_prospect_matches" as DBT_INTERNAL_DEST
        where (source_id, target_id) in (
            select distinct source_id, target_id
            from "stg_prospect_matches__dbt_tmp183957732187" as DBT_INTERNAL_SOURCE
        );

    

    insert into "finny_db"."public_staging"."stg_prospect_matches" ("source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score")
    (
        select "source_id", "target_id", "name_sim", "email_sim", "company_sim", "total_score"
        from "stg_prospect_matches__dbt_tmp183957732187"
    )
  
[0m18:39:57.941871 [debug] [Thread-18 ]: SQL status: INSERT 0 0 in 0.001 seconds
[0m18:39:57.942440 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m18:39:57.942668 [debug] [Thread-18 ]: Using postgres connection "model.dbt_service.stg_prospect_matches"
[0m18:39:57.942868 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: COMMIT
[0m18:39:57.943523 [debug] [Thread-18 ]: SQL status: COMMIT in 0.000 seconds
[0m18:39:57.943861 [debug] [Thread-18 ]: On model.dbt_service.stg_prospect_matches: Close
[0m18:39:57.944128 [debug] [Thread-18 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc05610>]}
[0m18:39:57.944415 [info ] [Thread-18 ]: 14 of 18 OK created sql incremental model public_staging.stg_prospect_matches .. [[32mINSERT 0 0[0m in 0.23s]
[0m18:39:57.944659 [debug] [Thread-18 ]: Finished running node model.dbt_service.stg_prospect_matches
[0m18:39:57.944992 [debug] [Thread-20 ]: Began running node model.dbt_service.prospect_matching_ratio
[0m18:39:57.945160 [debug] [Thread-21 ]: Began running node model.dbt_service.stg_entity_clusters
[0m18:39:57.945431 [info ] [Thread-20 ]: 15 of 18 START sql view model public_marts.prospect_matching_ratio ............. [RUN]
[0m18:39:57.945653 [info ] [Thread-21 ]: 16 of 18 START sql table model public_staging.stg_entity_clusters .............. [RUN]
[0m18:39:57.945927 [debug] [Thread-20 ]: Acquiring new postgres connection 'model.dbt_service.prospect_matching_ratio'
[0m18:39:57.946161 [debug] [Thread-21 ]: Acquiring new postgres connection 'model.dbt_service.stg_entity_clusters'
[0m18:39:57.946338 [debug] [Thread-20 ]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m18:39:57.946569 [debug] [Thread-21 ]: Began compiling node model.dbt_service.stg_entity_clusters
[0m18:39:57.948860 [debug] [Thread-20 ]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.950232 [debug] [Thread-21 ]: Writing injected SQL for node "model.dbt_service.stg_entity_clusters"
[0m18:39:57.950823 [debug] [Thread-20 ]: Began executing node model.dbt_service.prospect_matching_ratio
[0m18:39:57.950970 [debug] [Thread-21 ]: Began executing node model.dbt_service.stg_entity_clusters
[0m18:39:57.952427 [debug] [Thread-20 ]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.953846 [debug] [Thread-21 ]: Writing runtime sql for node "model.dbt_service.stg_entity_clusters"
[0m18:39:57.954262 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.954431 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.954604 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m18:39:57.954792 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: BEGIN
[0m18:39:57.955001 [debug] [Thread-20 ]: Opening a new connection, currently in state init
[0m18:39:57.955193 [debug] [Thread-21 ]: Opening a new connection, currently in state init
[0m18:39:57.962212 [debug] [Thread-21 ]: SQL status: BEGIN in 0.007 seconds
[0m18:39:57.962416 [debug] [Thread-20 ]: SQL status: BEGIN in 0.007 seconds
[0m18:39:57.962614 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.962779 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.962974 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */

  
    

  create  table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp"
  
  
    as
  
  (
    -- Entity clusters for prospect deduplication
-- This model identifies canonical (primary) records and duplicates to be merged
-- Results:

-- 87 duplicate pairs identified from high-confidence matches (score > 0.8)
-- Canonical ID logic: Uses the higher prospect_id as canonical (keeps the "later" record)
-- Merged ID logic: Lower prospect_id will be marked as duplicate
-- Range: Processing prospects from 658 to 9999
-- How it works:

-- Example: Prospect 658 is canonical, Prospect 291 should be merged into it
-- etc.



WITH ranked AS (
  SELECT 
    *,
    GREATEST(source_id, target_id) AS canonical_id,
    LEAST(source_id, target_id) AS merged_id
  FROM "finny_db"."public_staging"."stg_prospect_matches"
  WHERE total_score > 0.8  -- Only high-confidence matches
)

SELECT DISTINCT 
  canonical_id, 
  merged_id,
  'duplicate' as merge_reason
FROM ranked
ORDER BY canonical_id, merged_id
  );
  
[0m18:39:57.963288 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m18:39:57.965192 [debug] [Thread-20 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m18:39:57.966749 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.966942 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m18:39:57.967659 [debug] [Thread-20 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.968360 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m18:39:57.968551 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.968703 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m18:39:57.969465 [debug] [Thread-20 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:57.971792 [debug] [Thread-20 ]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m18:39:57.972070 [debug] [Thread-20 ]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:39:57.972252 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m18:39:57.972856 [debug] [Thread-20 ]: SQL status: DROP VIEW in 0.000 seconds
[0m18:39:57.973607 [debug] [Thread-20 ]: On model.dbt_service.prospect_matching_ratio: Close
[0m18:39:57.973884 [debug] [Thread-20 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd82590>]}
[0m18:39:57.974213 [info ] [Thread-20 ]: 15 of 18 OK created sql view model public_marts.prospect_matching_ratio ........ [[32mCREATE VIEW[0m in 0.03s]
[0m18:39:57.974515 [debug] [Thread-20 ]: Finished running node model.dbt_service.prospect_matching_ratio
[0m18:39:57.975601 [debug] [Thread-21 ]: SQL status: SELECT 15274 in 0.012 seconds
[0m18:39:57.977260 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.977448 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters" rename to "stg_entity_clusters__dbt_backup"
[0m18:39:57.978013 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.979172 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.979391 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
alter table "finny_db"."public_staging"."stg_entity_clusters__dbt_tmp" rename to "stg_entity_clusters"
[0m18:39:57.979898 [debug] [Thread-21 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:57.980452 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m18:39:57.980640 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.980784 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: COMMIT
[0m18:39:57.982825 [debug] [Thread-21 ]: SQL status: COMMIT in 0.002 seconds
[0m18:39:57.983800 [debug] [Thread-21 ]: Applying DROP to: "finny_db"."public_staging"."stg_entity_clusters__dbt_backup"
[0m18:39:57.984027 [debug] [Thread-21 ]: Using postgres connection "model.dbt_service.stg_entity_clusters"
[0m18:39:57.984165 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.stg_entity_clusters"} */
drop table if exists "finny_db"."public_staging"."stg_entity_clusters__dbt_backup" cascade
[0m18:39:57.985115 [debug] [Thread-21 ]: SQL status: DROP TABLE in 0.001 seconds
[0m18:39:57.985585 [debug] [Thread-21 ]: On model.dbt_service.stg_entity_clusters: Close
[0m18:39:57.985845 [debug] [Thread-21 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091103d0>]}
[0m18:39:57.986109 [info ] [Thread-21 ]: 16 of 18 OK created sql table model public_staging.stg_entity_clusters ......... [[32mSELECT 15274[0m in 0.04s]
[0m18:39:57.986353 [debug] [Thread-21 ]: Finished running node model.dbt_service.stg_entity_clusters
[0m18:39:57.986713 [debug] [Thread-23 ]: Began running node model.dbt_service.data_overview
[0m18:39:57.986910 [debug] [Thread-24 ]: Began running node model.dbt_service.setup_functions
[0m18:39:57.987184 [info ] [Thread-23 ]: 17 of 18 START sql view model public_marts.data_overview ....................... [RUN]
[0m18:39:57.987431 [info ] [Thread-24 ]: 18 of 18 START sql view model public.setup_functions ........................... [RUN]
[0m18:39:57.987687 [debug] [Thread-23 ]: Acquiring new postgres connection 'model.dbt_service.data_overview'
[0m18:39:57.987886 [debug] [Thread-24 ]: Acquiring new postgres connection 'model.dbt_service.setup_functions'
[0m18:39:57.988060 [debug] [Thread-23 ]: Began compiling node model.dbt_service.data_overview
[0m18:39:57.988206 [debug] [Thread-24 ]: Began compiling node model.dbt_service.setup_functions
[0m18:39:57.989763 [debug] [Thread-23 ]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m18:39:57.990791 [debug] [Thread-24 ]: Writing injected SQL for node "model.dbt_service.setup_functions"
[0m18:39:57.991439 [debug] [Thread-24 ]: Began executing node model.dbt_service.setup_functions
[0m18:39:57.991591 [debug] [Thread-23 ]: Began executing node model.dbt_service.data_overview
[0m18:39:57.992982 [debug] [Thread-24 ]: Writing runtime sql for node "model.dbt_service.setup_functions"
[0m18:39:57.994300 [debug] [Thread-23 ]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m18:39:57.994690 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:57.994845 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: BEGIN
[0m18:39:57.994999 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m18:39:57.995171 [debug] [Thread-24 ]: Opening a new connection, currently in state init
[0m18:39:57.995324 [debug] [Thread-23 ]: On model.dbt_service.data_overview: BEGIN
[0m18:39:57.995548 [debug] [Thread-23 ]: Opening a new connection, currently in state init
[0m18:39:58.002077 [debug] [Thread-23 ]: SQL status: BEGIN in 0.006 seconds
[0m18:39:58.002266 [debug] [Thread-24 ]: SQL status: BEGIN in 0.007 seconds
[0m18:39:58.002453 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m18:39:58.002625 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.002815 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m18:39:58.003010 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

  create view "finny_db"."public"."setup_functions__dbt_tmp"
    
    
  as (
    -- Database function setup for deduplication pipeline
-- This model ensures the merge_similar_entities() function exists in the database



select 'Function merge_similar_entities() created' as status
  );
[0m18:39:58.003828 [debug] [Thread-24 ]: SQL status: CREATE VIEW in 0.001 seconds
[0m18:39:58.005104 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.005240 [debug] [Thread-23 ]: SQL status: CREATE VIEW in 0.002 seconds
[0m18:39:58.005382 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions" rename to "setup_functions__dbt_backup"
[0m18:39:58.006435 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m18:39:58.006631 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m18:39:58.006873 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:39:58.007942 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.008111 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
alter table "finny_db"."public"."setup_functions__dbt_tmp" rename to "setup_functions"
[0m18:39:58.008271 [debug] [Thread-23 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:58.008830 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m18:39:58.008997 [debug] [Thread-24 ]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:39:58.009137 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m18:39:58.012090 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.012262 [debug] [Thread-23 ]: On model.dbt_service.data_overview: COMMIT
[0m18:39:58.012452 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */

        CREATE OR REPLACE FUNCTION merge_similar_entities()
  RETURNS VOID
  LANGUAGE plpgsql
  AS $$
  DECLARE
      rec RECORD;
      merge_count INTEGER := 0;
  BEGIN
      FOR rec IN
          SELECT canonical_id, merged_id
          FROM "finny_db"."public_staging"."stg_entity_clusters"
      LOOP
          -- Lock both entities to prevent race conditions
          PERFORM * FROM "finny_db"."public_staging"."stg_unified_prospects"
          WHERE prospect_id IN (rec.merged_id, rec.canonical_id) FOR UPDATE;

          -- Update canonical record fields only if null (fill in missing data)
          UPDATE "finny_db"."public_staging"."stg_unified_prospects" p
          SET
              name = COALESCE(p.name, s.name),
              email = COALESCE(p.email, s.email),
              company = COALESCE(p.company, s.company),
              title = COALESCE(p.title, s.title),
              city = COALESCE(p.city, s.city),
              state = COALESCE(p.state, s.state),
              company_revenue = COALESCE(p.company_revenue, s.company_revenue)
          FROM "finny_db"."public_staging"."stg_unified_prospects" s
          WHERE p.prospect_id = rec.canonical_id
            AND s.prospect_id = rec.merged_id;

          -- Mark merged record
          UPDATE "finny_db"."public_staging"."stg_unified_prospects"
          SET status = 'merged'
          WHERE prospect_id = rec.merged_id;

          merge_count := merge_count + 1;
      END LOOP;

      RAISE NOTICE 'Merged % entities', merge_count;
  END;
  $$;
      
[0m18:39:58.013590 [debug] [Thread-23 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:58.014485 [debug] [Thread-23 ]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m18:39:58.014681 [debug] [Thread-24 ]: SQL status: CREATE FUNCTION in 0.002 seconds
[0m18:39:58.015030 [debug] [Thread-23 ]: Using postgres connection "model.dbt_service.data_overview"
[0m18:39:58.015462 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m18:39:58.015617 [debug] [Thread-23 ]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m18:39:58.015777 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.015959 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: COMMIT
[0m18:39:58.016366 [debug] [Thread-23 ]: SQL status: DROP VIEW in 0.000 seconds
[0m18:39:58.016920 [debug] [Thread-23 ]: On model.dbt_service.data_overview: Close
[0m18:39:58.017079 [debug] [Thread-24 ]: SQL status: COMMIT in 0.001 seconds
[0m18:39:58.017349 [debug] [Thread-23 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdde190>]}
[0m18:39:58.018487 [debug] [Thread-24 ]: Applying DROP to: "finny_db"."public"."setup_functions__dbt_backup"
[0m18:39:58.018824 [info ] [Thread-23 ]: 17 of 18 OK created sql view model public_marts.data_overview .................. [[32mCREATE VIEW[0m in 0.03s]
[0m18:39:58.019095 [debug] [Thread-24 ]: Using postgres connection "model.dbt_service.setup_functions"
[0m18:39:58.019317 [debug] [Thread-23 ]: Finished running node model.dbt_service.data_overview
[0m18:39:58.019484 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.setup_functions"} */
drop view if exists "finny_db"."public"."setup_functions__dbt_backup" cascade
[0m18:39:58.020442 [debug] [Thread-24 ]: SQL status: DROP VIEW in 0.001 seconds
[0m18:39:58.020956 [debug] [Thread-24 ]: On model.dbt_service.setup_functions: Close
[0m18:39:58.021170 [debug] [Thread-24 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d28dea2-84cb-418e-a18c-f882b82cc17d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdcb050>]}
[0m18:39:58.021409 [info ] [Thread-24 ]: 18 of 18 OK created sql view model public.setup_functions ...................... [[32mCREATE VIEW[0m in 0.03s]
[0m18:39:58.021624 [debug] [Thread-24 ]: Finished running node model.dbt_service.setup_functions
[0m18:39:58.022604 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:58.022762 [debug] [MainThread]: On master: BEGIN
[0m18:39:58.022876 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:39:58.028316 [debug] [MainThread]: SQL status: BEGIN in 0.005 seconds
[0m18:39:58.028546 [debug] [MainThread]: On master: COMMIT
[0m18:39:58.028685 [debug] [MainThread]: Using postgres connection "master"
[0m18:39:58.028802 [debug] [MainThread]: On master: COMMIT
[0m18:39:58.029147 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:39:58.029272 [debug] [MainThread]: On master: Close
[0m18:39:58.029467 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:39:58.029586 [debug] [MainThread]: Connection 'model.dbt_service.raw_fxf_data' was properly closed.
[0m18:39:58.029697 [debug] [MainThread]: Connection 'model.dbt_service.raw_pdl_data' was properly closed.
[0m18:39:58.029801 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m18:39:58.029910 [debug] [MainThread]: Connection 'model.dbt_service.stg_pdl_data' was properly closed.
[0m18:39:58.030015 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m18:39:58.030115 [debug] [MainThread]: Connection 'model.dbt_service.raw_company_profiling' was properly closed.
[0m18:39:58.030225 [debug] [MainThread]: Connection 'model.dbt_service.raw_data_profiling' was properly closed.
[0m18:39:58.030326 [debug] [MainThread]: Connection 'model.dbt_service.raw_location_profiling' was properly closed.
[0m18:39:58.030426 [debug] [MainThread]: Connection 'model.dbt_service.stg_fxf_data' was properly closed.
[0m18:39:58.030524 [debug] [MainThread]: Connection 'model.dbt_service.company_analysis' was properly closed.
[0m18:39:58.030623 [debug] [MainThread]: Connection 'model.dbt_service.staging_city_state_profiling' was properly closed.
[0m18:39:58.030724 [debug] [MainThread]: Connection 'model.dbt_service.staging_company_profiling' was properly closed.
[0m18:39:58.030814 [debug] [MainThread]: Connection 'model.dbt_service.staging_data_profiling' was properly closed.
[0m18:39:58.030902 [debug] [MainThread]: Connection 'model.dbt_service.staging_location_profiling' was properly closed.
[0m18:39:58.030988 [debug] [MainThread]: Connection 'model.dbt_service.stg_unified_prospects' was properly closed.
[0m18:39:58.031075 [debug] [MainThread]: Connection 'model.dbt_service.stg_prospect_matches' was properly closed.
[0m18:39:58.031160 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m18:39:58.031247 [debug] [MainThread]: Connection 'model.dbt_service.stg_entity_clusters' was properly closed.
[0m18:39:58.031335 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m18:39:58.031433 [debug] [MainThread]: Connection 'model.dbt_service.setup_functions' was properly closed.
[0m18:39:58.031636 [info ] [MainThread]: 
[0m18:39:58.031788 [info ] [MainThread]: Finished running 1 incremental model, 12 table models, 5 view models in 0 hours 0 minutes and 1.04 seconds (1.04s).
[0m18:39:58.032773 [debug] [MainThread]: Command end result
[0m18:39:58.045440 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:39:58.046504 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:39:58.049548 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m18:39:58.049706 [info ] [MainThread]: 
[0m18:39:58.049864 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:39:58.049993 [info ] [MainThread]: 
[0m18:39:58.050125 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=18
[0m18:39:58.052538 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.4685336, "process_in_blocks": "0", "process_kernel_time": 0.261511, "process_mem_max_rss": "140853248", "process_out_blocks": "0", "process_user_time": 1.511157}
[0m18:39:58.052911 [debug] [MainThread]: Command `dbt run` succeeded at 18:39:58.052866 after 1.47 seconds
[0m18:39:58.053089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026e8050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071be050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1027236d0>]}
[0m18:39:58.053264 [debug] [MainThread]: Flushing usage events
[0m18:39:58.374855 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:51:50.832445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e96390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f04e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f139d0>]}


============================== 18:51:50.834929 | 9223da3b-b3a9-4908-869c-2e8498179597 ==============================
[0m18:51:50.834929 [info ] [MainThread]: Running with dbt=1.10.13
[0m18:51:50.835252 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'version_check': 'True', 'write_json': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'empty': 'None', 'warn_error': 'None', 'cache_selected_only': 'False', 'static_parser': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'no_print': 'None', 'target_path': 'None', 'invocation_command': 'dbt run-operation merge_entities', 'printer_width': '80', 'introspect': 'True', 'debug': 'False'}
[0m18:51:50.963016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9223da3b-b3a9-4908-869c-2e8498179597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f07f50>]}
[0m18:51:50.993325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9223da3b-b3a9-4908-869c-2e8498179597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104551f10>]}
[0m18:51:50.994071 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m18:51:51.039987 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m18:51:51.106540 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:51:51.106730 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:51:51.129314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9223da3b-b3a9-4908-869c-2e8498179597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108020c10>]}
[0m18:51:51.169739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:51:51.170744 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:51:51.180722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9223da3b-b3a9-4908-869c-2e8498179597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105317290>]}
[0m18:51:51.180964 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m18:51:51.181128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9223da3b-b3a9-4908-869c-2e8498179597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108023bd0>]}
[0m18:51:51.181382 [debug] [MainThread]: Acquiring new postgres connection 'macro_merge_entities'
[0m18:51:51.181544 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m18:51:51.181660 [debug] [MainThread]: On macro_merge_entities: BEGIN
[0m18:51:51.181764 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:51:51.212557 [debug] [MainThread]: SQL status: BEGIN in 0.031 seconds
[0m18:51:51.212782 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m18:51:51.212911 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m18:51:51.213015 [debug] [MainThread]: On macro_merge_entities: COMMIT
[0m18:51:51.213608 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:51:51.215923 [info ] [MainThread]: Executing prospect merge operation...
[0m18:51:51.270679 [debug] [MainThread]: Using postgres connection "macro_merge_entities"
[0m18:51:51.270947 [debug] [MainThread]: On macro_merge_entities: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "macro_merge_entities"} */

    
      SELECT merge_similar_entities();
    
  
[0m18:55:47.509762 [debug] [MainThread]: SQL status: SELECT 1 in 236.238 seconds
[0m18:55:47.515692 [info ] [MainThread]: Merge operation completed successfully
[0m18:55:47.516713 [debug] [MainThread]: On macro_merge_entities: Close
[0m18:55:47.526710 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m18:55:47.530045 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 236.73164, "process_in_blocks": "0", "process_kernel_time": 0.205841, "process_mem_max_rss": "130678784", "process_out_blocks": "0", "process_user_time": 0.840977}
[0m18:55:47.530574 [debug] [MainThread]: Command `dbt run-operation` succeeded at 18:55:47.530477 after 236.73 seconds
[0m18:55:47.530990 [debug] [MainThread]: Connection 'macro_merge_entities' was properly closed.
[0m18:55:47.531424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102488390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105314d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9ee90>]}
[0m18:55:47.531885 [debug] [MainThread]: Flushing usage events
[0m18:55:47.827655 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:56:51.014548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116631490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166a7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116699fd0>]}


============================== 18:56:51.017172 | 1594a546-c190-4e26-b462-794e442192fe ==============================
[0m18:56:51.017172 [info ] [MainThread]: Running with dbt=1.10.13
[0m18:56:51.017482 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'warn_error': 'None', 'static_parser': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'cache_selected_only': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select data_overview', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'version_check': 'True', 'empty': 'False', 'fail_fast': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'log_format': 'default', 'no_print': 'None', 'partial_parse': 'True', 'log_cache_events': 'False'}
[0m18:56:51.137211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116718790>]}
[0m18:56:51.166916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122f9590>]}
[0m18:56:51.167503 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m18:56:51.213313 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m18:56:51.280405 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:56:51.280615 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:56:51.303760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c56810>]}
[0m18:56:51.343048 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:56:51.344061 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:56:51.355692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1174af050>]}
[0m18:56:51.355926 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m18:56:51.356093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1177f6f10>]}
[0m18:56:51.356816 [info ] [MainThread]: 
[0m18:56:51.356977 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m18:56:51.357096 [info ] [MainThread]: 
[0m18:56:51.357309 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m18:56:51.357682 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:56:51.411609 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:56:51.411842 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:56:51.411967 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.434817 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.023 seconds
[0m18:56:51.435446 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:56:51.437629 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_staging)
[0m18:56:51.437879 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m18:56:51.438103 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m18:56:51.440561 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:56:51.441065 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw_analysis'
[0m18:56:51.441290 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m18:56:51.441516 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m18:56:51.442358 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:56:51.443470 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:56:51.443662 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m18:56:51.444396 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:56:51.445091 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:56:51.445806 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:56:51.445955 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m18:56:51.446074 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m18:56:51.446196 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:56:51.446319 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m18:56:51.446432 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m18:56:51.446540 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m18:56:51.446681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.446787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.446997 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.447140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.447271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:51.465047 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m18:56:51.465219 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:56:51.465357 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m18:56:51.468472 [debug] [ThreadPool]: SQL status: BEGIN in 0.021 seconds
[0m18:56:51.468620 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m18:56:51.468729 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m18:56:51.468841 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:56:51.468961 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:56:51.469075 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:56:51.469220 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m18:56:51.469362 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m18:56:51.469512 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m18:56:51.469679 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m18:56:51.469826 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m18:56:51.469957 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:56:51.470072 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:56:51.470285 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m18:56:51.470443 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m18:56:51.470685 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.005 seconds
[0m18:56:51.471220 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m18:56:51.472924 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m18:56:51.473588 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.003 seconds
[0m18:56:51.473734 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m18:56:51.473856 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.004 seconds
[0m18:56:51.474419 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m18:56:51.474597 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.004 seconds
[0m18:56:51.474740 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.004 seconds
[0m18:56:51.475269 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m18:56:51.475753 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m18:56:51.476274 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m18:56:51.476424 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m18:56:51.476846 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m18:56:51.477474 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m18:56:51.477591 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m18:56:51.477699 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m18:56:51.478384 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m18:56:51.482389 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.482615 [debug] [MainThread]: On master: BEGIN
[0m18:56:51.482786 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:56:51.489871 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m18:56:51.490095 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.490293 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m18:56:51.493308 [debug] [MainThread]: SQL status: SELECT 11 in 0.003 seconds
[0m18:56:51.494822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d4bdd0>]}
[0m18:56:51.495171 [debug] [MainThread]: On master: ROLLBACK
[0m18:56:51.495786 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.495948 [debug] [MainThread]: On master: BEGIN
[0m18:56:51.496575 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m18:56:51.496805 [debug] [MainThread]: On master: COMMIT
[0m18:56:51.496933 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.497074 [debug] [MainThread]: On master: COMMIT
[0m18:56:51.497590 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:56:51.497870 [debug] [MainThread]: On master: Close
[0m18:56:51.500467 [debug] [Thread-1 (]: Began running node model.dbt_service.data_overview
[0m18:56:51.500752 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.data_overview ......................... [RUN]
[0m18:56:51.501005 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_staging, now model.dbt_service.data_overview)
[0m18:56:51.501182 [debug] [Thread-1 (]: Began compiling node model.dbt_service.data_overview
[0m18:56:51.504859 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.data_overview"
[0m18:56:51.505486 [debug] [Thread-1 (]: Began executing node model.dbt_service.data_overview
[0m18:56:51.521548 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.data_overview"
[0m18:56:51.522081 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.522436 [debug] [Thread-1 (]: On model.dbt_service.data_overview: BEGIN
[0m18:56:51.522740 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:56:51.529877 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m18:56:51.530137 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.530330 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */

  create view "finny_db"."public_marts"."data_overview__dbt_tmp"
    
    
  as (
    -- Data overview from deduplication pipeline
-- This model provides a summary of our prospect matching and deduplication results



select
    'stg_unified_prospects' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Total unified prospects before deduplication' as description
from "finny_db"."public_staging"."stg_unified_prospects"

union all

select
    'stg_prospect_matches' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'Potential duplicate pairs identified' as description
from "finny_db"."public_staging"."stg_prospect_matches"

union all

select
    'stg_entity_clusters' as table_name,
    'staging' as schema_name,
    count(*) as row_count,
    'High-confidence duplicates for merging' as description
from "finny_db"."public_staging"."stg_entity_clusters"

union all

select
    'unique_prospects_remaining' as table_name,
    'computed' as schema_name,
    count(*) as row_count,
    'Unidentified prospects (not yet matched as duplicates)' as description
from "finny_db"."public_staging"."stg_unified_prospects"
where status is distinct from 'merged'
  );
[0m18:56:51.531991 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.001 seconds
[0m18:56:51.535007 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.535191 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview" rename to "data_overview__dbt_backup"
[0m18:56:51.535691 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:56:51.536976 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.537185 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
alter table "finny_db"."public_marts"."data_overview__dbt_tmp" rename to "data_overview"
[0m18:56:51.537655 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:56:51.544039 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m18:56:51.544264 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.544416 [debug] [Thread-1 (]: On model.dbt_service.data_overview: COMMIT
[0m18:56:51.545277 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m18:56:51.548028 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."data_overview__dbt_backup"
[0m18:56:51.550341 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.data_overview"
[0m18:56:51.550507 [debug] [Thread-1 (]: On model.dbt_service.data_overview: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.data_overview"} */
drop view if exists "finny_db"."public_marts"."data_overview__dbt_backup" cascade
[0m18:56:51.551615 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m18:56:51.552670 [debug] [Thread-1 (]: On model.dbt_service.data_overview: Close
[0m18:56:51.553593 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1594a546-c190-4e26-b462-794e442192fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d515d0>]}
[0m18:56:51.553957 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.data_overview .................... [[32mCREATE VIEW[0m in 0.05s]
[0m18:56:51.554215 [debug] [Thread-1 (]: Finished running node model.dbt_service.data_overview
[0m18:56:51.555261 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.555456 [debug] [MainThread]: On master: BEGIN
[0m18:56:51.555603 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:56:51.561155 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m18:56:51.561340 [debug] [MainThread]: On master: COMMIT
[0m18:56:51.561471 [debug] [MainThread]: Using postgres connection "master"
[0m18:56:51.561587 [debug] [MainThread]: On master: COMMIT
[0m18:56:51.561889 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:56:51.562040 [debug] [MainThread]: On master: Close
[0m18:56:51.562225 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:56:51.562341 [debug] [MainThread]: Connection 'model.dbt_service.data_overview' was properly closed.
[0m18:56:51.562450 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m18:56:51.562554 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m18:56:51.562653 [debug] [MainThread]: Connection 'list_finny_db_public_raw_analysis' was properly closed.
[0m18:56:51.562753 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m18:56:51.562856 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m18:56:51.562985 [info ] [MainThread]: 
[0m18:56:51.563168 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m18:56:51.563444 [debug] [MainThread]: Command end result
[0m18:56:51.575897 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:56:51.576735 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:56:51.579116 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m18:56:51.579249 [info ] [MainThread]: 
[0m18:56:51.579416 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:56:51.579538 [info ] [MainThread]: 
[0m18:56:51.579667 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m18:56:51.581009 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.6022273, "process_in_blocks": "0", "process_kernel_time": 0.218782, "process_mem_max_rss": "139427840", "process_out_blocks": "0", "process_user_time": 1.050555}
[0m18:56:51.581226 [debug] [MainThread]: Command `dbt run` succeeded at 18:56:51.581191 after 0.60 seconds
[0m18:56:51.581381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fac110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166d6090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106276a10>]}
[0m18:56:51.581539 [debug] [MainThread]: Flushing usage events
[0m18:56:51.872415 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:57:05.605607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b330450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9fe5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3a7f90>]}


============================== 18:57:05.607558 | 7dff638d-c27c-45a1-bbe5-d21805591b56 ==============================
[0m18:57:05.607558 [info ] [MainThread]: Running with dbt=1.10.13
[0m18:57:05.607853 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'profiles_dir': '/Users/mahmoud/Workspace/finny/dbt_service', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --select prospect_matching_ratio', 'debug': 'False', 'target_path': 'None', 'warn_error': 'None', 'no_print': 'None', 'use_experimental_parser': 'False', 'write_json': 'True', 'version_check': 'True', 'log_cache_events': 'False', 'introspect': 'True', 'empty': 'False', 'partial_parse': 'True', 'log_path': '/Users/mahmoud/Workspace/finny/dbt_service/logs', 'static_parser': 'True', 'quiet': 'False', 'fail_fast': 'False', 'printer_width': '80'}
[0m18:57:05.691357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b98e410>]}
[0m18:57:05.723159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106565390>]}
[0m18:57:05.723595 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m18:57:05.766620 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m18:57:05.822515 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:57:05.822730 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:57:05.845882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c12d950>]}
[0m18:57:05.887101 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:57:05.888331 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:57:05.897626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9d6750>]}
[0m18:57:05.897965 [info ] [MainThread]: Found 18 models, 3 seeds, 11 data tests, 2 sources, 453 macros
[0m18:57:05.898141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c44f690>]}
[0m18:57:05.898964 [info ] [MainThread]: 
[0m18:57:05.899119 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m18:57:05.899235 [info ] [MainThread]: 
[0m18:57:05.899458 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m18:57:05.899878 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db'
[0m18:57:05.953351 [debug] [ThreadPool]: Using postgres connection "list_finny_db"
[0m18:57:05.953602 [debug] [ThreadPool]: On list_finny_db: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db"} */

    select distinct nspname from pg_namespace
  
[0m18:57:05.953741 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.974960 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.021 seconds
[0m18:57:05.975534 [debug] [ThreadPool]: On list_finny_db: Close
[0m18:57:05.977871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_finny_db, now list_finny_db_public_raw_analysis)
[0m18:57:05.980425 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:57:05.980635 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging'
[0m18:57:05.980764 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: BEGIN
[0m18:57:05.980954 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_raw'
[0m18:57:05.981451 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_staging_analysis'
[0m18:57:05.981670 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public'
[0m18:57:05.982453 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:57:05.982702 [debug] [ThreadPool]: Acquiring new postgres connection 'list_finny_db_public_marts'
[0m18:57:05.982905 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:57:05.983635 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:57:05.984371 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:57:05.985057 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:57:05.985195 [debug] [ThreadPool]: On list_finny_db_public_staging: BEGIN
[0m18:57:05.985875 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:57:05.986114 [debug] [ThreadPool]: On list_finny_db_public_raw: BEGIN
[0m18:57:05.986268 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: BEGIN
[0m18:57:05.986403 [debug] [ThreadPool]: On list_finny_db_public: BEGIN
[0m18:57:05.986528 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.986664 [debug] [ThreadPool]: On list_finny_db_public_marts: BEGIN
[0m18:57:05.986782 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.986962 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.987096 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.987311 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:05.994426 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m18:57:05.994558 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw_analysis"
[0m18:57:05.994701 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw_analysis'
  
[0m18:57:05.994879 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m18:57:05.995113 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging"
[0m18:57:05.995301 [debug] [ThreadPool]: On list_finny_db_public_staging: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging'
  
[0m18:57:05.998043 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.003 seconds
[0m18:57:05.998586 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: ROLLBACK
[0m18:57:05.998706 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.003 seconds
[0m18:57:05.999184 [debug] [ThreadPool]: On list_finny_db_public_staging: ROLLBACK
[0m18:57:06.000406 [debug] [ThreadPool]: On list_finny_db_public_raw_analysis: Close
[0m18:57:06.000624 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m18:57:06.000845 [debug] [ThreadPool]: On list_finny_db_public_staging: Close
[0m18:57:06.001040 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m18:57:06.001169 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m18:57:06.001328 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m18:57:06.001859 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_raw"
[0m18:57:06.002084 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_marts"
[0m18:57:06.002526 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public"
[0m18:57:06.002664 [debug] [ThreadPool]: Using postgres connection "list_finny_db_public_staging_analysis"
[0m18:57:06.002812 [debug] [ThreadPool]: On list_finny_db_public_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_raw"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_raw'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_raw'
  
[0m18:57:06.002985 [debug] [ThreadPool]: On list_finny_db_public_marts: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_marts"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_marts'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_marts'
  
[0m18:57:06.003170 [debug] [ThreadPool]: On list_finny_db_public: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m18:57:06.003329 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "list_finny_db_public_staging_analysis"} */
select
      'finny_db' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public_staging_analysis'
    union all
    select
      'finny_db' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public_staging_analysis'
  
[0m18:57:06.005801 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.002 seconds
[0m18:57:06.005979 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.002 seconds
[0m18:57:06.006110 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.003 seconds
[0m18:57:06.006250 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.003 seconds
[0m18:57:06.006800 [debug] [ThreadPool]: On list_finny_db_public_marts: ROLLBACK
[0m18:57:06.007474 [debug] [ThreadPool]: On list_finny_db_public: ROLLBACK
[0m18:57:06.007928 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: ROLLBACK
[0m18:57:06.008307 [debug] [ThreadPool]: On list_finny_db_public_raw: ROLLBACK
[0m18:57:06.008848 [debug] [ThreadPool]: On list_finny_db_public: Close
[0m18:57:06.009054 [debug] [ThreadPool]: On list_finny_db_public_raw: Close
[0m18:57:06.009192 [debug] [ThreadPool]: On list_finny_db_public_staging_analysis: Close
[0m18:57:06.009312 [debug] [ThreadPool]: On list_finny_db_public_marts: Close
[0m18:57:06.013506 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.013684 [debug] [MainThread]: On master: BEGIN
[0m18:57:06.013793 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:57:06.020946 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m18:57:06.021172 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.021388 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m18:57:06.023690 [debug] [MainThread]: SQL status: SELECT 11 in 0.002 seconds
[0m18:57:06.026412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e23f950>]}
[0m18:57:06.026695 [debug] [MainThread]: On master: ROLLBACK
[0m18:57:06.027408 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.027556 [debug] [MainThread]: On master: BEGIN
[0m18:57:06.028368 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m18:57:06.028528 [debug] [MainThread]: On master: COMMIT
[0m18:57:06.028657 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.028774 [debug] [MainThread]: On master: COMMIT
[0m18:57:06.029158 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:57:06.029393 [debug] [MainThread]: On master: Close
[0m18:57:06.032521 [debug] [Thread-1 (]: Began running node model.dbt_service.prospect_matching_ratio
[0m18:57:06.032826 [info ] [Thread-1 (]: 1 of 1 START sql view model public_marts.prospect_matching_ratio ............... [RUN]
[0m18:57:06.033195 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_finny_db_public_raw_analysis, now model.dbt_service.prospect_matching_ratio)
[0m18:57:06.033461 [debug] [Thread-1 (]: Began compiling node model.dbt_service.prospect_matching_ratio
[0m18:57:06.037125 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.037804 [debug] [Thread-1 (]: Began executing node model.dbt_service.prospect_matching_ratio
[0m18:57:06.054115 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.054563 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.054722 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: BEGIN
[0m18:57:06.054860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:57:06.061167 [debug] [Thread-1 (]: SQL status: BEGIN in 0.006 seconds
[0m18:57:06.061454 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.061684 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */

  create view "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp"
    
    
  as (
    -- Prospect matching ratio analysis
-- This model shows the distribution of unidentified vs matched prospects



with prospect_status_summary as (
    select
        status,
        count(*) as prospect_count,
        round(100.0 * count(*) / sum(count(*)) over(), 2) as percentage
    from "finny_db"."public_staging"."stg_unified_prospects"
    group by status
),

matching_summary as (
    select
        count(*) as total_matches,
        count(distinct source_id) as unique_sources_matched,
        count(distinct target_id) as unique_targets_matched
    from "finny_db"."public_staging"."stg_prospect_matches"
),

ratio_analysis as (
    select
        'Total Prospects' as metric_type,
        sum(prospect_count) as value,
        'prospects' as unit,
        null as percentage
    from prospect_status_summary
    
    union all
    
    select
        'Unidentified Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'unidentified'
    
    union all
    
    select
        'Merged Prospects' as metric_type,
        prospect_count as value,
        'prospects' as unit,
        percentage
    from prospect_status_summary
    where status = 'merged'
    
    union all
    
    select
        'Potential Matches Found' as metric_type,
        total_matches as value,
        'match pairs' as unit,
        null as percentage
    from matching_summary
    
    union all
    
    select
        'Unique Prospects with Matches' as metric_type,
        (unique_sources_matched + unique_targets_matched) as value,
        'prospects' as unit,
        round(100.0 * (unique_sources_matched + unique_targets_matched) / 
              (select sum(prospect_count) from prospect_status_summary), 2) as percentage
    from matching_summary
)

select 
    metric_type,
    value,
    unit,
    case 
        when percentage is not null then percentage || '%'
        else null
    end as percentage
from ratio_analysis
order by 
    case metric_type
        when 'Total Prospects' then 1
        when 'Unidentified Prospects' then 2  
        when 'Merged Prospects' then 3
        when 'Potential Matches Found' then 4
        when 'Unique Prospects with Matches' then 5
    end
  );
[0m18:57:06.063780 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m18:57:06.066947 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.067138 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio" rename to "prospect_matching_ratio__dbt_backup"
[0m18:57:06.067919 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m18:57:06.069297 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.069464 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
alter table "finny_db"."public_marts"."prospect_matching_ratio__dbt_tmp" rename to "prospect_matching_ratio"
[0m18:57:06.070071 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m18:57:06.077182 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m18:57:06.077368 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.077512 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: COMMIT
[0m18:57:06.078729 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m18:57:06.081657 [debug] [Thread-1 (]: Applying DROP to: "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup"
[0m18:57:06.083421 [debug] [Thread-1 (]: Using postgres connection "model.dbt_service.prospect_matching_ratio"
[0m18:57:06.083585 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dbt_service", "target_name": "dev", "node_id": "model.dbt_service.prospect_matching_ratio"} */
drop view if exists "finny_db"."public_marts"."prospect_matching_ratio__dbt_backup" cascade
[0m18:57:06.084934 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m18:57:06.086333 [debug] [Thread-1 (]: On model.dbt_service.prospect_matching_ratio: Close
[0m18:57:06.087234 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7dff638d-c27c-45a1-bbe5-d21805591b56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e619590>]}
[0m18:57:06.087601 [info ] [Thread-1 (]: 1 of 1 OK created sql view model public_marts.prospect_matching_ratio .......... [[32mCREATE VIEW[0m in 0.05s]
[0m18:57:06.087848 [debug] [Thread-1 (]: Finished running node model.dbt_service.prospect_matching_ratio
[0m18:57:06.088917 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.089086 [debug] [MainThread]: On master: BEGIN
[0m18:57:06.089215 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:57:06.094768 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m18:57:06.094959 [debug] [MainThread]: On master: COMMIT
[0m18:57:06.095089 [debug] [MainThread]: Using postgres connection "master"
[0m18:57:06.095207 [debug] [MainThread]: On master: COMMIT
[0m18:57:06.095522 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m18:57:06.095643 [debug] [MainThread]: On master: Close
[0m18:57:06.095829 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:57:06.095945 [debug] [MainThread]: Connection 'model.dbt_service.prospect_matching_ratio' was properly closed.
[0m18:57:06.096054 [debug] [MainThread]: Connection 'list_finny_db_public_staging' was properly closed.
[0m18:57:06.096163 [debug] [MainThread]: Connection 'list_finny_db_public_raw' was properly closed.
[0m18:57:06.096268 [debug] [MainThread]: Connection 'list_finny_db_public_staging_analysis' was properly closed.
[0m18:57:06.096367 [debug] [MainThread]: Connection 'list_finny_db_public' was properly closed.
[0m18:57:06.096471 [debug] [MainThread]: Connection 'list_finny_db_public_marts' was properly closed.
[0m18:57:06.096598 [info ] [MainThread]: 
[0m18:57:06.096746 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m18:57:06.097020 [debug] [MainThread]: Command end result
[0m18:57:06.119072 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/manifest.json
[0m18:57:06.120073 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mahmoud/Workspace/finny/dbt_service/target/semantic_manifest.json
[0m18:57:06.123613 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mahmoud/Workspace/finny/dbt_service/target/run_results.json
[0m18:57:06.123806 [info ] [MainThread]: 
[0m18:57:06.123997 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:57:06.124125 [info ] [MainThread]: 
[0m18:57:06.124475 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m18:57:06.125972 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.5538899, "process_in_blocks": "0", "process_kernel_time": 0.173502, "process_mem_max_rss": "136085504", "process_out_blocks": "0", "process_user_time": 0.996837}
[0m18:57:06.126198 [debug] [MainThread]: Command `dbt run` succeeded at 18:57:06.126163 after 0.55 seconds
[0m18:57:06.126361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3d7cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10451c0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104553710>]}
[0m18:57:06.126517 [debug] [MainThread]: Flushing usage events
[0m18:57:06.343804 [debug] [MainThread]: An error was encountered while trying to flush usage events
